{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Mini Challenge: Gradient Descent – Notebook\n",
    " \n",
    " **Inhalt:**\n",
    " \n",
    " 1. **Aufgabe 1:** Laden und Erkunden des MNIST-Datensatzes  \n",
    "    - Verwendung von `torchvision` und `matplotlib` zur Visualisierung und Analyse der Daten.\n",
    " \n",
    " 2. **Aufgabe 2:** Implementierung eines linearen Layers  \n",
    "    - Erstellung einer Klasse `LinearLayer` mit Methoden für Forward-Pass, Backward-Pass und Parameter-Update.  \n",
    "    - Unittests anhand eines kleinen Beispiels mit handberechneten Ergebnissen.\n",
    " \n",
    " 3. **Aufgabe 3:** Aufbau eines einfachen neuronalen Netzwerks (mit einem Hidden Layer) zur binären Klassifikation  \n",
    "    - Ziel: Erkennung einer bestimmten Ziffer (z.B. 7).  \n",
    "    - Implementierung der Aktivierungsfunktionen (ReLU, Sigmoid), der Kostenfunktion (Binary Cross Entropy) sowie des Trainingsloops.\n",
    " \n",
    " 4. **Aufgabe 4:** Training des Netzwerks für verschiedene Hyperparameter  \n",
    "    - Variation von Lernraten und Hidden-Layer-Größen und Auswertung anhand von Kosten- und Evaluationsmetriken.\n",
    " \n",
    " 5. **Aufgabe 5:** Erweiterung des Netzwerks auf drei Hidden Layers für die Mehrklassenklassifikation  \n",
    "    - Implementierung eines Netzwerks mit Mini-Batch Training, Softmax-Aktivierung, Cross-Entropy-Kostenfunktion und Experimenten zu Lernraten und Layer-Größen.\n",
    " \n",
    " *Hinweis:* Für Aufgabe 1 wird `torchvision` verwendet – in den folgenden Aufgaben kommen nur noch `numpy`, `matplotlib` und Python Built-ins zum Einsatz.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 1: MNIST-Datensatz laden und erkunden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainingsdaten:  60000\n",
      "Testdaten:  10000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Für Aufgabe 1: Verwendung von torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Definieren des Transforms: Wir wandeln das Bild in einen Tensor um.\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# MNIST-Datensatz laden (Training und Test)\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset  = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "print(\"Trainingsdaten: \", len(train_dataset))\n",
    "print(\"Testdaten: \", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x300 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualisieren Sie einige Beispiele aus dem Trainingsdatensatz\n",
    "fig, axes = plt.subplots(1, 5, figsize=(12,3))\n",
    "for i, ax in enumerate(axes):\n",
    "    img, label = train_dataset[i]\n",
    "    ax.imshow(img.squeeze(), cmap='gray')\n",
    "    ax.set_title(f\"Label: {label}\")\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hilfsfunktion: Konvertieren des Datasets in numpy Arrays**\n",
    "\n",
    "Für die nachfolgenden Aufgaben (2–5) extrahieren wir die Bilder und Labels aus dem MNIST-Datensatz und wandeln sie in numpy Arrays um.  \n",
    "Dabei wird das Bild (1x28x28) zu einem Vektor (28*28 = 784) umgeformt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (60000, 784)\n",
      "y_train_bin shape: (60000,)\n"
     ]
    }
   ],
   "source": [
    "def dataset_to_numpy(dataset):\n",
    "    X = []\n",
    "    y = []\n",
    "    for img, label in dataset:\n",
    "        X.append(img.numpy().reshape(-1))\n",
    "        y.append(label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_train, y_train = dataset_to_numpy(train_dataset)\n",
    "X_test, y_test   = dataset_to_numpy(test_dataset)\n",
    "\n",
    "# Für die binäre Klassifikation (Aufgabe 3–4) definieren wir:\n",
    "# Ziel: Erkenne die Ziffer 7 (1, falls Label == 7, sonst 0)\n",
    "y_train_bin = (y_train == 7).astype(np.int32)\n",
    "y_test_bin  = (y_test == 7).astype(np.int32)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train_bin shape:\", y_train_bin.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 2: Implementierung eines linearen Layers\n",
    "\n",
    " Wir erstellen eine Klasse `LinearLayer`, die folgende Methoden besitzt:\n",
    " \n",
    " - `forward(x)`: Berechnet die Ausgabe \\( z = xW^T + b \\)\n",
    " - `backward(d_out)`: Berechnet die Gradienten (basierend auf dem gespeicherten Input)  \n",
    " - `update(learning_rate)`: Aktualisiert die Parameter mittels Gradient Descent\n",
    " \n",
    "Anschließend folgt ein Unittest, der anhand eines kleinen Beispiels (mit handberechneten Ergebnissen) die Funktionsweise überprüft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        # Initialisierung der Gewichte (W) und Bias (b)\n",
    "        # He-Initialization für ReLU: Skaliere die zufälligen Gewichte mit sqrt(2/input_dim)\n",
    "        self.W = np.random.randn(output_dim, input_dim) * np.sqrt(2.0 / input_dim)\n",
    "        self.b = np.zeros((output_dim, 1))\n",
    "        # Platzhalter für Zwischenergebnisse\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, input_dim)\n",
    "        self.x = x  # für Backward-Pass speichern\n",
    "        # Berechnung: x * W^T + b^T\n",
    "        out = x.dot(self.W.T) + self.b.T\n",
    "        return out\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        # d_out: (batch_size, output_dim)\n",
    "        batch_size = self.x.shape[0]\n",
    "        self.dW = (d_out.T.dot(self.x)) / batch_size\n",
    "        self.db = (np.sum(d_out, axis=0, keepdims=True).T) / batch_size\n",
    "        # Gradient bezüglich des Inputs\n",
    "        dx = d_out.dot(self.W)\n",
    "        return dx\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        self.W -= learning_rate * self.dW\n",
    "        self.b -= learning_rate * self.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unittest für LinearLayer**\n",
    " \n",
    " Wir definieren einen Test für ein Layer mit 2 Knoten und 2 Datensätzen (jeweils 2 floats).\n",
    " \n",
    " Gegebene Testwerte:\n",
    " \n",
    " - **Input:**  \n",
    "\n",
    "   $ X = \\begin{pmatrix} 1.0 & 2.0 \\\\ 3.0 & 4.0 \\end{pmatrix} $\n",
    " \n",
    " - **Initiale Parameter:**  \n",
    "\n",
    "   $ W = \\begin{pmatrix} 0.1 & 0.2 \\\\ 0.3 & 0.4 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 0.5 \\\\ 0.6 \\end{pmatrix} $\n",
    " \n",
    " - **Forward-Berechnung:**  \n",
    "\n",
    "    $ X \\cdot W^T = \\begin{pmatrix} 0.5 & 1.1 \\\\ 1.1 & 2.5 \\end{pmatrix}  $\n",
    "\n",
    "    $ \\text{out} = X \\cdot W^T + b^T = \\begin{pmatrix} 1.0 & 1.7 \\\\ 1.6 & 3.1 \\end{pmatrix} $\n",
    " \n",
    " - **Backward-Pass:**  \n",
    "\n",
    "   Mit $ d\\_out = \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix} $ und Batchgröße 2,  \n",
    "\n",
    "    $ dW = \\begin{pmatrix} 2 & 3 \\\\ 2 & 3 \\end{pmatrix} $ und $ db = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} $.\n",
    " \n",
    " - **Update (mit Lernrate 0.1):**  \n",
    "\n",
    "   Neue Parameter:  \n",
    "\n",
    "   $ W_{neu} = \\begin{pmatrix} -0.1 & -0.1 \\\\ 0.1 & 0.1 \\end{pmatrix} $, \n",
    "   $ b_{neu} = \\begin{pmatrix} 0.4 \\\\ 0.5 \\end{pmatrix} $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unittest für LinearLayer erfolgreich bestanden.\n"
     ]
    }
   ],
   "source": [
    "def test_linear_layer():\n",
    "    # Feste Testwerte\n",
    "    X_test_input = np.array([[1.0, 2.0],\n",
    "                             [3.0, 4.0]])\n",
    "    \n",
    "    # Erstellen eines Layers und manuelles Setzen der Parameter\n",
    "    layer = LinearLayer(input_dim=2, output_dim=2)\n",
    "    layer.W = np.array([[0.1, 0.2],\n",
    "                        [0.3, 0.4]])\n",
    "    layer.b = np.array([[0.5],\n",
    "                        [0.6]])\n",
    "    \n",
    "    # Forward-Pass\n",
    "    out = layer.forward(X_test_input)\n",
    "    expected_out = np.array([[1.0, 1.7],\n",
    "                             [1.6, 3.1]])\n",
    "    assert np.allclose(out, expected_out, atol=1e-6), f\"Forward-Pass fehlerhaft: {out} != {expected_out}\"\n",
    "    \n",
    "    # Backward-Pass\n",
    "    d_out = np.ones((2,2))\n",
    "    layer.backward(d_out)\n",
    "    # Erwartete Gradienten ohne zusätzliche Division, da backward() bereits durch die Batchgröße teilt.\n",
    "    expected_dW = np.array([[2, 3],\n",
    "                            [2, 3]])\n",
    "    expected_db = np.array([[1],\n",
    "                            [1]])\n",
    "    \n",
    "    assert np.allclose(layer.dW, expected_dW, atol=1e-6), f\"dW fehlerhaft: {layer.dW} != {expected_dW}\"\n",
    "    assert np.allclose(layer.db, expected_db, atol=1e-6), f\"db fehlerhaft: {layer.db} != {expected_db}\"\n",
    "    \n",
    "    # Parameter-Update mit Lernrate 0.1\n",
    "    learning_rate = 0.1\n",
    "    layer.update(learning_rate)\n",
    "    expected_W_new = np.array([[0.1, 0.2],\n",
    "                               [0.3, 0.4]]) - 0.1 * expected_dW\n",
    "    expected_b_new = np.array([[0.5],\n",
    "                               [0.6]]) - 0.1 * expected_db\n",
    "    assert np.allclose(layer.W, expected_W_new, atol=1e-6), f\"W update fehlerhaft: {layer.W} != {expected_W_new}\"\n",
    "    assert np.allclose(layer.b, expected_b_new, atol=1e-6), f\"b update fehlerhaft: {layer.b} != {expected_b_new}\"\n",
    "    \n",
    "    print(\"Unittest für LinearLayer erfolgreich bestanden.\")\n",
    "\n",
    "# Test ausführen\n",
    "test_linear_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 3: Einfaches neuronales Netzwerk ( binäre Klassifikation )\n",
    " \n",
    " **Ziel:** Ein Netzwerk, das anhand eines versteckten (Hidden) linearen Layers die Ziffer 7 (1) von allen anderen (0) unterscheidet.\n",
    " \n",
    " Wir implementieren:\n",
    " \n",
    " - **Aktivierungsfunktionen:** ReLU (für den Hidden Layer) und Sigmoid (für den Output)  \n",
    " - **Kostenfunktion:** Binary Cross Entropy (BCE)  \n",
    " - **Trainingsloop:** Vollständiger Batch-Training (ohne Mini-Batches)\n",
    " \n",
    " **Mathematische Definition der Binary Cross Entropy:**  \n",
    "\n",
    " $\n",
    "L = -\\frac{1}{N}\\sum_{i=1}^{N} \\Big[y_i \\log(\\hat{y}_i) + (1-y_i) \\log(1-\\hat{y}_i)\\Big]\n",
    " $\n",
    " \n",
    " _Hinweis:_ Für die Ableitungen nutzen wir die Eigenschaft, dass bei Verwendung von Sigmoid in Kombination mit der BCE der Gradiententerm vereinfacht zu $\\hat{y} - y$ wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aktivierungsfunktionen und deren Ableitungen\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN:\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        # Initialisierung der beiden linearen Layer\n",
    "        self.layer1 = LinearLayer(input_dim, hidden_dim)\n",
    "        self.layer2 = LinearLayer(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Vorwärtsdurchlauf:\n",
    "        # Hidden Layer: Linearer Transform gefolgt von ReLU\n",
    "        self.z1 = self.layer1.forward(x)\n",
    "        self.a1 = relu(self.z1)\n",
    "        # Output Layer: Linearer Transform gefolgt von Sigmoid\n",
    "        self.z2 = self.layer2.forward(self.a1)\n",
    "        self.a2 = sigmoid(self.z2)\n",
    "        return self.a2\n",
    "    \n",
    "    def backward(self, x, y, output, weight_pos=3):\n",
    "        \"\"\"\n",
    "        Berechnet den Gradienten unter Berücksichtigung einer Gewichtung für positive Beispiele.\n",
    "        weight_pos: Gewichtungsfaktor für Beispiele, bei denen y == 1 (z.B. Ziffer 7).\n",
    "        \"\"\"\n",
    "        # Erzeuge einen Gewichtungsfaktor: Bei y==1 wird weight_pos verwendet, sonst 1.\n",
    "        weight_factor = np.where(y.reshape(-1, 1) == 1, weight_pos, 1)\n",
    "        # Angepasster Fehler: (output - y) multipliziert mit dem Gewichtungsfaktor\n",
    "        dz2 = (output - y.reshape(-1, 1)) * weight_factor\n",
    "        # Backward-Pass durch den Output-Layer; liefert den Gradienten bezüglich der Hidden-Aktivierung zurück\n",
    "        da1 = self.layer2.backward(dz2)\n",
    "        # Gradient des Hidden Layers: Multiplikation mit der Ableitung der ReLU-Funktion\n",
    "        dz1 = da1 * relu_derivative(self.z1)\n",
    "        # Backward-Pass durch den Hidden Layer\n",
    "        self.layer1.backward(dz1)\n",
    "    \n",
    "    def update(self, learning_rate):\n",
    "        # Aktualisierung der Parameter beider Layer mittels Gradient Descent\n",
    "        self.layer1.update(learning_rate)\n",
    "        self.layer2.update(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kostenfunktion (Binary Cross Entropy) und Accuracy\n",
    "def compute_loss(y_true, y_pred):\n",
    "    epsilon = 1e-8\n",
    "    loss = -np.mean(y_true * np.log(y_pred + epsilon) + (1 - y_true) * np.log(1 - y_pred + epsilon))\n",
    "    return loss\n",
    "\n",
    "# Gewichteter Loss, um die Klassenungleichheit ( 7 vs. Rest der Ziffern ) auszugleichen\n",
    "def compute_loss_weighted(y_true, y_pred, weight_pos=3):\n",
    "    epsilon = 1e-8\n",
    "    loss = -np.mean(weight_pos * y_true * np.log(y_pred + epsilon) + (1 - y_true) * np.log(1 - y_pred + epsilon))\n",
    "    return loss\n",
    "\n",
    "def compute_accuracy(y_true, y_pred):\n",
    "    preds = (y_pred >= 0.5).astype(int).flatten()\n",
    "    return np.mean(preds == y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainingsloop\n",
    "def train_binary_nn_weighted(model, X, y, epochs, learning_rate, weight_pos=3):\n",
    "    loss_history = []\n",
    "    acc_history = []\n",
    "    for epoch in range(epochs):\n",
    "        # Vorwärtsdurchlauf\n",
    "        output = model.forward(X)\n",
    "        # Berechne den gewichteten Loss\n",
    "        loss = compute_loss_weighted(y, output, weight_pos)\n",
    "        # Berechne die Accuracy (diese bleibt natürlich weiterhin über den Schwellenwert definiert)\n",
    "        acc = compute_accuracy(y, output)\n",
    "        loss_history.append(loss)\n",
    "        acc_history.append(acc)\n",
    "        \n",
    "        # Rückwärtsdurchlauf mit Gewichtung im Gradienten\n",
    "        model.backward(X, y, output, weight_pos)\n",
    "        model.update(learning_rate)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} – Loss: {loss:.4f}, Accuracy: {acc*100:.2f}%\")\n",
    "    return loss_history, acc_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training des binären Netzwerks (Aufgabe 3 & 4)\n",
    " \n",
    " Wir wählen:\n",
    " - Input-Dimension: 784  \n",
    " - Hidden-Dimension: 32 (wird später variiert)  \n",
    " - Output-Dimension: 1  \n",
    " - Lernrate: 0.1 (wird später variiert)\n",
    " \n",
    " Die Daten sind bereits vorbereitet:\n",
    " \n",
    " `X_train` (Trainingsbilder, flach), `y_train_bin` (Labels: 1, falls Ziffer 7, sonst 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 – Loss: 1.0592, Accuracy: 18.52%\n",
      "Epoch 2/10 – Loss: 0.9770, Accuracy: 27.90%\n",
      "Epoch 3/10 – Loss: 0.9152, Accuracy: 40.08%\n",
      "Epoch 4/10 – Loss: 0.8696, Accuracy: 54.52%\n",
      "Epoch 5/10 – Loss: 0.8364, Accuracy: 66.06%\n",
      "Epoch 6/10 – Loss: 0.8123, Accuracy: 73.38%\n",
      "Epoch 7/10 – Loss: 0.7949, Accuracy: 78.96%\n",
      "Epoch 8/10 – Loss: 0.7823, Accuracy: 82.68%\n",
      "Epoch 9/10 – Loss: 0.7731, Accuracy: 85.24%\n",
      "Epoch 10/10 – Loss: 0.7665, Accuracy: 86.62%\n",
      "\n",
      "Test Loss: 0.7424, Test Accuracy: 88.03%\n"
     ]
    }
   ],
   "source": [
    "# Für die Experimente verwenden wir nur einen Teil der Trainingsdaten (z.B. 5000 Beispiele)\n",
    "sample_size = 5000\n",
    "X_train_sample = X_train[:sample_size]\n",
    "y_train_bin_sample = y_train_bin[:sample_size]\n",
    "\n",
    "# Hyperparameter für das initiale Training\n",
    "input_dim = 784\n",
    "hidden_dim = 32\n",
    "output_dim = 1\n",
    "epochs = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Initialisiere das Netzwerk\n",
    "binary_nn = SimpleNN(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Training\n",
    "loss_hist, acc_hist = train_binary_nn_weighted(binary_nn, X_train_sample, y_train_bin_sample, epochs, learning_rate)\n",
    "\n",
    "# Evaluierung auf Testdaten\n",
    "test_output = binary_nn.forward(X_test)\n",
    "test_loss = compute_loss_weighted(y_test_bin, test_output)\n",
    "test_acc = compute_accuracy(y_test_bin, test_output)\n",
    "print(f\"\\nTest Loss: {test_loss:.4f}, Test Accuracy: {test_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 4: Experimente mit unterschiedlichen Hyperparametern\n",
    " \n",
    " Wir variieren:\n",
    " - **Lernrate:** 0.01, 0.1, 1.0  \n",
    " - **Hidden-Layer-Größe:** 4, 8, 16\n",
    " \n",
    " Für jede Kombination trainieren wir 10 Epochen und protokollieren den Test-Accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training mit Lernrate=0.01 und Hidden-Dimension=4\n",
      "Epoch 1/10 – Loss: 0.7898, Accuracy: 83.46%\n",
      "Epoch 2/10 – Loss: 0.7860, Accuracy: 86.34%\n",
      "Epoch 3/10 – Loss: 0.7822, Accuracy: 86.78%\n",
      "Epoch 4/10 – Loss: 0.7783, Accuracy: 87.16%\n",
      "Epoch 5/10 – Loss: 0.7744, Accuracy: 87.58%\n",
      "Epoch 6/10 – Loss: 0.7705, Accuracy: 87.84%\n",
      "Epoch 7/10 – Loss: 0.7668, Accuracy: 88.10%\n",
      "Epoch 8/10 – Loss: 0.7632, Accuracy: 88.28%\n",
      "Epoch 9/10 – Loss: 0.7598, Accuracy: 88.44%\n",
      "Epoch 10/10 – Loss: 0.7566, Accuracy: 88.48%\n",
      "Test Accuracy: 89.33%\n",
      "\n",
      "Training mit Lernrate=0.01 und Hidden-Dimension=8\n",
      "Epoch 1/10 – Loss: 0.8518, Accuracy: 47.16%\n",
      "Epoch 2/10 – Loss: 0.8251, Accuracy: 59.72%\n",
      "Epoch 3/10 – Loss: 0.8050, Accuracy: 69.20%\n",
      "Epoch 4/10 – Loss: 0.7897, Accuracy: 75.04%\n",
      "Epoch 5/10 – Loss: 0.7779, Accuracy: 79.48%\n",
      "Epoch 6/10 – Loss: 0.7687, Accuracy: 82.32%\n",
      "Epoch 7/10 – Loss: 0.7613, Accuracy: 84.34%\n",
      "Epoch 8/10 – Loss: 0.7554, Accuracy: 85.52%\n",
      "Epoch 9/10 – Loss: 0.7507, Accuracy: 86.30%\n",
      "Epoch 10/10 – Loss: 0.7469, Accuracy: 86.72%\n",
      "Test Accuracy: 88.19%\n",
      "\n",
      "Training mit Lernrate=0.01 und Hidden-Dimension=16\n",
      "Epoch 1/10 – Loss: 0.8831, Accuracy: 42.30%\n",
      "Epoch 2/10 – Loss: 0.8499, Accuracy: 52.76%\n",
      "Epoch 3/10 – Loss: 0.8255, Accuracy: 62.68%\n",
      "Epoch 4/10 – Loss: 0.8072, Accuracy: 70.52%\n",
      "Epoch 5/10 – Loss: 0.7932, Accuracy: 75.44%\n",
      "Epoch 6/10 – Loss: 0.7823, Accuracy: 78.74%\n",
      "Epoch 7/10 – Loss: 0.7737, Accuracy: 81.62%\n",
      "Epoch 8/10 – Loss: 0.7668, Accuracy: 83.56%\n",
      "Epoch 9/10 – Loss: 0.7613, Accuracy: 84.50%\n",
      "Epoch 10/10 – Loss: 0.7569, Accuracy: 85.18%\n",
      "Test Accuracy: 87.41%\n",
      "\n",
      "Training mit Lernrate=0.1 und Hidden-Dimension=4\n",
      "Epoch 1/10 – Loss: 0.9046, Accuracy: 17.58%\n",
      "Epoch 2/10 – Loss: 0.8425, Accuracy: 58.10%\n",
      "Epoch 3/10 – Loss: 0.8112, Accuracy: 83.02%\n",
      "Epoch 4/10 – Loss: 0.7807, Accuracy: 88.74%\n",
      "Epoch 5/10 – Loss: 0.7565, Accuracy: 89.12%\n",
      "Epoch 6/10 – Loss: 0.7414, Accuracy: 89.10%\n",
      "Epoch 7/10 – Loss: 0.7338, Accuracy: 89.18%\n",
      "Epoch 8/10 – Loss: 0.7320, Accuracy: 89.18%\n",
      "Epoch 9/10 – Loss: 0.7341, Accuracy: 89.28%\n",
      "Epoch 10/10 – Loss: 0.7391, Accuracy: 89.72%\n",
      "Test Accuracy: 91.28%\n",
      "\n",
      "Training mit Lernrate=0.1 und Hidden-Dimension=8\n",
      "Epoch 1/10 – Loss: 0.9952, Accuracy: 12.36%\n",
      "Epoch 2/10 – Loss: 0.8447, Accuracy: 59.12%\n",
      "Epoch 3/10 – Loss: 0.8313, Accuracy: 80.56%\n",
      "Epoch 4/10 – Loss: 0.8207, Accuracy: 87.36%\n",
      "Epoch 5/10 – Loss: 0.8090, Accuracy: 89.08%\n",
      "Epoch 6/10 – Loss: 0.7916, Accuracy: 90.36%\n",
      "Epoch 7/10 – Loss: 0.7606, Accuracy: 91.60%\n",
      "Epoch 8/10 – Loss: 0.7419, Accuracy: 92.68%\n",
      "Epoch 9/10 – Loss: 0.7450, Accuracy: 93.26%\n",
      "Epoch 10/10 – Loss: 0.7565, Accuracy: 94.16%\n",
      "Test Accuracy: 95.32%\n",
      "\n",
      "Training mit Lernrate=0.1 und Hidden-Dimension=16\n",
      "Epoch 1/10 – Loss: 0.8873, Accuracy: 34.64%\n",
      "Epoch 2/10 – Loss: 0.7568, Accuracy: 89.94%\n",
      "Epoch 3/10 – Loss: 0.7421, Accuracy: 89.88%\n",
      "Epoch 4/10 – Loss: 0.7538, Accuracy: 90.30%\n",
      "Epoch 5/10 – Loss: 0.7684, Accuracy: 91.58%\n",
      "Epoch 6/10 – Loss: 0.7842, Accuracy: 92.94%\n",
      "Epoch 7/10 – Loss: 0.8016, Accuracy: 94.06%\n",
      "Epoch 8/10 – Loss: 0.8199, Accuracy: 94.66%\n",
      "Epoch 9/10 – Loss: 0.8388, Accuracy: 95.38%\n",
      "Epoch 10/10 – Loss: 0.8582, Accuracy: 95.92%\n",
      "Test Accuracy: 96.09%\n",
      "\n",
      "Training mit Lernrate=1.0 und Hidden-Dimension=4\n",
      "Epoch 1/10 – Loss: 0.8788, Accuracy: 11.00%\n",
      "Epoch 2/10 – Loss: 0.7734, Accuracy: 89.20%\n",
      "Epoch 3/10 – Loss: 0.7449, Accuracy: 89.46%\n",
      "Epoch 4/10 – Loss: 0.7535, Accuracy: 91.68%\n",
      "Epoch 5/10 – Loss: 0.7357, Accuracy: 93.62%\n",
      "Epoch 6/10 – Loss: 1.0162, Accuracy: 69.14%\n",
      "Epoch 7/10 – Loss: 1.5859, Accuracy: 89.00%\n",
      "Epoch 8/10 – Loss: 0.7129, Accuracy: 89.00%\n",
      "Epoch 9/10 – Loss: 0.7155, Accuracy: 89.00%\n",
      "Epoch 10/10 – Loss: 0.8014, Accuracy: 86.80%\n",
      "Test Accuracy: 89.72%\n",
      "\n",
      "Training mit Lernrate=1.0 und Hidden-Dimension=8\n",
      "Epoch 1/10 – Loss: 0.7595, Accuracy: 84.02%\n",
      "Epoch 2/10 – Loss: 1.1749, Accuracy: 89.02%\n",
      "Epoch 3/10 – Loss: 0.8923, Accuracy: 70.18%\n",
      "Epoch 4/10 – Loss: 3.8180, Accuracy: 89.00%\n",
      "Epoch 5/10 – Loss: 0.8369, Accuracy: 58.62%\n",
      "Epoch 6/10 – Loss: 1.2876, Accuracy: 95.30%\n",
      "Epoch 7/10 – Loss: 1.0599, Accuracy: 94.44%\n",
      "Epoch 8/10 – Loss: 1.6065, Accuracy: 96.88%\n",
      "Epoch 9/10 – Loss: 1.4235, Accuracy: 90.98%\n",
      "Epoch 10/10 – Loss: 2.3930, Accuracy: 93.58%\n",
      "Test Accuracy: 44.23%\n",
      "\n",
      "Training mit Lernrate=1.0 und Hidden-Dimension=16\n",
      "Epoch 1/10 – Loss: 0.9091, Accuracy: 30.14%\n",
      "Epoch 2/10 – Loss: 0.8697, Accuracy: 89.06%\n",
      "Epoch 3/10 – Loss: 2.9572, Accuracy: 11.12%\n",
      "Epoch 4/10 – Loss: 3.0018, Accuracy: 89.00%\n",
      "Epoch 5/10 – Loss: 0.7235, Accuracy: 89.00%\n",
      "Epoch 6/10 – Loss: 0.7234, Accuracy: 89.00%\n",
      "Epoch 7/10 – Loss: 0.7726, Accuracy: 91.12%\n",
      "Epoch 8/10 – Loss: 1.0290, Accuracy: 96.54%\n",
      "Epoch 9/10 – Loss: 1.2162, Accuracy: 95.82%\n",
      "Epoch 10/10 – Loss: 1.6162, Accuracy: 96.58%\n",
      "Test Accuracy: 50.89%\n",
      "\n",
      "Ergebnisse der Hyperparameter-Experimente:\n",
      "Lernrate 0.01, Hidden 4 -> Accuracy: 89.33%\n",
      "Lernrate 0.01, Hidden 8 -> Accuracy: 88.19%\n",
      "Lernrate 0.01, Hidden 16 -> Accuracy: 87.41%\n",
      "Lernrate 0.1, Hidden 4 -> Accuracy: 91.28%\n",
      "Lernrate 0.1, Hidden 8 -> Accuracy: 95.32%\n",
      "Lernrate 0.1, Hidden 16 -> Accuracy: 96.09%\n",
      "Lernrate 1.0, Hidden 4 -> Accuracy: 89.72%\n",
      "Lernrate 1.0, Hidden 8 -> Accuracy: 44.23%\n",
      "Lernrate 1.0, Hidden 16 -> Accuracy: 50.89%\n"
     ]
    }
   ],
   "source": [
    "# Für die Experimente verwenden wir nur einen Teil der Trainingsdaten (z.B. 5000 Beispiele)\n",
    "sample_size = 5000\n",
    "X_train_sample = X_train[:sample_size]\n",
    "y_train_bin_sample = y_train_bin[:sample_size]\n",
    "\n",
    "learning_rates = [0.01, 0.1, 1.0]\n",
    "hidden_sizes = [4, 8, 16]\n",
    "results = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for h_dim in hidden_sizes:\n",
    "        print(f\"\\nTraining mit Lernrate={lr} und Hidden-Dimension={h_dim}\")\n",
    "        model = SimpleNN(input_dim, h_dim, output_dim)\n",
    "        train_binary_nn_weighted(model, X_train_sample, y_train_bin_sample, epochs=10, learning_rate=lr)\n",
    "        test_out = model.forward(X_test)\n",
    "        acc = compute_accuracy(y_test_bin, test_out)\n",
    "        results[(lr, h_dim)] = acc\n",
    "        print(f\"Test Accuracy: {acc*100:.2f}%\")\n",
    "\n",
    "print(\"\\nErgebnisse der Hyperparameter-Experimente:\")\n",
    "for key, acc in results.items():\n",
    "    print(f\"Lernrate {key[0]}, Hidden {key[1]} -> Accuracy: {acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 5: Erweiterung auf ein Netzwerk für Mehrklassenklassifikation\n",
    " \n",
    " **Ziel:** Klassifizierung aller 10 Ziffern (0–9)  \n",
    "\n",
    " **Architektur:**  \n",
    "\n",
    " - 3 Hidden Layers (alle mit gleicher Größe)  \n",
    " - Output Layer mit 10 Knoten\n",
    " \n",
    " **Kostenfunktion:** Softmax-Cross-Entropy  \n",
    " \n",
    " **Mathematische Definition (Softmax und Cross-Entropy):**\n",
    " \n",
    " - **Softmax:**  \n",
    "\n",
    "$\n",
    " \\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}}\n",
    "$\n",
    " \n",
    " - **Cross-Entropy:**  \n",
    "\n",
    "$\n",
    " L = -\\frac{1}{N}\\sum_{i=1}^{N} \\sum_{c=1}^{C} y_{i,c} \\log(\\hat{y}_{i,c})\n",
    "$\n",
    " \n",
    " **Warum Mini-Batches?**  \n",
    "\n",
    " - Reduzierung des Speicherbedarfs  \n",
    " - Bessere Generalisierung durch stochastische Gradientenabstiege  \n",
    " - Effiziente Nutzung von Rechenressourcen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax-Funktion\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "# Cross-Entropy Verlust\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    epsilon = 1e-8\n",
    "    loss = -np.mean(np.sum(y_true * np.log(y_pred + epsilon), axis=1))\n",
    "    return loss\n",
    "\n",
    "# Accuracy für Mehrklassenklassifikation\n",
    "def compute_accuracy_multiclass(y_true, y_pred):\n",
    "    preds = np.argmax(y_pred, axis=1)\n",
    "    true_labels = np.argmax(y_true, axis=1)\n",
    "    return np.mean(preds == true_labels)\n",
    "\n",
    "# One-hot Encoding\n",
    "def one_hot_encode(y, num_classes=10):\n",
    "    return np.eye(num_classes)[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mehrschichtiges Netzwerk mit 3 Hidden Layers und 10 Outputs\n",
    "class MultiLayerNN:\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim=10):\n",
    "        self.layer1 = LinearLayer(input_dim, hidden_dim)\n",
    "        self.layer2 = LinearLayer(hidden_dim, hidden_dim)\n",
    "        self.layer3 = LinearLayer(hidden_dim, hidden_dim)\n",
    "        self.layer4 = LinearLayer(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.z1 = self.layer1.forward(x)\n",
    "        self.a1 = relu(self.z1)\n",
    "        self.z2 = self.layer2.forward(self.a1)\n",
    "        self.a2 = relu(self.z2)\n",
    "        self.z3 = self.layer3.forward(self.a2)\n",
    "        self.a3 = relu(self.z3)\n",
    "        self.z4 = self.layer4.forward(self.a3)\n",
    "        self.a4 = softmax(self.z4)\n",
    "        return self.a4\n",
    "    \n",
    "    def backward(self, x, y, output):\n",
    "        m = y.shape[0]\n",
    "        # Für Softmax mit Cross-Entropy vereinfacht sich der Gradient:\n",
    "        dz4 = (output - y) / m\n",
    "        da3 = self.layer4.backward(dz4)\n",
    "        dz3 = da3 * relu_derivative(self.z3)\n",
    "        da2 = self.layer3.backward(dz3)\n",
    "        dz2 = da2 * relu_derivative(self.z2)\n",
    "        da1 = self.layer2.backward(dz2)\n",
    "        dz1 = da1 * relu_derivative(self.z1)\n",
    "        self.layer1.backward(dz1)\n",
    "        \n",
    "    def update(self, learning_rate):\n",
    "        self.layer1.update(learning_rate)\n",
    "        self.layer2.update(learning_rate)\n",
    "        self.layer3.update(learning_rate)\n",
    "        self.layer4.update(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainingsloop mit Mini-Batch Training\n",
    "def train_multiclass_nn(model, X, y, epochs, learning_rate, batch_size=64):\n",
    "    loss_history = []\n",
    "    acc_history = []\n",
    "    num_samples = X.shape[0]\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle der Daten\n",
    "        indices = np.arange(num_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        X_shuffled = X[indices]\n",
    "        y_shuffled = y[indices]\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        num_batches = int(np.ceil(num_samples / batch_size))\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "            start = i * batch_size\n",
    "            end = min(start + batch_size, num_samples)\n",
    "            X_batch = X_shuffled[start:end]\n",
    "            y_batch = y_shuffled[start:end]\n",
    "            \n",
    "            # Forward-Pass\n",
    "            output = model.forward(X_batch)\n",
    "            loss = cross_entropy_loss(y_batch, output)\n",
    "            epoch_loss += loss\n",
    "            \n",
    "            # Backward-Pass\n",
    "            model.backward(X_batch, y_batch, output)\n",
    "            \n",
    "            # Update der Parameter\n",
    "            model.update(learning_rate)\n",
    "        \n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        # Evaluierung auf dem gesamten Trainingssatz\n",
    "        output_train = model.forward(X)\n",
    "        acc = compute_accuracy_multiclass(y, output_train)\n",
    "        loss_history.append(avg_loss)\n",
    "        acc_history.append(acc)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} – Loss: {avg_loss:.4f}, Accuracy: {acc*100:.2f}%\")\n",
    "    return loss_history, acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vorbereitung der Daten für Mehrklassenklassifikation\n",
    "# Wir verwenden nun die Original-Labels (0-9) und one-hot encodieren diese.\n",
    "y_train_oh = one_hot_encode(y_train, num_classes=10)\n",
    "y_test_oh  = one_hot_encode(y_test, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training (Mehrklassen) mit Lernrate=0.001 und Hidden-Dimension=16\n",
      "Epoch 1/100 – Loss: 2.3668, Accuracy: 11.56%\n",
      "Epoch 2/100 – Loss: 2.3653, Accuracy: 11.56%\n",
      "Epoch 3/100 – Loss: 2.3639, Accuracy: 11.60%\n",
      "Epoch 4/100 – Loss: 2.3627, Accuracy: 11.58%\n",
      "Epoch 5/100 – Loss: 2.3610, Accuracy: 11.62%\n",
      "Epoch 6/100 – Loss: 2.3580, Accuracy: 11.64%\n",
      "Epoch 7/100 – Loss: 2.3580, Accuracy: 11.62%\n",
      "Epoch 8/100 – Loss: 2.3586, Accuracy: 11.66%\n",
      "Epoch 9/100 – Loss: 2.3566, Accuracy: 11.70%\n",
      "Epoch 10/100 – Loss: 2.3542, Accuracy: 11.72%\n",
      "Epoch 11/100 – Loss: 2.3546, Accuracy: 11.76%\n",
      "Epoch 12/100 – Loss: 2.3524, Accuracy: 11.78%\n",
      "Epoch 13/100 – Loss: 2.3508, Accuracy: 11.74%\n",
      "Epoch 14/100 – Loss: 2.3514, Accuracy: 11.76%\n",
      "Epoch 15/100 – Loss: 2.3492, Accuracy: 11.74%\n",
      "Epoch 16/100 – Loss: 2.3470, Accuracy: 11.84%\n",
      "Epoch 17/100 – Loss: 2.3455, Accuracy: 11.86%\n",
      "Epoch 18/100 – Loss: 2.3443, Accuracy: 11.90%\n",
      "Epoch 19/100 – Loss: 2.3465, Accuracy: 11.96%\n",
      "Epoch 20/100 – Loss: 2.3436, Accuracy: 12.00%\n",
      "Epoch 21/100 – Loss: 2.3404, Accuracy: 12.06%\n",
      "Epoch 22/100 – Loss: 2.3417, Accuracy: 12.10%\n",
      "Epoch 23/100 – Loss: 2.3377, Accuracy: 12.08%\n",
      "Epoch 24/100 – Loss: 2.3395, Accuracy: 12.02%\n",
      "Epoch 25/100 – Loss: 2.3368, Accuracy: 12.12%\n",
      "Epoch 26/100 – Loss: 2.3338, Accuracy: 12.14%\n",
      "Epoch 27/100 – Loss: 2.3343, Accuracy: 12.16%\n",
      "Epoch 28/100 – Loss: 2.3346, Accuracy: 12.18%\n",
      "Epoch 29/100 – Loss: 2.3336, Accuracy: 12.14%\n",
      "Epoch 30/100 – Loss: 2.3303, Accuracy: 12.20%\n",
      "Epoch 31/100 – Loss: 2.3309, Accuracy: 12.20%\n",
      "Epoch 32/100 – Loss: 2.3302, Accuracy: 12.26%\n",
      "Epoch 33/100 – Loss: 2.3299, Accuracy: 12.26%\n",
      "Epoch 34/100 – Loss: 2.3259, Accuracy: 12.28%\n",
      "Epoch 35/100 – Loss: 2.3280, Accuracy: 12.38%\n",
      "Epoch 36/100 – Loss: 2.3256, Accuracy: 12.42%\n",
      "Epoch 37/100 – Loss: 2.3221, Accuracy: 12.42%\n",
      "Epoch 38/100 – Loss: 2.3234, Accuracy: 12.40%\n",
      "Epoch 39/100 – Loss: 2.3212, Accuracy: 12.42%\n",
      "Epoch 40/100 – Loss: 2.3201, Accuracy: 12.40%\n",
      "Epoch 41/100 – Loss: 2.3184, Accuracy: 12.42%\n",
      "Epoch 42/100 – Loss: 2.3179, Accuracy: 12.46%\n",
      "Epoch 43/100 – Loss: 2.3165, Accuracy: 12.44%\n",
      "Epoch 44/100 – Loss: 2.3165, Accuracy: 12.44%\n",
      "Epoch 45/100 – Loss: 2.3150, Accuracy: 12.44%\n",
      "Epoch 46/100 – Loss: 2.3127, Accuracy: 12.40%\n",
      "Epoch 47/100 – Loss: 2.3149, Accuracy: 12.38%\n",
      "Epoch 48/100 – Loss: 2.3125, Accuracy: 12.34%\n",
      "Epoch 49/100 – Loss: 2.3123, Accuracy: 12.40%\n",
      "Epoch 50/100 – Loss: 2.3101, Accuracy: 12.38%\n",
      "Epoch 51/100 – Loss: 2.3104, Accuracy: 12.36%\n",
      "Epoch 52/100 – Loss: 2.3094, Accuracy: 12.36%\n",
      "Epoch 53/100 – Loss: 2.3071, Accuracy: 12.36%\n",
      "Epoch 54/100 – Loss: 2.3089, Accuracy: 12.34%\n",
      "Epoch 55/100 – Loss: 2.3083, Accuracy: 12.40%\n",
      "Epoch 56/100 – Loss: 2.3080, Accuracy: 12.44%\n",
      "Epoch 57/100 – Loss: 2.3049, Accuracy: 12.46%\n",
      "Epoch 58/100 – Loss: 2.3042, Accuracy: 12.46%\n",
      "Epoch 59/100 – Loss: 2.3031, Accuracy: 12.46%\n",
      "Epoch 60/100 – Loss: 2.3027, Accuracy: 12.48%\n",
      "Epoch 61/100 – Loss: 2.3018, Accuracy: 12.48%\n",
      "Epoch 62/100 – Loss: 2.3009, Accuracy: 12.48%\n",
      "Epoch 63/100 – Loss: 2.2994, Accuracy: 12.52%\n",
      "Epoch 64/100 – Loss: 2.2997, Accuracy: 12.56%\n",
      "Epoch 65/100 – Loss: 2.2993, Accuracy: 12.56%\n",
      "Epoch 66/100 – Loss: 2.2979, Accuracy: 12.60%\n",
      "Epoch 67/100 – Loss: 2.2961, Accuracy: 12.60%\n",
      "Epoch 68/100 – Loss: 2.2953, Accuracy: 12.62%\n",
      "Epoch 69/100 – Loss: 2.2943, Accuracy: 12.64%\n",
      "Epoch 70/100 – Loss: 2.2946, Accuracy: 12.66%\n",
      "Epoch 71/100 – Loss: 2.2938, Accuracy: 12.72%\n",
      "Epoch 72/100 – Loss: 2.2932, Accuracy: 12.72%\n",
      "Epoch 73/100 – Loss: 2.2921, Accuracy: 12.74%\n",
      "Epoch 74/100 – Loss: 2.2915, Accuracy: 12.72%\n",
      "Epoch 75/100 – Loss: 2.2923, Accuracy: 12.80%\n",
      "Epoch 76/100 – Loss: 2.2915, Accuracy: 12.84%\n",
      "Epoch 77/100 – Loss: 2.2906, Accuracy: 12.86%\n",
      "Epoch 78/100 – Loss: 2.2882, Accuracy: 12.82%\n",
      "Epoch 79/100 – Loss: 2.2878, Accuracy: 12.86%\n",
      "Epoch 80/100 – Loss: 2.2889, Accuracy: 12.88%\n",
      "Epoch 81/100 – Loss: 2.2869, Accuracy: 12.90%\n",
      "Epoch 82/100 – Loss: 2.2848, Accuracy: 12.90%\n",
      "Epoch 83/100 – Loss: 2.2876, Accuracy: 12.90%\n",
      "Epoch 84/100 – Loss: 2.2860, Accuracy: 12.86%\n",
      "Epoch 85/100 – Loss: 2.2846, Accuracy: 12.88%\n",
      "Epoch 86/100 – Loss: 2.2843, Accuracy: 12.96%\n",
      "Epoch 87/100 – Loss: 2.2824, Accuracy: 12.94%\n",
      "Epoch 88/100 – Loss: 2.2836, Accuracy: 12.94%\n",
      "Epoch 89/100 – Loss: 2.2829, Accuracy: 12.96%\n",
      "Epoch 90/100 – Loss: 2.2831, Accuracy: 13.04%\n",
      "Epoch 91/100 – Loss: 2.2813, Accuracy: 13.12%\n",
      "Epoch 92/100 – Loss: 2.2810, Accuracy: 13.12%\n",
      "Epoch 93/100 – Loss: 2.2799, Accuracy: 13.06%\n",
      "Epoch 94/100 – Loss: 2.2793, Accuracy: 13.12%\n",
      "Epoch 95/100 – Loss: 2.2800, Accuracy: 13.12%\n",
      "Epoch 96/100 – Loss: 2.2789, Accuracy: 13.10%\n",
      "Epoch 97/100 – Loss: 2.2782, Accuracy: 13.04%\n",
      "Epoch 98/100 – Loss: 2.2782, Accuracy: 13.02%\n",
      "Epoch 99/100 – Loss: 2.2766, Accuracy: 13.08%\n",
      "Epoch 100/100 – Loss: 2.2765, Accuracy: 13.16%\n",
      "Test Accuracy (Mehrklassen): 12.39%\n",
      "\n",
      "Training (Mehrklassen) mit Lernrate=0.001 und Hidden-Dimension=32\n",
      "Epoch 1/100 – Loss: 2.3344, Accuracy: 9.38%\n",
      "Epoch 2/100 – Loss: 2.3344, Accuracy: 9.42%\n",
      "Epoch 3/100 – Loss: 2.3342, Accuracy: 9.44%\n",
      "Epoch 4/100 – Loss: 2.3324, Accuracy: 9.42%\n",
      "Epoch 5/100 – Loss: 2.3325, Accuracy: 9.46%\n",
      "Epoch 6/100 – Loss: 2.3317, Accuracy: 9.52%\n",
      "Epoch 7/100 – Loss: 2.3310, Accuracy: 9.56%\n",
      "Epoch 8/100 – Loss: 2.3298, Accuracy: 9.60%\n",
      "Epoch 9/100 – Loss: 2.3293, Accuracy: 9.60%\n",
      "Epoch 10/100 – Loss: 2.3279, Accuracy: 9.60%\n",
      "Epoch 11/100 – Loss: 2.3292, Accuracy: 9.62%\n",
      "Epoch 12/100 – Loss: 2.3259, Accuracy: 9.74%\n",
      "Epoch 13/100 – Loss: 2.3259, Accuracy: 9.72%\n",
      "Epoch 14/100 – Loss: 2.3255, Accuracy: 9.74%\n",
      "Epoch 15/100 – Loss: 2.3260, Accuracy: 9.74%\n",
      "Epoch 16/100 – Loss: 2.3233, Accuracy: 9.70%\n",
      "Epoch 17/100 – Loss: 2.3225, Accuracy: 9.72%\n",
      "Epoch 18/100 – Loss: 2.3245, Accuracy: 9.74%\n",
      "Epoch 19/100 – Loss: 2.3227, Accuracy: 9.78%\n",
      "Epoch 20/100 – Loss: 2.3211, Accuracy: 9.80%\n",
      "Epoch 21/100 – Loss: 2.3221, Accuracy: 9.82%\n",
      "Epoch 22/100 – Loss: 2.3180, Accuracy: 9.82%\n",
      "Epoch 23/100 – Loss: 2.3189, Accuracy: 9.84%\n",
      "Epoch 24/100 – Loss: 2.3189, Accuracy: 9.84%\n",
      "Epoch 25/100 – Loss: 2.3167, Accuracy: 9.86%\n",
      "Epoch 26/100 – Loss: 2.3183, Accuracy: 9.90%\n",
      "Epoch 27/100 – Loss: 2.3159, Accuracy: 10.00%\n",
      "Epoch 28/100 – Loss: 2.3140, Accuracy: 9.96%\n",
      "Epoch 29/100 – Loss: 2.3146, Accuracy: 9.96%\n",
      "Epoch 30/100 – Loss: 2.3144, Accuracy: 10.00%\n",
      "Epoch 31/100 – Loss: 2.3117, Accuracy: 10.06%\n",
      "Epoch 32/100 – Loss: 2.3108, Accuracy: 10.16%\n",
      "Epoch 33/100 – Loss: 2.3116, Accuracy: 10.24%\n",
      "Epoch 34/100 – Loss: 2.3117, Accuracy: 10.24%\n",
      "Epoch 35/100 – Loss: 2.3122, Accuracy: 10.30%\n",
      "Epoch 36/100 – Loss: 2.3101, Accuracy: 10.30%\n",
      "Epoch 37/100 – Loss: 2.3084, Accuracy: 10.32%\n",
      "Epoch 38/100 – Loss: 2.3093, Accuracy: 10.38%\n",
      "Epoch 39/100 – Loss: 2.3085, Accuracy: 10.38%\n",
      "Epoch 40/100 – Loss: 2.3080, Accuracy: 10.48%\n",
      "Epoch 41/100 – Loss: 2.3096, Accuracy: 10.46%\n",
      "Epoch 42/100 – Loss: 2.3067, Accuracy: 10.52%\n",
      "Epoch 43/100 – Loss: 2.3057, Accuracy: 10.66%\n",
      "Epoch 44/100 – Loss: 2.3056, Accuracy: 10.68%\n",
      "Epoch 45/100 – Loss: 2.3038, Accuracy: 10.72%\n",
      "Epoch 46/100 – Loss: 2.3051, Accuracy: 10.78%\n",
      "Epoch 47/100 – Loss: 2.3042, Accuracy: 10.78%\n",
      "Epoch 48/100 – Loss: 2.3027, Accuracy: 10.82%\n",
      "Epoch 49/100 – Loss: 2.3034, Accuracy: 10.90%\n",
      "Epoch 50/100 – Loss: 2.3005, Accuracy: 10.96%\n",
      "Epoch 51/100 – Loss: 2.3005, Accuracy: 11.02%\n",
      "Epoch 52/100 – Loss: 2.3021, Accuracy: 11.08%\n",
      "Epoch 53/100 – Loss: 2.2989, Accuracy: 11.12%\n",
      "Epoch 54/100 – Loss: 2.2993, Accuracy: 11.14%\n",
      "Epoch 55/100 – Loss: 2.2977, Accuracy: 11.24%\n",
      "Epoch 56/100 – Loss: 2.2986, Accuracy: 11.30%\n",
      "Epoch 57/100 – Loss: 2.2960, Accuracy: 11.36%\n",
      "Epoch 58/100 – Loss: 2.2964, Accuracy: 11.40%\n",
      "Epoch 59/100 – Loss: 2.2960, Accuracy: 11.48%\n",
      "Epoch 60/100 – Loss: 2.2946, Accuracy: 11.48%\n",
      "Epoch 61/100 – Loss: 2.2958, Accuracy: 11.54%\n",
      "Epoch 62/100 – Loss: 2.2941, Accuracy: 11.54%\n",
      "Epoch 63/100 – Loss: 2.2933, Accuracy: 11.60%\n",
      "Epoch 64/100 – Loss: 2.2926, Accuracy: 11.68%\n",
      "Epoch 65/100 – Loss: 2.2914, Accuracy: 11.76%\n",
      "Epoch 66/100 – Loss: 2.2925, Accuracy: 11.80%\n",
      "Epoch 67/100 – Loss: 2.2923, Accuracy: 11.88%\n",
      "Epoch 68/100 – Loss: 2.2898, Accuracy: 11.96%\n",
      "Epoch 69/100 – Loss: 2.2906, Accuracy: 12.02%\n",
      "Epoch 70/100 – Loss: 2.2893, Accuracy: 12.06%\n",
      "Epoch 71/100 – Loss: 2.2884, Accuracy: 12.24%\n",
      "Epoch 72/100 – Loss: 2.2891, Accuracy: 12.30%\n",
      "Epoch 73/100 – Loss: 2.2897, Accuracy: 12.26%\n",
      "Epoch 74/100 – Loss: 2.2872, Accuracy: 12.28%\n",
      "Epoch 75/100 – Loss: 2.2867, Accuracy: 12.32%\n",
      "Epoch 76/100 – Loss: 2.2845, Accuracy: 12.36%\n",
      "Epoch 77/100 – Loss: 2.2850, Accuracy: 12.44%\n",
      "Epoch 78/100 – Loss: 2.2841, Accuracy: 12.46%\n",
      "Epoch 79/100 – Loss: 2.2830, Accuracy: 12.50%\n",
      "Epoch 80/100 – Loss: 2.2843, Accuracy: 12.56%\n",
      "Epoch 81/100 – Loss: 2.2827, Accuracy: 12.56%\n",
      "Epoch 82/100 – Loss: 2.2817, Accuracy: 12.60%\n",
      "Epoch 83/100 – Loss: 2.2805, Accuracy: 12.68%\n",
      "Epoch 84/100 – Loss: 2.2819, Accuracy: 12.74%\n",
      "Epoch 85/100 – Loss: 2.2821, Accuracy: 12.80%\n",
      "Epoch 86/100 – Loss: 2.2807, Accuracy: 12.86%\n",
      "Epoch 87/100 – Loss: 2.2792, Accuracy: 13.00%\n",
      "Epoch 88/100 – Loss: 2.2788, Accuracy: 13.08%\n",
      "Epoch 89/100 – Loss: 2.2784, Accuracy: 13.18%\n",
      "Epoch 90/100 – Loss: 2.2770, Accuracy: 13.26%\n",
      "Epoch 91/100 – Loss: 2.2764, Accuracy: 13.38%\n",
      "Epoch 92/100 – Loss: 2.2771, Accuracy: 13.46%\n",
      "Epoch 93/100 – Loss: 2.2761, Accuracy: 13.56%\n",
      "Epoch 94/100 – Loss: 2.2763, Accuracy: 13.58%\n",
      "Epoch 95/100 – Loss: 2.2747, Accuracy: 13.68%\n",
      "Epoch 96/100 – Loss: 2.2748, Accuracy: 13.68%\n",
      "Epoch 97/100 – Loss: 2.2734, Accuracy: 13.84%\n",
      "Epoch 98/100 – Loss: 2.2742, Accuracy: 13.88%\n",
      "Epoch 99/100 – Loss: 2.2726, Accuracy: 13.90%\n",
      "Epoch 100/100 – Loss: 2.2711, Accuracy: 14.02%\n",
      "Test Accuracy (Mehrklassen): 12.72%\n",
      "\n",
      "Training (Mehrklassen) mit Lernrate=0.001 und Hidden-Dimension=64\n",
      "Epoch 1/100 – Loss: 2.3708, Accuracy: 9.72%\n",
      "Epoch 2/100 – Loss: 2.3720, Accuracy: 9.74%\n",
      "Epoch 3/100 – Loss: 2.3679, Accuracy: 9.74%\n",
      "Epoch 4/100 – Loss: 2.3680, Accuracy: 9.84%\n",
      "Epoch 5/100 – Loss: 2.3657, Accuracy: 9.88%\n",
      "Epoch 6/100 – Loss: 2.3658, Accuracy: 9.86%\n",
      "Epoch 7/100 – Loss: 2.3633, Accuracy: 9.94%\n",
      "Epoch 8/100 – Loss: 2.3596, Accuracy: 9.98%\n",
      "Epoch 9/100 – Loss: 2.3585, Accuracy: 9.98%\n",
      "Epoch 10/100 – Loss: 2.3605, Accuracy: 10.02%\n",
      "Epoch 11/100 – Loss: 2.3576, Accuracy: 10.06%\n",
      "Epoch 12/100 – Loss: 2.3552, Accuracy: 10.06%\n",
      "Epoch 13/100 – Loss: 2.3561, Accuracy: 10.10%\n",
      "Epoch 14/100 – Loss: 2.3530, Accuracy: 10.18%\n",
      "Epoch 15/100 – Loss: 2.3496, Accuracy: 10.18%\n",
      "Epoch 16/100 – Loss: 2.3502, Accuracy: 10.24%\n",
      "Epoch 17/100 – Loss: 2.3476, Accuracy: 10.20%\n",
      "Epoch 18/100 – Loss: 2.3460, Accuracy: 10.26%\n",
      "Epoch 19/100 – Loss: 2.3465, Accuracy: 10.32%\n",
      "Epoch 20/100 – Loss: 2.3450, Accuracy: 10.30%\n",
      "Epoch 21/100 – Loss: 2.3431, Accuracy: 10.38%\n",
      "Epoch 22/100 – Loss: 2.3454, Accuracy: 10.40%\n",
      "Epoch 23/100 – Loss: 2.3395, Accuracy: 10.42%\n",
      "Epoch 24/100 – Loss: 2.3378, Accuracy: 10.40%\n",
      "Epoch 25/100 – Loss: 2.3381, Accuracy: 10.50%\n",
      "Epoch 26/100 – Loss: 2.3384, Accuracy: 10.50%\n",
      "Epoch 27/100 – Loss: 2.3358, Accuracy: 10.50%\n",
      "Epoch 28/100 – Loss: 2.3321, Accuracy: 10.60%\n",
      "Epoch 29/100 – Loss: 2.3321, Accuracy: 10.66%\n",
      "Epoch 30/100 – Loss: 2.3308, Accuracy: 10.70%\n",
      "Epoch 31/100 – Loss: 2.3315, Accuracy: 10.78%\n",
      "Epoch 32/100 – Loss: 2.3306, Accuracy: 10.84%\n",
      "Epoch 33/100 – Loss: 2.3289, Accuracy: 10.86%\n",
      "Epoch 34/100 – Loss: 2.3273, Accuracy: 10.90%\n",
      "Epoch 35/100 – Loss: 2.3251, Accuracy: 10.94%\n",
      "Epoch 36/100 – Loss: 2.3232, Accuracy: 11.00%\n",
      "Epoch 37/100 – Loss: 2.3228, Accuracy: 10.98%\n",
      "Epoch 38/100 – Loss: 2.3208, Accuracy: 11.04%\n",
      "Epoch 39/100 – Loss: 2.3196, Accuracy: 11.06%\n",
      "Epoch 40/100 – Loss: 2.3218, Accuracy: 11.10%\n",
      "Epoch 41/100 – Loss: 2.3179, Accuracy: 11.20%\n",
      "Epoch 42/100 – Loss: 2.3154, Accuracy: 11.18%\n",
      "Epoch 43/100 – Loss: 2.3149, Accuracy: 11.18%\n",
      "Epoch 44/100 – Loss: 2.3155, Accuracy: 11.20%\n",
      "Epoch 45/100 – Loss: 2.3126, Accuracy: 11.24%\n",
      "Epoch 46/100 – Loss: 2.3125, Accuracy: 11.26%\n",
      "Epoch 47/100 – Loss: 2.3096, Accuracy: 11.34%\n",
      "Epoch 48/100 – Loss: 2.3103, Accuracy: 11.38%\n",
      "Epoch 49/100 – Loss: 2.3057, Accuracy: 11.32%\n",
      "Epoch 50/100 – Loss: 2.3063, Accuracy: 11.32%\n",
      "Epoch 51/100 – Loss: 2.3071, Accuracy: 11.36%\n",
      "Epoch 52/100 – Loss: 2.3039, Accuracy: 11.40%\n",
      "Epoch 53/100 – Loss: 2.3058, Accuracy: 11.46%\n",
      "Epoch 54/100 – Loss: 2.3037, Accuracy: 11.48%\n",
      "Epoch 55/100 – Loss: 2.3012, Accuracy: 11.48%\n",
      "Epoch 56/100 – Loss: 2.3014, Accuracy: 11.56%\n",
      "Epoch 57/100 – Loss: 2.3008, Accuracy: 11.50%\n",
      "Epoch 58/100 – Loss: 2.2983, Accuracy: 11.54%\n",
      "Epoch 59/100 – Loss: 2.2987, Accuracy: 11.58%\n",
      "Epoch 60/100 – Loss: 2.2951, Accuracy: 11.66%\n",
      "Epoch 61/100 – Loss: 2.2912, Accuracy: 11.66%\n",
      "Epoch 62/100 – Loss: 2.2933, Accuracy: 11.68%\n",
      "Epoch 63/100 – Loss: 2.2960, Accuracy: 11.78%\n",
      "Epoch 64/100 – Loss: 2.2899, Accuracy: 11.84%\n",
      "Epoch 65/100 – Loss: 2.2883, Accuracy: 11.88%\n",
      "Epoch 66/100 – Loss: 2.2877, Accuracy: 11.90%\n",
      "Epoch 67/100 – Loss: 2.2881, Accuracy: 11.98%\n",
      "Epoch 68/100 – Loss: 2.2876, Accuracy: 11.98%\n",
      "Epoch 69/100 – Loss: 2.2897, Accuracy: 12.04%\n",
      "Epoch 70/100 – Loss: 2.2844, Accuracy: 12.10%\n",
      "Epoch 71/100 – Loss: 2.2848, Accuracy: 12.18%\n",
      "Epoch 72/100 – Loss: 2.2808, Accuracy: 12.28%\n",
      "Epoch 73/100 – Loss: 2.2800, Accuracy: 12.36%\n",
      "Epoch 74/100 – Loss: 2.2808, Accuracy: 12.46%\n",
      "Epoch 75/100 – Loss: 2.2789, Accuracy: 12.52%\n",
      "Epoch 76/100 – Loss: 2.2787, Accuracy: 12.48%\n",
      "Epoch 77/100 – Loss: 2.2786, Accuracy: 12.62%\n",
      "Epoch 78/100 – Loss: 2.2750, Accuracy: 12.64%\n",
      "Epoch 79/100 – Loss: 2.2726, Accuracy: 12.66%\n",
      "Epoch 80/100 – Loss: 2.2749, Accuracy: 12.72%\n",
      "Epoch 81/100 – Loss: 2.2731, Accuracy: 12.76%\n",
      "Epoch 82/100 – Loss: 2.2734, Accuracy: 12.80%\n",
      "Epoch 83/100 – Loss: 2.2703, Accuracy: 12.84%\n",
      "Epoch 84/100 – Loss: 2.2701, Accuracy: 12.80%\n",
      "Epoch 85/100 – Loss: 2.2674, Accuracy: 12.84%\n",
      "Epoch 86/100 – Loss: 2.2705, Accuracy: 12.94%\n",
      "Epoch 87/100 – Loss: 2.2656, Accuracy: 13.06%\n",
      "Epoch 88/100 – Loss: 2.2662, Accuracy: 13.08%\n",
      "Epoch 89/100 – Loss: 2.2646, Accuracy: 13.14%\n",
      "Epoch 90/100 – Loss: 2.2648, Accuracy: 13.22%\n",
      "Epoch 91/100 – Loss: 2.2628, Accuracy: 13.24%\n",
      "Epoch 92/100 – Loss: 2.2618, Accuracy: 13.40%\n",
      "Epoch 93/100 – Loss: 2.2587, Accuracy: 13.42%\n",
      "Epoch 94/100 – Loss: 2.2592, Accuracy: 13.54%\n",
      "Epoch 95/100 – Loss: 2.2586, Accuracy: 13.58%\n",
      "Epoch 96/100 – Loss: 2.2572, Accuracy: 13.58%\n",
      "Epoch 97/100 – Loss: 2.2556, Accuracy: 13.56%\n",
      "Epoch 98/100 – Loss: 2.2548, Accuracy: 13.56%\n",
      "Epoch 99/100 – Loss: 2.2530, Accuracy: 13.68%\n",
      "Epoch 100/100 – Loss: 2.2573, Accuracy: 13.70%\n",
      "Test Accuracy (Mehrklassen): 13.27%\n",
      "\n",
      "Training (Mehrklassen) mit Lernrate=0.01 und Hidden-Dimension=16\n",
      "Epoch 1/100 – Loss: 2.3609, Accuracy: 11.88%\n",
      "Epoch 2/100 – Loss: 2.3484, Accuracy: 12.06%\n",
      "Epoch 3/100 – Loss: 2.3369, Accuracy: 12.28%\n",
      "Epoch 4/100 – Loss: 2.3295, Accuracy: 12.70%\n",
      "Epoch 5/100 – Loss: 2.3209, Accuracy: 13.04%\n",
      "Epoch 6/100 – Loss: 2.3145, Accuracy: 13.24%\n",
      "Epoch 7/100 – Loss: 2.3083, Accuracy: 13.50%\n",
      "Epoch 8/100 – Loss: 2.3010, Accuracy: 13.78%\n",
      "Epoch 9/100 – Loss: 2.2947, Accuracy: 14.40%\n",
      "Epoch 10/100 – Loss: 2.2904, Accuracy: 14.64%\n",
      "Epoch 11/100 – Loss: 2.2868, Accuracy: 15.00%\n",
      "Epoch 12/100 – Loss: 2.2835, Accuracy: 15.44%\n",
      "Epoch 13/100 – Loss: 2.2786, Accuracy: 15.72%\n",
      "Epoch 14/100 – Loss: 2.2749, Accuracy: 16.16%\n",
      "Epoch 15/100 – Loss: 2.2684, Accuracy: 16.52%\n",
      "Epoch 16/100 – Loss: 2.2656, Accuracy: 16.74%\n",
      "Epoch 17/100 – Loss: 2.2632, Accuracy: 17.08%\n",
      "Epoch 18/100 – Loss: 2.2592, Accuracy: 17.28%\n",
      "Epoch 19/100 – Loss: 2.2544, Accuracy: 17.42%\n",
      "Epoch 20/100 – Loss: 2.2503, Accuracy: 17.66%\n",
      "Epoch 21/100 – Loss: 2.2477, Accuracy: 17.86%\n",
      "Epoch 22/100 – Loss: 2.2428, Accuracy: 18.02%\n",
      "Epoch 23/100 – Loss: 2.2400, Accuracy: 18.32%\n",
      "Epoch 24/100 – Loss: 2.2364, Accuracy: 18.40%\n",
      "Epoch 25/100 – Loss: 2.2341, Accuracy: 18.48%\n",
      "Epoch 26/100 – Loss: 2.2310, Accuracy: 18.72%\n",
      "Epoch 27/100 – Loss: 2.2283, Accuracy: 18.68%\n",
      "Epoch 28/100 – Loss: 2.2226, Accuracy: 18.72%\n",
      "Epoch 29/100 – Loss: 2.2210, Accuracy: 18.94%\n",
      "Epoch 30/100 – Loss: 2.2162, Accuracy: 18.98%\n",
      "Epoch 31/100 – Loss: 2.2129, Accuracy: 19.20%\n",
      "Epoch 32/100 – Loss: 2.2100, Accuracy: 19.30%\n",
      "Epoch 33/100 – Loss: 2.2060, Accuracy: 19.52%\n",
      "Epoch 34/100 – Loss: 2.2024, Accuracy: 19.72%\n",
      "Epoch 35/100 – Loss: 2.1983, Accuracy: 19.96%\n",
      "Epoch 36/100 – Loss: 2.1957, Accuracy: 20.18%\n",
      "Epoch 37/100 – Loss: 2.1882, Accuracy: 20.14%\n",
      "Epoch 38/100 – Loss: 2.1851, Accuracy: 20.42%\n",
      "Epoch 39/100 – Loss: 2.1835, Accuracy: 20.64%\n",
      "Epoch 40/100 – Loss: 2.1795, Accuracy: 21.06%\n",
      "Epoch 41/100 – Loss: 2.1761, Accuracy: 21.16%\n",
      "Epoch 42/100 – Loss: 2.1711, Accuracy: 21.32%\n",
      "Epoch 43/100 – Loss: 2.1676, Accuracy: 21.54%\n",
      "Epoch 44/100 – Loss: 2.1636, Accuracy: 21.56%\n",
      "Epoch 45/100 – Loss: 2.1617, Accuracy: 22.02%\n",
      "Epoch 46/100 – Loss: 2.1546, Accuracy: 22.18%\n",
      "Epoch 47/100 – Loss: 2.1533, Accuracy: 22.40%\n",
      "Epoch 48/100 – Loss: 2.1472, Accuracy: 22.64%\n",
      "Epoch 49/100 – Loss: 2.1438, Accuracy: 23.14%\n",
      "Epoch 50/100 – Loss: 2.1408, Accuracy: 23.44%\n",
      "Epoch 51/100 – Loss: 2.1372, Accuracy: 23.54%\n",
      "Epoch 52/100 – Loss: 2.1338, Accuracy: 23.80%\n",
      "Epoch 53/100 – Loss: 2.1302, Accuracy: 23.86%\n",
      "Epoch 54/100 – Loss: 2.1264, Accuracy: 24.08%\n",
      "Epoch 55/100 – Loss: 2.1216, Accuracy: 24.20%\n",
      "Epoch 56/100 – Loss: 2.1179, Accuracy: 24.42%\n",
      "Epoch 57/100 – Loss: 2.1163, Accuracy: 24.60%\n",
      "Epoch 58/100 – Loss: 2.1055, Accuracy: 24.58%\n",
      "Epoch 59/100 – Loss: 2.1082, Accuracy: 24.90%\n",
      "Epoch 60/100 – Loss: 2.1037, Accuracy: 25.30%\n",
      "Epoch 61/100 – Loss: 2.1018, Accuracy: 25.62%\n",
      "Epoch 62/100 – Loss: 2.0955, Accuracy: 25.82%\n",
      "Epoch 63/100 – Loss: 2.0910, Accuracy: 25.88%\n",
      "Epoch 64/100 – Loss: 2.0909, Accuracy: 26.20%\n",
      "Epoch 65/100 – Loss: 2.0857, Accuracy: 26.44%\n",
      "Epoch 66/100 – Loss: 2.0813, Accuracy: 26.58%\n",
      "Epoch 67/100 – Loss: 2.0802, Accuracy: 26.70%\n",
      "Epoch 68/100 – Loss: 2.0759, Accuracy: 27.08%\n",
      "Epoch 69/100 – Loss: 2.0732, Accuracy: 27.28%\n",
      "Epoch 70/100 – Loss: 2.0684, Accuracy: 27.48%\n",
      "Epoch 71/100 – Loss: 2.0615, Accuracy: 27.70%\n",
      "Epoch 72/100 – Loss: 2.0592, Accuracy: 27.86%\n",
      "Epoch 73/100 – Loss: 2.0589, Accuracy: 28.10%\n",
      "Epoch 74/100 – Loss: 2.0518, Accuracy: 28.36%\n",
      "Epoch 75/100 – Loss: 2.0491, Accuracy: 28.44%\n",
      "Epoch 76/100 – Loss: 2.0431, Accuracy: 28.80%\n",
      "Epoch 77/100 – Loss: 2.0418, Accuracy: 29.00%\n",
      "Epoch 78/100 – Loss: 2.0375, Accuracy: 29.32%\n",
      "Epoch 79/100 – Loss: 2.0333, Accuracy: 29.54%\n",
      "Epoch 80/100 – Loss: 2.0330, Accuracy: 29.92%\n",
      "Epoch 81/100 – Loss: 2.0293, Accuracy: 30.26%\n",
      "Epoch 82/100 – Loss: 2.0206, Accuracy: 30.56%\n",
      "Epoch 83/100 – Loss: 2.0252, Accuracy: 30.80%\n",
      "Epoch 84/100 – Loss: 2.0197, Accuracy: 30.94%\n",
      "Epoch 85/100 – Loss: 2.0126, Accuracy: 31.10%\n",
      "Epoch 86/100 – Loss: 2.0079, Accuracy: 31.46%\n",
      "Epoch 87/100 – Loss: 2.0088, Accuracy: 31.64%\n",
      "Epoch 88/100 – Loss: 2.0032, Accuracy: 32.04%\n",
      "Epoch 89/100 – Loss: 2.0004, Accuracy: 32.20%\n",
      "Epoch 90/100 – Loss: 1.9977, Accuracy: 32.46%\n",
      "Epoch 91/100 – Loss: 1.9932, Accuracy: 32.86%\n",
      "Epoch 92/100 – Loss: 1.9909, Accuracy: 32.94%\n",
      "Epoch 93/100 – Loss: 1.9869, Accuracy: 33.18%\n",
      "Epoch 94/100 – Loss: 1.9819, Accuracy: 33.32%\n",
      "Epoch 95/100 – Loss: 1.9780, Accuracy: 33.52%\n",
      "Epoch 96/100 – Loss: 1.9734, Accuracy: 33.92%\n",
      "Epoch 97/100 – Loss: 1.9695, Accuracy: 34.16%\n",
      "Epoch 98/100 – Loss: 1.9686, Accuracy: 34.36%\n",
      "Epoch 99/100 – Loss: 1.9631, Accuracy: 34.54%\n",
      "Epoch 100/100 – Loss: 1.9646, Accuracy: 34.96%\n",
      "Test Accuracy (Mehrklassen): 32.75%\n",
      "\n",
      "Training (Mehrklassen) mit Lernrate=0.01 und Hidden-Dimension=32\n",
      "Epoch 1/100 – Loss: 2.3303, Accuracy: 15.46%\n",
      "Epoch 2/100 – Loss: 2.3105, Accuracy: 16.58%\n",
      "Epoch 3/100 – Loss: 2.2945, Accuracy: 16.94%\n",
      "Epoch 4/100 – Loss: 2.2779, Accuracy: 17.54%\n",
      "Epoch 5/100 – Loss: 2.2658, Accuracy: 18.36%\n",
      "Epoch 6/100 – Loss: 2.2535, Accuracy: 19.30%\n",
      "Epoch 7/100 – Loss: 2.2428, Accuracy: 20.12%\n",
      "Epoch 8/100 – Loss: 2.2327, Accuracy: 21.00%\n",
      "Epoch 9/100 – Loss: 2.2244, Accuracy: 21.60%\n",
      "Epoch 10/100 – Loss: 2.2169, Accuracy: 22.18%\n",
      "Epoch 11/100 – Loss: 2.2082, Accuracy: 22.94%\n",
      "Epoch 12/100 – Loss: 2.1971, Accuracy: 23.52%\n",
      "Epoch 13/100 – Loss: 2.1908, Accuracy: 24.18%\n",
      "Epoch 14/100 – Loss: 2.1817, Accuracy: 25.02%\n",
      "Epoch 15/100 – Loss: 2.1720, Accuracy: 25.42%\n",
      "Epoch 16/100 – Loss: 2.1624, Accuracy: 26.42%\n",
      "Epoch 17/100 – Loss: 2.1553, Accuracy: 27.14%\n",
      "Epoch 18/100 – Loss: 2.1464, Accuracy: 27.86%\n",
      "Epoch 19/100 – Loss: 2.1389, Accuracy: 28.22%\n",
      "Epoch 20/100 – Loss: 2.1282, Accuracy: 28.82%\n",
      "Epoch 21/100 – Loss: 2.1196, Accuracy: 29.90%\n",
      "Epoch 22/100 – Loss: 2.1083, Accuracy: 30.76%\n",
      "Epoch 23/100 – Loss: 2.1013, Accuracy: 31.98%\n",
      "Epoch 24/100 – Loss: 2.0910, Accuracy: 32.78%\n",
      "Epoch 25/100 – Loss: 2.0820, Accuracy: 33.42%\n",
      "Epoch 26/100 – Loss: 2.0705, Accuracy: 34.44%\n",
      "Epoch 27/100 – Loss: 2.0635, Accuracy: 35.44%\n",
      "Epoch 28/100 – Loss: 2.0530, Accuracy: 36.26%\n",
      "Epoch 29/100 – Loss: 2.0440, Accuracy: 37.46%\n",
      "Epoch 30/100 – Loss: 2.0301, Accuracy: 38.10%\n",
      "Epoch 31/100 – Loss: 2.0232, Accuracy: 38.84%\n",
      "Epoch 32/100 – Loss: 2.0119, Accuracy: 39.58%\n",
      "Epoch 33/100 – Loss: 2.0033, Accuracy: 40.14%\n",
      "Epoch 34/100 – Loss: 1.9891, Accuracy: 41.16%\n",
      "Epoch 35/100 – Loss: 1.9800, Accuracy: 42.08%\n",
      "Epoch 36/100 – Loss: 1.9698, Accuracy: 42.88%\n",
      "Epoch 37/100 – Loss: 1.9611, Accuracy: 43.44%\n",
      "Epoch 38/100 – Loss: 1.9491, Accuracy: 44.20%\n",
      "Epoch 39/100 – Loss: 1.9389, Accuracy: 45.04%\n",
      "Epoch 40/100 – Loss: 1.9258, Accuracy: 45.70%\n",
      "Epoch 41/100 – Loss: 1.9149, Accuracy: 46.36%\n",
      "Epoch 42/100 – Loss: 1.9022, Accuracy: 47.08%\n",
      "Epoch 43/100 – Loss: 1.8931, Accuracy: 47.88%\n",
      "Epoch 44/100 – Loss: 1.8827, Accuracy: 48.58%\n",
      "Epoch 45/100 – Loss: 1.8654, Accuracy: 49.24%\n",
      "Epoch 46/100 – Loss: 1.8599, Accuracy: 49.82%\n",
      "Epoch 47/100 – Loss: 1.8466, Accuracy: 50.32%\n",
      "Epoch 48/100 – Loss: 1.8352, Accuracy: 50.86%\n",
      "Epoch 49/100 – Loss: 1.8226, Accuracy: 51.20%\n",
      "Epoch 50/100 – Loss: 1.8086, Accuracy: 51.76%\n",
      "Epoch 51/100 – Loss: 1.7986, Accuracy: 52.36%\n",
      "Epoch 52/100 – Loss: 1.7853, Accuracy: 52.90%\n",
      "Epoch 53/100 – Loss: 1.7730, Accuracy: 53.10%\n",
      "Epoch 54/100 – Loss: 1.7617, Accuracy: 53.78%\n",
      "Epoch 55/100 – Loss: 1.7502, Accuracy: 54.14%\n",
      "Epoch 56/100 – Loss: 1.7370, Accuracy: 54.46%\n",
      "Epoch 57/100 – Loss: 1.7258, Accuracy: 55.08%\n",
      "Epoch 58/100 – Loss: 1.7147, Accuracy: 55.46%\n",
      "Epoch 59/100 – Loss: 1.7033, Accuracy: 56.14%\n",
      "Epoch 60/100 – Loss: 1.6924, Accuracy: 56.50%\n",
      "Epoch 61/100 – Loss: 1.6807, Accuracy: 56.86%\n",
      "Epoch 62/100 – Loss: 1.6665, Accuracy: 57.12%\n",
      "Epoch 63/100 – Loss: 1.6570, Accuracy: 57.48%\n",
      "Epoch 64/100 – Loss: 1.6430, Accuracy: 58.08%\n",
      "Epoch 65/100 – Loss: 1.6327, Accuracy: 58.52%\n",
      "Epoch 66/100 – Loss: 1.6184, Accuracy: 58.96%\n",
      "Epoch 67/100 – Loss: 1.6096, Accuracy: 59.24%\n",
      "Epoch 68/100 – Loss: 1.5958, Accuracy: 59.36%\n",
      "Epoch 69/100 – Loss: 1.5835, Accuracy: 59.76%\n",
      "Epoch 70/100 – Loss: 1.5748, Accuracy: 60.24%\n",
      "Epoch 71/100 – Loss: 1.5649, Accuracy: 60.70%\n",
      "Epoch 72/100 – Loss: 1.5445, Accuracy: 61.08%\n",
      "Epoch 73/100 – Loss: 1.5332, Accuracy: 61.26%\n",
      "Epoch 74/100 – Loss: 1.5221, Accuracy: 61.42%\n",
      "Epoch 75/100 – Loss: 1.5123, Accuracy: 61.88%\n",
      "Epoch 76/100 – Loss: 1.5011, Accuracy: 62.22%\n",
      "Epoch 77/100 – Loss: 1.4837, Accuracy: 62.44%\n",
      "Epoch 78/100 – Loss: 1.4699, Accuracy: 62.76%\n",
      "Epoch 79/100 – Loss: 1.4616, Accuracy: 63.18%\n",
      "Epoch 80/100 – Loss: 1.4522, Accuracy: 63.54%\n",
      "Epoch 81/100 – Loss: 1.4430, Accuracy: 63.74%\n",
      "Epoch 82/100 – Loss: 1.4352, Accuracy: 64.00%\n",
      "Epoch 83/100 – Loss: 1.4206, Accuracy: 64.26%\n",
      "Epoch 84/100 – Loss: 1.4022, Accuracy: 64.46%\n",
      "Epoch 85/100 – Loss: 1.3922, Accuracy: 64.52%\n",
      "Epoch 86/100 – Loss: 1.3857, Accuracy: 65.08%\n",
      "Epoch 87/100 – Loss: 1.3709, Accuracy: 65.24%\n",
      "Epoch 88/100 – Loss: 1.3635, Accuracy: 65.66%\n",
      "Epoch 89/100 – Loss: 1.3526, Accuracy: 65.82%\n",
      "Epoch 90/100 – Loss: 1.3451, Accuracy: 66.00%\n",
      "Epoch 91/100 – Loss: 1.3264, Accuracy: 66.32%\n",
      "Epoch 92/100 – Loss: 1.3198, Accuracy: 66.64%\n",
      "Epoch 93/100 – Loss: 1.3046, Accuracy: 67.02%\n",
      "Epoch 94/100 – Loss: 1.2992, Accuracy: 67.30%\n",
      "Epoch 95/100 – Loss: 1.2864, Accuracy: 67.52%\n",
      "Epoch 96/100 – Loss: 1.2757, Accuracy: 67.58%\n",
      "Epoch 97/100 – Loss: 1.2648, Accuracy: 67.80%\n",
      "Epoch 98/100 – Loss: 1.2554, Accuracy: 68.14%\n",
      "Epoch 99/100 – Loss: 1.2469, Accuracy: 68.34%\n",
      "Epoch 100/100 – Loss: 1.2321, Accuracy: 68.58%\n",
      "Test Accuracy (Mehrklassen): 66.83%\n",
      "\n",
      "Training (Mehrklassen) mit Lernrate=0.01 und Hidden-Dimension=64\n",
      "Epoch 1/100 – Loss: 2.5344, Accuracy: 8.82%\n",
      "Epoch 2/100 – Loss: 2.4798, Accuracy: 9.50%\n",
      "Epoch 3/100 – Loss: 2.4418, Accuracy: 10.22%\n",
      "Epoch 4/100 – Loss: 2.4123, Accuracy: 10.80%\n",
      "Epoch 5/100 – Loss: 2.3872, Accuracy: 11.76%\n",
      "Epoch 6/100 – Loss: 2.3607, Accuracy: 12.32%\n",
      "Epoch 7/100 – Loss: 2.3400, Accuracy: 13.34%\n",
      "Epoch 8/100 – Loss: 2.3252, Accuracy: 14.26%\n",
      "Epoch 9/100 – Loss: 2.3056, Accuracy: 15.26%\n",
      "Epoch 10/100 – Loss: 2.2878, Accuracy: 15.98%\n",
      "Epoch 11/100 – Loss: 2.2750, Accuracy: 17.22%\n",
      "Epoch 12/100 – Loss: 2.2608, Accuracy: 18.56%\n",
      "Epoch 13/100 – Loss: 2.2481, Accuracy: 19.36%\n",
      "Epoch 14/100 – Loss: 2.2347, Accuracy: 19.82%\n",
      "Epoch 15/100 – Loss: 2.2199, Accuracy: 20.86%\n",
      "Epoch 16/100 – Loss: 2.2117, Accuracy: 21.64%\n",
      "Epoch 17/100 – Loss: 2.1996, Accuracy: 22.42%\n",
      "Epoch 18/100 – Loss: 2.1888, Accuracy: 23.34%\n",
      "Epoch 19/100 – Loss: 2.1762, Accuracy: 24.12%\n",
      "Epoch 20/100 – Loss: 2.1646, Accuracy: 25.02%\n",
      "Epoch 21/100 – Loss: 2.1547, Accuracy: 26.00%\n",
      "Epoch 22/100 – Loss: 2.1438, Accuracy: 26.58%\n",
      "Epoch 23/100 – Loss: 2.1324, Accuracy: 27.22%\n",
      "Epoch 24/100 – Loss: 2.1245, Accuracy: 28.08%\n",
      "Epoch 25/100 – Loss: 2.1136, Accuracy: 28.52%\n",
      "Epoch 26/100 – Loss: 2.1046, Accuracy: 28.98%\n",
      "Epoch 27/100 – Loss: 2.0934, Accuracy: 29.78%\n",
      "Epoch 28/100 – Loss: 2.0827, Accuracy: 30.36%\n",
      "Epoch 29/100 – Loss: 2.0768, Accuracy: 30.86%\n",
      "Epoch 30/100 – Loss: 2.0650, Accuracy: 31.64%\n",
      "Epoch 31/100 – Loss: 2.0577, Accuracy: 32.30%\n",
      "Epoch 32/100 – Loss: 2.0432, Accuracy: 33.04%\n",
      "Epoch 33/100 – Loss: 2.0378, Accuracy: 33.52%\n",
      "Epoch 34/100 – Loss: 2.0279, Accuracy: 34.06%\n",
      "Epoch 35/100 – Loss: 2.0138, Accuracy: 34.82%\n",
      "Epoch 36/100 – Loss: 2.0086, Accuracy: 35.22%\n",
      "Epoch 37/100 – Loss: 1.9984, Accuracy: 35.76%\n",
      "Epoch 38/100 – Loss: 1.9858, Accuracy: 36.92%\n",
      "Epoch 39/100 – Loss: 1.9756, Accuracy: 37.48%\n",
      "Epoch 40/100 – Loss: 1.9668, Accuracy: 38.18%\n",
      "Epoch 41/100 – Loss: 1.9556, Accuracy: 38.92%\n",
      "Epoch 42/100 – Loss: 1.9468, Accuracy: 39.50%\n",
      "Epoch 43/100 – Loss: 1.9384, Accuracy: 40.24%\n",
      "Epoch 44/100 – Loss: 1.9285, Accuracy: 40.92%\n",
      "Epoch 45/100 – Loss: 1.9174, Accuracy: 41.36%\n",
      "Epoch 46/100 – Loss: 1.9097, Accuracy: 42.18%\n",
      "Epoch 47/100 – Loss: 1.8965, Accuracy: 43.20%\n",
      "Epoch 48/100 – Loss: 1.8907, Accuracy: 43.80%\n",
      "Epoch 49/100 – Loss: 1.8765, Accuracy: 44.38%\n",
      "Epoch 50/100 – Loss: 1.8709, Accuracy: 44.90%\n",
      "Epoch 51/100 – Loss: 1.8586, Accuracy: 45.52%\n",
      "Epoch 52/100 – Loss: 1.8478, Accuracy: 46.22%\n",
      "Epoch 53/100 – Loss: 1.8397, Accuracy: 47.08%\n",
      "Epoch 54/100 – Loss: 1.8287, Accuracy: 47.82%\n",
      "Epoch 55/100 – Loss: 1.8197, Accuracy: 48.44%\n",
      "Epoch 56/100 – Loss: 1.8065, Accuracy: 48.88%\n",
      "Epoch 57/100 – Loss: 1.7951, Accuracy: 49.52%\n",
      "Epoch 58/100 – Loss: 1.7878, Accuracy: 49.90%\n",
      "Epoch 59/100 – Loss: 1.7753, Accuracy: 50.68%\n",
      "Epoch 60/100 – Loss: 1.7661, Accuracy: 51.46%\n",
      "Epoch 61/100 – Loss: 1.7599, Accuracy: 51.86%\n",
      "Epoch 62/100 – Loss: 1.7448, Accuracy: 52.16%\n",
      "Epoch 63/100 – Loss: 1.7331, Accuracy: 52.76%\n",
      "Epoch 64/100 – Loss: 1.7275, Accuracy: 53.24%\n",
      "Epoch 65/100 – Loss: 1.7157, Accuracy: 53.74%\n",
      "Epoch 66/100 – Loss: 1.7028, Accuracy: 54.36%\n",
      "Epoch 67/100 – Loss: 1.6927, Accuracy: 54.92%\n",
      "Epoch 68/100 – Loss: 1.6841, Accuracy: 55.12%\n",
      "Epoch 69/100 – Loss: 1.6766, Accuracy: 55.52%\n",
      "Epoch 70/100 – Loss: 1.6617, Accuracy: 55.94%\n",
      "Epoch 71/100 – Loss: 1.6562, Accuracy: 56.48%\n",
      "Epoch 72/100 – Loss: 1.6424, Accuracy: 56.84%\n",
      "Epoch 73/100 – Loss: 1.6313, Accuracy: 57.42%\n",
      "Epoch 74/100 – Loss: 1.6173, Accuracy: 57.52%\n",
      "Epoch 75/100 – Loss: 1.6059, Accuracy: 58.24%\n",
      "Epoch 76/100 – Loss: 1.5959, Accuracy: 58.70%\n",
      "Epoch 77/100 – Loss: 1.5895, Accuracy: 59.06%\n",
      "Epoch 78/100 – Loss: 1.5820, Accuracy: 59.68%\n",
      "Epoch 79/100 – Loss: 1.5660, Accuracy: 60.50%\n",
      "Epoch 80/100 – Loss: 1.5535, Accuracy: 61.08%\n",
      "Epoch 81/100 – Loss: 1.5460, Accuracy: 61.30%\n",
      "Epoch 82/100 – Loss: 1.5336, Accuracy: 62.04%\n",
      "Epoch 83/100 – Loss: 1.5237, Accuracy: 62.38%\n",
      "Epoch 84/100 – Loss: 1.5080, Accuracy: 62.68%\n",
      "Epoch 85/100 – Loss: 1.5076, Accuracy: 63.14%\n",
      "Epoch 86/100 – Loss: 1.4848, Accuracy: 63.70%\n",
      "Epoch 87/100 – Loss: 1.4786, Accuracy: 64.12%\n",
      "Epoch 88/100 – Loss: 1.4713, Accuracy: 64.30%\n",
      "Epoch 89/100 – Loss: 1.4570, Accuracy: 64.64%\n",
      "Epoch 90/100 – Loss: 1.4489, Accuracy: 65.18%\n",
      "Epoch 91/100 – Loss: 1.4382, Accuracy: 65.62%\n",
      "Epoch 92/100 – Loss: 1.4275, Accuracy: 65.70%\n",
      "Epoch 93/100 – Loss: 1.4146, Accuracy: 66.38%\n",
      "Epoch 94/100 – Loss: 1.4076, Accuracy: 66.58%\n",
      "Epoch 95/100 – Loss: 1.4016, Accuracy: 67.16%\n",
      "Epoch 96/100 – Loss: 1.3797, Accuracy: 67.50%\n",
      "Epoch 97/100 – Loss: 1.3720, Accuracy: 67.84%\n",
      "Epoch 98/100 – Loss: 1.3593, Accuracy: 68.04%\n",
      "Epoch 99/100 – Loss: 1.3544, Accuracy: 68.30%\n",
      "Epoch 100/100 – Loss: 1.3388, Accuracy: 68.54%\n",
      "Test Accuracy (Mehrklassen): 68.01%\n",
      "\n",
      "Training (Mehrklassen) mit Lernrate=0.1 und Hidden-Dimension=16\n",
      "Epoch 1/100 – Loss: 2.3087, Accuracy: 14.46%\n",
      "Epoch 2/100 – Loss: 2.2543, Accuracy: 18.48%\n",
      "Epoch 3/100 – Loss: 2.2044, Accuracy: 22.02%\n",
      "Epoch 4/100 – Loss: 2.1403, Accuracy: 25.36%\n",
      "Epoch 5/100 – Loss: 2.0787, Accuracy: 27.38%\n",
      "Epoch 6/100 – Loss: 2.0170, Accuracy: 27.52%\n",
      "Epoch 7/100 – Loss: 1.9577, Accuracy: 30.04%\n",
      "Epoch 8/100 – Loss: 1.9016, Accuracy: 34.34%\n",
      "Epoch 9/100 – Loss: 1.8493, Accuracy: 37.86%\n",
      "Epoch 10/100 – Loss: 1.7921, Accuracy: 40.12%\n",
      "Epoch 11/100 – Loss: 1.7386, Accuracy: 45.62%\n",
      "Epoch 12/100 – Loss: 1.6816, Accuracy: 49.70%\n",
      "Epoch 13/100 – Loss: 1.6226, Accuracy: 50.56%\n",
      "Epoch 14/100 – Loss: 1.5593, Accuracy: 52.60%\n",
      "Epoch 15/100 – Loss: 1.4958, Accuracy: 54.04%\n",
      "Epoch 16/100 – Loss: 1.4342, Accuracy: 57.86%\n",
      "Epoch 17/100 – Loss: 1.3722, Accuracy: 59.32%\n",
      "Epoch 18/100 – Loss: 1.3176, Accuracy: 61.96%\n",
      "Epoch 19/100 – Loss: 1.2586, Accuracy: 64.44%\n",
      "Epoch 20/100 – Loss: 1.2054, Accuracy: 65.92%\n",
      "Epoch 21/100 – Loss: 1.1599, Accuracy: 67.28%\n",
      "Epoch 22/100 – Loss: 1.1186, Accuracy: 69.06%\n",
      "Epoch 23/100 – Loss: 1.0754, Accuracy: 69.58%\n",
      "Epoch 24/100 – Loss: 1.0328, Accuracy: 70.02%\n",
      "Epoch 25/100 – Loss: 1.0048, Accuracy: 72.10%\n",
      "Epoch 26/100 – Loss: 0.9725, Accuracy: 72.56%\n",
      "Epoch 27/100 – Loss: 0.9399, Accuracy: 73.54%\n",
      "Epoch 28/100 – Loss: 0.9065, Accuracy: 73.20%\n",
      "Epoch 29/100 – Loss: 0.8836, Accuracy: 74.74%\n",
      "Epoch 30/100 – Loss: 0.8657, Accuracy: 73.60%\n",
      "Epoch 31/100 – Loss: 0.8444, Accuracy: 75.42%\n",
      "Epoch 32/100 – Loss: 0.8212, Accuracy: 72.24%\n",
      "Epoch 33/100 – Loss: 0.8056, Accuracy: 77.52%\n",
      "Epoch 34/100 – Loss: 0.7847, Accuracy: 77.96%\n",
      "Epoch 35/100 – Loss: 0.7591, Accuracy: 77.80%\n",
      "Epoch 36/100 – Loss: 0.7468, Accuracy: 78.14%\n",
      "Epoch 37/100 – Loss: 0.7394, Accuracy: 76.80%\n",
      "Epoch 38/100 – Loss: 0.7283, Accuracy: 79.26%\n",
      "Epoch 39/100 – Loss: 0.7072, Accuracy: 78.60%\n",
      "Epoch 40/100 – Loss: 0.6906, Accuracy: 78.82%\n",
      "Epoch 41/100 – Loss: 0.6810, Accuracy: 79.92%\n",
      "Epoch 42/100 – Loss: 0.6684, Accuracy: 81.30%\n",
      "Epoch 43/100 – Loss: 0.6632, Accuracy: 76.74%\n",
      "Epoch 44/100 – Loss: 0.6668, Accuracy: 81.32%\n",
      "Epoch 45/100 – Loss: 0.6351, Accuracy: 82.10%\n",
      "Epoch 46/100 – Loss: 0.6272, Accuracy: 82.18%\n",
      "Epoch 47/100 – Loss: 0.6136, Accuracy: 79.82%\n",
      "Epoch 48/100 – Loss: 0.6084, Accuracy: 82.38%\n",
      "Epoch 49/100 – Loss: 0.5961, Accuracy: 82.02%\n",
      "Epoch 50/100 – Loss: 0.5839, Accuracy: 81.66%\n",
      "Epoch 51/100 – Loss: 0.5798, Accuracy: 82.90%\n",
      "Epoch 52/100 – Loss: 0.5717, Accuracy: 81.70%\n",
      "Epoch 53/100 – Loss: 0.5620, Accuracy: 83.38%\n",
      "Epoch 54/100 – Loss: 0.5532, Accuracy: 82.74%\n",
      "Epoch 55/100 – Loss: 0.5454, Accuracy: 84.38%\n",
      "Epoch 56/100 – Loss: 0.5353, Accuracy: 84.60%\n",
      "Epoch 57/100 – Loss: 0.5306, Accuracy: 82.64%\n",
      "Epoch 58/100 – Loss: 0.5292, Accuracy: 84.46%\n",
      "Epoch 59/100 – Loss: 0.5191, Accuracy: 85.02%\n",
      "Epoch 60/100 – Loss: 0.5232, Accuracy: 84.24%\n",
      "Epoch 61/100 – Loss: 0.5113, Accuracy: 85.20%\n",
      "Epoch 62/100 – Loss: 0.5045, Accuracy: 83.94%\n",
      "Epoch 63/100 – Loss: 0.5030, Accuracy: 85.68%\n",
      "Epoch 64/100 – Loss: 0.4927, Accuracy: 84.30%\n",
      "Epoch 65/100 – Loss: 0.4934, Accuracy: 85.26%\n",
      "Epoch 66/100 – Loss: 0.4840, Accuracy: 86.14%\n",
      "Epoch 67/100 – Loss: 0.4741, Accuracy: 84.68%\n",
      "Epoch 68/100 – Loss: 0.4750, Accuracy: 85.50%\n",
      "Epoch 69/100 – Loss: 0.4754, Accuracy: 86.06%\n",
      "Epoch 70/100 – Loss: 0.4638, Accuracy: 81.58%\n",
      "Epoch 71/100 – Loss: 0.4708, Accuracy: 87.02%\n",
      "Epoch 72/100 – Loss: 0.4558, Accuracy: 75.62%\n",
      "Epoch 73/100 – Loss: 0.4829, Accuracy: 85.66%\n",
      "Epoch 74/100 – Loss: 0.4493, Accuracy: 87.54%\n",
      "Epoch 75/100 – Loss: 0.4483, Accuracy: 84.90%\n",
      "Epoch 76/100 – Loss: 0.4479, Accuracy: 87.88%\n",
      "Epoch 77/100 – Loss: 0.4320, Accuracy: 87.44%\n",
      "Epoch 78/100 – Loss: 0.4278, Accuracy: 88.46%\n",
      "Epoch 79/100 – Loss: 0.4203, Accuracy: 87.70%\n",
      "Epoch 80/100 – Loss: 0.4168, Accuracy: 88.48%\n",
      "Epoch 81/100 – Loss: 0.4095, Accuracy: 88.56%\n",
      "Epoch 82/100 – Loss: 0.4097, Accuracy: 86.24%\n",
      "Epoch 83/100 – Loss: 0.4128, Accuracy: 88.68%\n",
      "Epoch 84/100 – Loss: 0.4059, Accuracy: 87.84%\n",
      "Epoch 85/100 – Loss: 0.4051, Accuracy: 87.86%\n",
      "Epoch 86/100 – Loss: 0.4030, Accuracy: 87.34%\n",
      "Epoch 87/100 – Loss: 0.3999, Accuracy: 88.20%\n",
      "Epoch 88/100 – Loss: 0.3921, Accuracy: 89.50%\n",
      "Epoch 89/100 – Loss: 0.3833, Accuracy: 89.66%\n",
      "Epoch 90/100 – Loss: 0.3838, Accuracy: 88.24%\n",
      "Epoch 91/100 – Loss: 0.3841, Accuracy: 89.18%\n",
      "Epoch 92/100 – Loss: 0.3750, Accuracy: 89.76%\n",
      "Epoch 93/100 – Loss: 0.3740, Accuracy: 88.94%\n",
      "Epoch 94/100 – Loss: 0.3707, Accuracy: 90.18%\n",
      "Epoch 95/100 – Loss: 0.3684, Accuracy: 88.30%\n",
      "Epoch 96/100 – Loss: 0.3676, Accuracy: 89.18%\n",
      "Epoch 97/100 – Loss: 0.3663, Accuracy: 88.10%\n",
      "Epoch 98/100 – Loss: 0.3719, Accuracy: 87.76%\n",
      "Epoch 99/100 – Loss: 0.3648, Accuracy: 90.42%\n",
      "Epoch 100/100 – Loss: 0.3582, Accuracy: 87.06%\n",
      "Test Accuracy (Mehrklassen): 84.10%\n",
      "\n",
      "Training (Mehrklassen) mit Lernrate=0.1 und Hidden-Dimension=32\n",
      "Epoch 1/100 – Loss: 2.3864, Accuracy: 13.92%\n",
      "Epoch 2/100 – Loss: 2.2238, Accuracy: 17.58%\n",
      "Epoch 3/100 – Loss: 2.1451, Accuracy: 26.32%\n",
      "Epoch 4/100 – Loss: 2.0673, Accuracy: 30.58%\n",
      "Epoch 5/100 – Loss: 1.9941, Accuracy: 33.74%\n",
      "Epoch 6/100 – Loss: 1.9190, Accuracy: 38.80%\n",
      "Epoch 7/100 – Loss: 1.8446, Accuracy: 42.48%\n",
      "Epoch 8/100 – Loss: 1.7614, Accuracy: 44.78%\n",
      "Epoch 9/100 – Loss: 1.6792, Accuracy: 48.62%\n",
      "Epoch 10/100 – Loss: 1.5896, Accuracy: 51.14%\n",
      "Epoch 11/100 – Loss: 1.5078, Accuracy: 58.36%\n",
      "Epoch 12/100 – Loss: 1.4169, Accuracy: 62.32%\n",
      "Epoch 13/100 – Loss: 1.3370, Accuracy: 64.48%\n",
      "Epoch 14/100 – Loss: 1.2678, Accuracy: 63.64%\n",
      "Epoch 15/100 – Loss: 1.2028, Accuracy: 68.88%\n",
      "Epoch 16/100 – Loss: 1.1309, Accuracy: 69.34%\n",
      "Epoch 17/100 – Loss: 1.0770, Accuracy: 71.48%\n",
      "Epoch 18/100 – Loss: 1.0252, Accuracy: 72.10%\n",
      "Epoch 19/100 – Loss: 0.9739, Accuracy: 71.50%\n",
      "Epoch 20/100 – Loss: 0.9415, Accuracy: 74.36%\n",
      "Epoch 21/100 – Loss: 0.8919, Accuracy: 75.10%\n",
      "Epoch 22/100 – Loss: 0.8614, Accuracy: 75.76%\n",
      "Epoch 23/100 – Loss: 0.8307, Accuracy: 77.54%\n",
      "Epoch 24/100 – Loss: 0.7942, Accuracy: 77.38%\n",
      "Epoch 25/100 – Loss: 0.7719, Accuracy: 77.46%\n",
      "Epoch 26/100 – Loss: 0.7527, Accuracy: 79.34%\n",
      "Epoch 27/100 – Loss: 0.7290, Accuracy: 78.88%\n",
      "Epoch 28/100 – Loss: 0.7054, Accuracy: 79.46%\n",
      "Epoch 29/100 – Loss: 0.6851, Accuracy: 78.52%\n",
      "Epoch 30/100 – Loss: 0.6739, Accuracy: 80.56%\n",
      "Epoch 31/100 – Loss: 0.6580, Accuracy: 80.24%\n",
      "Epoch 32/100 – Loss: 0.6456, Accuracy: 81.86%\n",
      "Epoch 33/100 – Loss: 0.6270, Accuracy: 82.14%\n",
      "Epoch 34/100 – Loss: 0.6111, Accuracy: 82.24%\n",
      "Epoch 35/100 – Loss: 0.5956, Accuracy: 81.54%\n",
      "Epoch 36/100 – Loss: 0.5866, Accuracy: 82.86%\n",
      "Epoch 37/100 – Loss: 0.5827, Accuracy: 68.78%\n",
      "Epoch 38/100 – Loss: 0.6024, Accuracy: 83.40%\n",
      "Epoch 39/100 – Loss: 0.5508, Accuracy: 84.12%\n",
      "Epoch 40/100 – Loss: 0.5410, Accuracy: 84.30%\n",
      "Epoch 41/100 – Loss: 0.5328, Accuracy: 84.62%\n",
      "Epoch 42/100 – Loss: 0.5238, Accuracy: 85.08%\n",
      "Epoch 43/100 – Loss: 0.5216, Accuracy: 82.76%\n",
      "Epoch 44/100 – Loss: 0.5212, Accuracy: 81.36%\n",
      "Epoch 45/100 – Loss: 0.5205, Accuracy: 85.36%\n",
      "Epoch 46/100 – Loss: 0.5014, Accuracy: 83.60%\n",
      "Epoch 47/100 – Loss: 0.5066, Accuracy: 83.48%\n",
      "Epoch 48/100 – Loss: 0.4906, Accuracy: 84.08%\n",
      "Epoch 49/100 – Loss: 0.4846, Accuracy: 85.56%\n",
      "Epoch 50/100 – Loss: 0.4730, Accuracy: 86.62%\n",
      "Epoch 51/100 – Loss: 0.4634, Accuracy: 86.10%\n",
      "Epoch 52/100 – Loss: 0.4600, Accuracy: 85.94%\n",
      "Epoch 53/100 – Loss: 0.4587, Accuracy: 84.50%\n",
      "Epoch 54/100 – Loss: 0.4523, Accuracy: 86.42%\n",
      "Epoch 55/100 – Loss: 0.4439, Accuracy: 86.00%\n",
      "Epoch 56/100 – Loss: 0.4384, Accuracy: 82.48%\n",
      "Epoch 57/100 – Loss: 0.4479, Accuracy: 87.52%\n",
      "Epoch 58/100 – Loss: 0.4238, Accuracy: 87.64%\n",
      "Epoch 59/100 – Loss: 0.4244, Accuracy: 87.04%\n",
      "Epoch 60/100 – Loss: 0.4167, Accuracy: 88.08%\n",
      "Epoch 61/100 – Loss: 0.4113, Accuracy: 88.10%\n",
      "Epoch 62/100 – Loss: 0.4106, Accuracy: 87.84%\n",
      "Epoch 63/100 – Loss: 0.4055, Accuracy: 88.24%\n",
      "Epoch 64/100 – Loss: 0.4010, Accuracy: 87.48%\n",
      "Epoch 65/100 – Loss: 0.3985, Accuracy: 88.94%\n",
      "Epoch 66/100 – Loss: 0.3925, Accuracy: 89.46%\n",
      "Epoch 67/100 – Loss: 0.3890, Accuracy: 87.96%\n",
      "Epoch 68/100 – Loss: 0.3859, Accuracy: 88.86%\n",
      "Epoch 69/100 – Loss: 0.3912, Accuracy: 88.80%\n",
      "Epoch 70/100 – Loss: 0.3772, Accuracy: 89.46%\n",
      "Epoch 71/100 – Loss: 0.3736, Accuracy: 89.26%\n",
      "Epoch 72/100 – Loss: 0.3710, Accuracy: 89.06%\n",
      "Epoch 73/100 – Loss: 0.3671, Accuracy: 89.76%\n",
      "Epoch 74/100 – Loss: 0.3641, Accuracy: 89.60%\n",
      "Epoch 75/100 – Loss: 0.3583, Accuracy: 90.22%\n",
      "Epoch 76/100 – Loss: 0.3615, Accuracy: 87.58%\n",
      "Epoch 77/100 – Loss: 0.3688, Accuracy: 88.72%\n",
      "Epoch 78/100 – Loss: 0.3554, Accuracy: 90.36%\n",
      "Epoch 79/100 – Loss: 0.3459, Accuracy: 89.84%\n",
      "Epoch 80/100 – Loss: 0.3431, Accuracy: 89.08%\n",
      "Epoch 81/100 – Loss: 0.3445, Accuracy: 89.92%\n",
      "Epoch 82/100 – Loss: 0.3406, Accuracy: 89.16%\n",
      "Epoch 83/100 – Loss: 0.3427, Accuracy: 81.38%\n",
      "Epoch 84/100 – Loss: 0.3594, Accuracy: 89.20%\n",
      "Epoch 85/100 – Loss: 0.3336, Accuracy: 90.60%\n",
      "Epoch 86/100 – Loss: 0.3277, Accuracy: 91.24%\n",
      "Epoch 87/100 – Loss: 0.3264, Accuracy: 90.18%\n",
      "Epoch 88/100 – Loss: 0.3310, Accuracy: 90.38%\n",
      "Epoch 89/100 – Loss: 0.3331, Accuracy: 89.50%\n",
      "Epoch 90/100 – Loss: 0.3261, Accuracy: 91.38%\n",
      "Epoch 91/100 – Loss: 0.3156, Accuracy: 89.04%\n",
      "Epoch 92/100 – Loss: 0.3199, Accuracy: 90.78%\n",
      "Epoch 93/100 – Loss: 0.3226, Accuracy: 90.70%\n",
      "Epoch 94/100 – Loss: 0.3134, Accuracy: 90.76%\n",
      "Epoch 95/100 – Loss: 0.3087, Accuracy: 89.76%\n",
      "Epoch 96/100 – Loss: 0.3071, Accuracy: 91.82%\n",
      "Epoch 97/100 – Loss: 0.3003, Accuracy: 91.90%\n",
      "Epoch 98/100 – Loss: 0.2991, Accuracy: 90.60%\n",
      "Epoch 99/100 – Loss: 0.3030, Accuracy: 89.70%\n",
      "Epoch 100/100 – Loss: 0.2985, Accuracy: 91.62%\n",
      "Test Accuracy (Mehrklassen): 88.66%\n",
      "\n",
      "Training (Mehrklassen) mit Lernrate=0.1 und Hidden-Dimension=64\n",
      "Epoch 1/100 – Loss: 2.3102, Accuracy: 13.02%\n",
      "Epoch 2/100 – Loss: 2.2062, Accuracy: 22.48%\n",
      "Epoch 3/100 – Loss: 2.1225, Accuracy: 31.22%\n",
      "Epoch 4/100 – Loss: 2.0336, Accuracy: 38.64%\n",
      "Epoch 5/100 – Loss: 1.9292, Accuracy: 46.98%\n",
      "Epoch 6/100 – Loss: 1.8151, Accuracy: 52.60%\n",
      "Epoch 7/100 – Loss: 1.7015, Accuracy: 60.88%\n",
      "Epoch 8/100 – Loss: 1.5809, Accuracy: 61.82%\n",
      "Epoch 9/100 – Loss: 1.4597, Accuracy: 66.78%\n",
      "Epoch 10/100 – Loss: 1.3411, Accuracy: 67.08%\n",
      "Epoch 11/100 – Loss: 1.2421, Accuracy: 70.26%\n",
      "Epoch 12/100 – Loss: 1.1508, Accuracy: 73.66%\n",
      "Epoch 13/100 – Loss: 1.0588, Accuracy: 74.62%\n",
      "Epoch 14/100 – Loss: 0.9941, Accuracy: 75.94%\n",
      "Epoch 15/100 – Loss: 0.9348, Accuracy: 76.74%\n",
      "Epoch 16/100 – Loss: 0.8806, Accuracy: 76.28%\n",
      "Epoch 17/100 – Loss: 0.8300, Accuracy: 77.34%\n",
      "Epoch 18/100 – Loss: 0.7891, Accuracy: 78.80%\n",
      "Epoch 19/100 – Loss: 0.7492, Accuracy: 79.90%\n",
      "Epoch 20/100 – Loss: 0.7253, Accuracy: 80.58%\n",
      "Epoch 21/100 – Loss: 0.6918, Accuracy: 80.70%\n",
      "Epoch 22/100 – Loss: 0.6737, Accuracy: 81.92%\n",
      "Epoch 23/100 – Loss: 0.6472, Accuracy: 82.58%\n",
      "Epoch 24/100 – Loss: 0.6249, Accuracy: 80.84%\n",
      "Epoch 25/100 – Loss: 0.6109, Accuracy: 80.76%\n",
      "Epoch 26/100 – Loss: 0.5944, Accuracy: 80.46%\n",
      "Epoch 27/100 – Loss: 0.5840, Accuracy: 83.32%\n",
      "Epoch 28/100 – Loss: 0.5625, Accuracy: 83.66%\n",
      "Epoch 29/100 – Loss: 0.5478, Accuracy: 85.20%\n",
      "Epoch 30/100 – Loss: 0.5326, Accuracy: 85.18%\n",
      "Epoch 31/100 – Loss: 0.5222, Accuracy: 86.14%\n",
      "Epoch 32/100 – Loss: 0.5069, Accuracy: 84.40%\n",
      "Epoch 33/100 – Loss: 0.4981, Accuracy: 86.80%\n",
      "Epoch 34/100 – Loss: 0.4833, Accuracy: 86.46%\n",
      "Epoch 35/100 – Loss: 0.4715, Accuracy: 85.06%\n",
      "Epoch 36/100 – Loss: 0.4698, Accuracy: 86.44%\n",
      "Epoch 37/100 – Loss: 0.4601, Accuracy: 83.50%\n",
      "Epoch 38/100 – Loss: 0.4557, Accuracy: 86.64%\n",
      "Epoch 39/100 – Loss: 0.4445, Accuracy: 86.22%\n",
      "Epoch 40/100 – Loss: 0.4362, Accuracy: 87.26%\n",
      "Epoch 41/100 – Loss: 0.4237, Accuracy: 88.74%\n",
      "Epoch 42/100 – Loss: 0.4167, Accuracy: 88.02%\n",
      "Epoch 43/100 – Loss: 0.4136, Accuracy: 86.44%\n",
      "Epoch 44/100 – Loss: 0.4100, Accuracy: 89.28%\n",
      "Epoch 45/100 – Loss: 0.3957, Accuracy: 86.32%\n",
      "Epoch 46/100 – Loss: 0.3982, Accuracy: 89.58%\n",
      "Epoch 47/100 – Loss: 0.3820, Accuracy: 89.58%\n",
      "Epoch 48/100 – Loss: 0.3798, Accuracy: 89.02%\n",
      "Epoch 49/100 – Loss: 0.3766, Accuracy: 90.02%\n",
      "Epoch 50/100 – Loss: 0.3667, Accuracy: 89.50%\n",
      "Epoch 51/100 – Loss: 0.3633, Accuracy: 89.88%\n",
      "Epoch 52/100 – Loss: 0.3621, Accuracy: 89.80%\n",
      "Epoch 53/100 – Loss: 0.3549, Accuracy: 88.08%\n",
      "Epoch 54/100 – Loss: 0.3561, Accuracy: 89.22%\n",
      "Epoch 55/100 – Loss: 0.3494, Accuracy: 90.84%\n",
      "Epoch 56/100 – Loss: 0.3449, Accuracy: 90.64%\n",
      "Epoch 57/100 – Loss: 0.3364, Accuracy: 91.12%\n",
      "Epoch 58/100 – Loss: 0.3351, Accuracy: 87.78%\n",
      "Epoch 59/100 – Loss: 0.3417, Accuracy: 91.04%\n",
      "Epoch 60/100 – Loss: 0.3279, Accuracy: 90.10%\n",
      "Epoch 61/100 – Loss: 0.3264, Accuracy: 90.64%\n",
      "Epoch 62/100 – Loss: 0.3231, Accuracy: 91.22%\n",
      "Epoch 63/100 – Loss: 0.3176, Accuracy: 90.42%\n",
      "Epoch 64/100 – Loss: 0.3171, Accuracy: 91.56%\n",
      "Epoch 65/100 – Loss: 0.3123, Accuracy: 89.86%\n",
      "Epoch 66/100 – Loss: 0.3183, Accuracy: 90.78%\n",
      "Epoch 67/100 – Loss: 0.3109, Accuracy: 88.94%\n",
      "Epoch 68/100 – Loss: 0.3145, Accuracy: 89.14%\n",
      "Epoch 69/100 – Loss: 0.3147, Accuracy: 88.12%\n",
      "Epoch 70/100 – Loss: 0.3195, Accuracy: 92.04%\n",
      "Epoch 71/100 – Loss: 0.2979, Accuracy: 90.98%\n",
      "Epoch 72/100 – Loss: 0.2952, Accuracy: 92.28%\n",
      "Epoch 73/100 – Loss: 0.2932, Accuracy: 92.32%\n",
      "Epoch 74/100 – Loss: 0.2866, Accuracy: 92.54%\n",
      "Epoch 75/100 – Loss: 0.2834, Accuracy: 92.24%\n",
      "Epoch 76/100 – Loss: 0.2872, Accuracy: 86.70%\n",
      "Epoch 77/100 – Loss: 0.2946, Accuracy: 92.02%\n",
      "Epoch 78/100 – Loss: 0.2772, Accuracy: 91.86%\n",
      "Epoch 79/100 – Loss: 0.2793, Accuracy: 90.26%\n",
      "Epoch 80/100 – Loss: 0.2843, Accuracy: 92.72%\n",
      "Epoch 81/100 – Loss: 0.2694, Accuracy: 92.72%\n",
      "Epoch 82/100 – Loss: 0.2691, Accuracy: 92.64%\n",
      "Epoch 83/100 – Loss: 0.2741, Accuracy: 89.02%\n",
      "Epoch 84/100 – Loss: 0.2905, Accuracy: 91.42%\n",
      "Epoch 85/100 – Loss: 0.2733, Accuracy: 92.88%\n",
      "Epoch 86/100 – Loss: 0.2657, Accuracy: 92.18%\n",
      "Epoch 87/100 – Loss: 0.2646, Accuracy: 92.64%\n",
      "Epoch 88/100 – Loss: 0.2692, Accuracy: 90.44%\n",
      "Epoch 89/100 – Loss: 0.2667, Accuracy: 92.46%\n",
      "Epoch 90/100 – Loss: 0.2587, Accuracy: 92.64%\n",
      "Epoch 91/100 – Loss: 0.2550, Accuracy: 93.42%\n",
      "Epoch 92/100 – Loss: 0.2509, Accuracy: 92.04%\n",
      "Epoch 93/100 – Loss: 0.2530, Accuracy: 93.26%\n",
      "Epoch 94/100 – Loss: 0.2496, Accuracy: 91.62%\n",
      "Epoch 95/100 – Loss: 0.2520, Accuracy: 93.24%\n",
      "Epoch 96/100 – Loss: 0.2475, Accuracy: 91.74%\n",
      "Epoch 97/100 – Loss: 0.2516, Accuracy: 93.56%\n",
      "Epoch 98/100 – Loss: 0.2404, Accuracy: 93.42%\n",
      "Epoch 99/100 – Loss: 0.2387, Accuracy: 93.74%\n",
      "Epoch 100/100 – Loss: 0.2378, Accuracy: 93.16%\n",
      "Test Accuracy (Mehrklassen): 89.78%\n",
      "\n",
      "Ergebnisse der Mehrklassen-Experimente:\n",
      "Lernrate 0.001, Hidden 16 -> Accuracy: 12.39%\n",
      "Lernrate 0.001, Hidden 32 -> Accuracy: 12.72%\n",
      "Lernrate 0.001, Hidden 64 -> Accuracy: 13.27%\n",
      "Lernrate 0.01, Hidden 16 -> Accuracy: 32.75%\n",
      "Lernrate 0.01, Hidden 32 -> Accuracy: 66.83%\n",
      "Lernrate 0.01, Hidden 64 -> Accuracy: 68.01%\n",
      "Lernrate 0.1, Hidden 16 -> Accuracy: 84.10%\n",
      "Lernrate 0.1, Hidden 32 -> Accuracy: 88.66%\n",
      "Lernrate 0.1, Hidden 64 -> Accuracy: 89.78%\n"
     ]
    }
   ],
   "source": [
    "# Für Experimente begrenzen wir auch hier aus Zeitgründen die Trainingsdaten (z.B. 5000 Beispiele)\n",
    "sample_size = 5000\n",
    "X_train_mc = X_train[:sample_size]\n",
    "y_train_mc = y_train_oh[:sample_size]\n",
    "\n",
    "learning_rates_mc = [0.001, 0.01, 0.1]\n",
    "hidden_sizes_mc = [16, 32, 64]\n",
    "results_mc = {}\n",
    "\n",
    "for lr in learning_rates_mc:\n",
    "    for h_dim in hidden_sizes_mc:\n",
    "        print(f\"\\nTraining (Mehrklassen) mit Lernrate={lr} und Hidden-Dimension={h_dim}\")\n",
    "        model_mc = MultiLayerNN(input_dim, h_dim, output_dim=10)\n",
    "        train_multiclass_nn(model_mc, X_train_mc, y_train_mc, epochs=100, learning_rate=lr, batch_size=64)\n",
    "        test_out_mc = model_mc.forward(X_test)\n",
    "        acc_mc = compute_accuracy_multiclass(y_test_oh, test_out_mc)\n",
    "        results_mc[(lr, h_dim)] = acc_mc\n",
    "        print(f\"Test Accuracy (Mehrklassen): {acc_mc*100:.2f}%\")\n",
    "\n",
    "print(\"\\nErgebnisse der Mehrklassen-Experimente:\")\n",
    "for key, acc in results_mc.items():\n",
    "    print(f\"Lernrate {key[0]}, Hidden {key[1]} -> Accuracy: {acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainiere das beste Modell (Lernrate=0.1, Hidden=64) mit dem vollen Datensatz...\n",
      "Epoch 1/100 – Loss: 1.9372, Accuracy: 64.16%\n",
      "Epoch 2/100 – Loss: 1.0136, Accuracy: 80.40%\n",
      "Epoch 3/100 – Loss: 0.6134, Accuracy: 85.34%\n",
      "Epoch 4/100 – Loss: 0.4805, Accuracy: 87.66%\n",
      "Epoch 5/100 – Loss: 0.4151, Accuracy: 89.03%\n",
      "Epoch 6/100 – Loss: 0.3755, Accuracy: 89.77%\n",
      "Epoch 7/100 – Loss: 0.3480, Accuracy: 90.49%\n",
      "Epoch 8/100 – Loss: 0.3274, Accuracy: 90.90%\n",
      "Epoch 9/100 – Loss: 0.3114, Accuracy: 91.31%\n",
      "Epoch 10/100 – Loss: 0.2976, Accuracy: 91.71%\n",
      "Epoch 11/100 – Loss: 0.2861, Accuracy: 91.97%\n",
      "Epoch 12/100 – Loss: 0.2760, Accuracy: 92.27%\n",
      "Epoch 13/100 – Loss: 0.2671, Accuracy: 92.41%\n",
      "Epoch 14/100 – Loss: 0.2592, Accuracy: 92.74%\n",
      "Epoch 15/100 – Loss: 0.2518, Accuracy: 92.89%\n",
      "Epoch 16/100 – Loss: 0.2451, Accuracy: 93.14%\n",
      "Epoch 17/100 – Loss: 0.2389, Accuracy: 93.26%\n",
      "Epoch 18/100 – Loss: 0.2332, Accuracy: 93.32%\n",
      "Epoch 19/100 – Loss: 0.2276, Accuracy: 93.59%\n",
      "Epoch 20/100 – Loss: 0.2227, Accuracy: 93.73%\n",
      "Epoch 21/100 – Loss: 0.2180, Accuracy: 93.84%\n",
      "Epoch 22/100 – Loss: 0.2132, Accuracy: 93.92%\n",
      "Epoch 23/100 – Loss: 0.2092, Accuracy: 94.13%\n",
      "Epoch 24/100 – Loss: 0.2051, Accuracy: 94.22%\n",
      "Epoch 25/100 – Loss: 0.2012, Accuracy: 94.31%\n",
      "Epoch 26/100 – Loss: 0.1976, Accuracy: 94.42%\n",
      "Epoch 27/100 – Loss: 0.1940, Accuracy: 94.52%\n",
      "Epoch 28/100 – Loss: 0.1906, Accuracy: 94.67%\n",
      "Epoch 29/100 – Loss: 0.1875, Accuracy: 94.70%\n",
      "Epoch 30/100 – Loss: 0.1844, Accuracy: 94.70%\n",
      "Epoch 31/100 – Loss: 0.1812, Accuracy: 94.90%\n",
      "Epoch 32/100 – Loss: 0.1785, Accuracy: 95.00%\n",
      "Epoch 33/100 – Loss: 0.1756, Accuracy: 95.06%\n",
      "Epoch 34/100 – Loss: 0.1728, Accuracy: 95.17%\n",
      "Epoch 35/100 – Loss: 0.1701, Accuracy: 95.19%\n",
      "Epoch 36/100 – Loss: 0.1677, Accuracy: 95.32%\n",
      "Epoch 37/100 – Loss: 0.1652, Accuracy: 95.40%\n",
      "Epoch 38/100 – Loss: 0.1630, Accuracy: 95.43%\n",
      "Epoch 39/100 – Loss: 0.1605, Accuracy: 95.43%\n",
      "Epoch 40/100 – Loss: 0.1585, Accuracy: 95.56%\n",
      "Epoch 41/100 – Loss: 0.1563, Accuracy: 95.56%\n",
      "Epoch 42/100 – Loss: 0.1541, Accuracy: 95.66%\n",
      "Epoch 43/100 – Loss: 0.1521, Accuracy: 95.74%\n",
      "Epoch 44/100 – Loss: 0.1503, Accuracy: 95.73%\n",
      "Epoch 45/100 – Loss: 0.1482, Accuracy: 95.77%\n",
      "Epoch 46/100 – Loss: 0.1464, Accuracy: 95.86%\n",
      "Epoch 47/100 – Loss: 0.1447, Accuracy: 95.90%\n",
      "Epoch 48/100 – Loss: 0.1427, Accuracy: 96.01%\n",
      "Epoch 49/100 – Loss: 0.1410, Accuracy: 95.98%\n",
      "Epoch 50/100 – Loss: 0.1395, Accuracy: 96.07%\n",
      "Epoch 51/100 – Loss: 0.1378, Accuracy: 96.11%\n",
      "Epoch 52/100 – Loss: 0.1362, Accuracy: 96.18%\n",
      "Epoch 53/100 – Loss: 0.1347, Accuracy: 96.19%\n",
      "Epoch 54/100 – Loss: 0.1332, Accuracy: 96.29%\n",
      "Epoch 55/100 – Loss: 0.1315, Accuracy: 96.26%\n",
      "Epoch 56/100 – Loss: 0.1302, Accuracy: 96.33%\n",
      "Epoch 57/100 – Loss: 0.1287, Accuracy: 96.35%\n",
      "Epoch 58/100 – Loss: 0.1273, Accuracy: 96.37%\n",
      "Epoch 59/100 – Loss: 0.1260, Accuracy: 96.46%\n",
      "Epoch 60/100 – Loss: 0.1247, Accuracy: 96.40%\n",
      "Epoch 61/100 – Loss: 0.1234, Accuracy: 96.54%\n",
      "Epoch 62/100 – Loss: 0.1221, Accuracy: 96.54%\n",
      "Epoch 63/100 – Loss: 0.1208, Accuracy: 96.61%\n",
      "Epoch 64/100 – Loss: 0.1196, Accuracy: 96.61%\n",
      "Epoch 65/100 – Loss: 0.1185, Accuracy: 96.69%\n",
      "Epoch 66/100 – Loss: 0.1174, Accuracy: 96.71%\n",
      "Epoch 67/100 – Loss: 0.1161, Accuracy: 96.72%\n",
      "Epoch 68/100 – Loss: 0.1150, Accuracy: 96.74%\n",
      "Epoch 69/100 – Loss: 0.1140, Accuracy: 96.78%\n",
      "Epoch 70/100 – Loss: 0.1129, Accuracy: 96.75%\n",
      "Epoch 71/100 – Loss: 0.1118, Accuracy: 96.85%\n",
      "Epoch 72/100 – Loss: 0.1108, Accuracy: 96.90%\n",
      "Epoch 73/100 – Loss: 0.1098, Accuracy: 96.95%\n",
      "Epoch 74/100 – Loss: 0.1088, Accuracy: 96.91%\n",
      "Epoch 75/100 – Loss: 0.1078, Accuracy: 96.97%\n",
      "Epoch 76/100 – Loss: 0.1068, Accuracy: 96.99%\n",
      "Epoch 77/100 – Loss: 0.1059, Accuracy: 97.04%\n",
      "Epoch 78/100 – Loss: 0.1049, Accuracy: 97.08%\n",
      "Epoch 79/100 – Loss: 0.1040, Accuracy: 97.09%\n",
      "Epoch 80/100 – Loss: 0.1030, Accuracy: 97.02%\n",
      "Epoch 81/100 – Loss: 0.1022, Accuracy: 97.17%\n",
      "Epoch 82/100 – Loss: 0.1015, Accuracy: 97.19%\n",
      "Epoch 83/100 – Loss: 0.1004, Accuracy: 97.18%\n",
      "Epoch 84/100 – Loss: 0.0998, Accuracy: 97.16%\n",
      "Epoch 85/100 – Loss: 0.0989, Accuracy: 97.24%\n",
      "Epoch 86/100 – Loss: 0.0979, Accuracy: 97.26%\n",
      "Epoch 87/100 – Loss: 0.0972, Accuracy: 97.27%\n",
      "Epoch 88/100 – Loss: 0.0963, Accuracy: 97.34%\n",
      "Epoch 89/100 – Loss: 0.0956, Accuracy: 97.31%\n",
      "Epoch 90/100 – Loss: 0.0948, Accuracy: 97.34%\n",
      "Epoch 91/100 – Loss: 0.0940, Accuracy: 97.38%\n",
      "Epoch 92/100 – Loss: 0.0932, Accuracy: 97.37%\n",
      "Epoch 93/100 – Loss: 0.0926, Accuracy: 97.41%\n",
      "Epoch 94/100 – Loss: 0.0918, Accuracy: 97.43%\n",
      "Epoch 95/100 – Loss: 0.0911, Accuracy: 97.44%\n",
      "Epoch 96/100 – Loss: 0.0904, Accuracy: 97.52%\n",
      "Epoch 97/100 – Loss: 0.0897, Accuracy: 97.53%\n",
      "Epoch 98/100 – Loss: 0.0890, Accuracy: 97.54%\n",
      "Epoch 99/100 – Loss: 0.0882, Accuracy: 97.58%\n",
      "Epoch 100/100 – Loss: 0.0876, Accuracy: 97.56%\n",
      "\n",
      "Test Loss: 0.1156, Test Accuracy: 96.55%\n"
     ]
    }
   ],
   "source": [
    "# Bestes Modell: Lernrate 0.1, Hidden 64, volle Trainingsdaten\n",
    "\n",
    "# Parameter festlegen\n",
    "best_lr = 0.1\n",
    "best_hidden = 64\n",
    "epochs = 100      # je nach Rechenleistung ggf. anpassen\n",
    "batch_size = 64   # Mini-Batch Größe\n",
    "\n",
    "print(\"Trainiere das beste Modell (Lernrate=0.1, Hidden=64) mit dem vollen Datensatz...\")\n",
    "\n",
    "# Modell initialisieren\n",
    "model_best = MultiLayerNN(input_dim, best_hidden, output_dim=10)\n",
    "\n",
    "# Verwenden Sie den vollen Trainingsdatensatz:\n",
    "X_train_full = X_train        # X_train enthält alle Trainingsbeispiele\n",
    "y_train_full = y_train_oh     # y_train_oh enthält die one-hot encodierten Labels\n",
    "\n",
    "# Training durchführen\n",
    "train_loss_hist, train_acc_hist = train_multiclass_nn(model_best, X_train_full, y_train_full, \n",
    "                                                      epochs=epochs, learning_rate=best_lr, \n",
    "                                                      batch_size=batch_size)\n",
    "\n",
    "# Evaluation auf den Testdaten\n",
    "test_out_best = model_best.forward(X_test)\n",
    "test_loss_best = cross_entropy_loss(y_test_oh, test_out_best)\n",
    "test_acc_best = compute_accuracy_multiclass(y_test_oh, test_out_best)\n",
    "\n",
    "print(f\"\\nTest Loss: {test_loss_best:.4f}, Test Accuracy: {test_acc_best*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zusatz: Precision, Recall und Konfusionmatrix\n",
    "\n",
    "•\tZunächst werden mit np.argmax sowohl die echten Labels als auch die Vorhersagen in Index-Form (0 bis 9) umgewandelt.\n",
    "\n",
    "•\tAnschließend wird die Confusion Matrix erstellt, in der der Eintrag conf_matrix[i, j] angibt, wie oft ein Beispiel der wahren Klasse i als Klasse j klassifiziert wurde.\n",
    "\n",
    "•\tFür jede Klasse wird dann Precision berechnet als  $\\frac{TP}{TP + FP}$  und Recall als  $\\frac{TP}{TP + FN}$ .\n",
    "    \n",
    "•\tAbschließend werden auch die Macro-Werte (Durchschnitt über alle Klassen) ausgegeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[ 944    0    5    1    4    6    8    6    3    3]\n",
      " [   0 1104    4    5    1    2    3    1   14    1]\n",
      " [  11    9  908   15   15    3   18   19   22   12]\n",
      " [   5    1   24  896    2   40    0   20    5   17]\n",
      " [   0    4    2    0  893    1   13    2    1   66]\n",
      " [  11    5    6   35   27  758   17   12    9   12]\n",
      " [  18    3   16    0   38   16  864    1    2    0]\n",
      " [   1   15   24    2   11    0    0  922    2   51]\n",
      " [  14    8   14   34    7   25   20   17  776   59]\n",
      " [  10    6    3   12   38    6    1   19    1  913]]\n",
      "\n",
      "Precision pro Klasse:\n",
      "Klasse 0: 0.9310\n",
      "Klasse 1: 0.9558\n",
      "Klasse 2: 0.9026\n",
      "Klasse 3: 0.8960\n",
      "Klasse 4: 0.8620\n",
      "Klasse 5: 0.8845\n",
      "Klasse 6: 0.9153\n",
      "Klasse 7: 0.9048\n",
      "Klasse 8: 0.9293\n",
      "Klasse 9: 0.8051\n",
      "\n",
      "Recall pro Klasse:\n",
      "Klasse 0: 0.9633\n",
      "Klasse 1: 0.9727\n",
      "Klasse 2: 0.8798\n",
      "Klasse 3: 0.8871\n",
      "Klasse 4: 0.9094\n",
      "Klasse 5: 0.8498\n",
      "Klasse 6: 0.9019\n",
      "Klasse 7: 0.8969\n",
      "Klasse 8: 0.7967\n",
      "Klasse 9: 0.9049\n",
      "\n",
      "Macro Precision: 0.8986\n",
      "Macro Recall: 0.8962\n"
     ]
    }
   ],
   "source": [
    "# - y_test_oh enthält die one-hot encodierten Ground-Truth-Labels (Form: [num_samples, 10])\n",
    "# - test_out_mc enthält die Softmax-Ausgaben des Modells (Form: [num_samples, 10])\n",
    "# Wir wandeln beide in Label-Indices um.\n",
    "\n",
    "true_labels = np.argmax(y_test_oh, axis=1)\n",
    "pred_labels = np.argmax(test_out_mc, axis=1)\n",
    "num_classes = 10\n",
    "\n",
    "# Confusion Matrix berechnen: Zeilen = wahre Klassen, Spalten = vorhergesagte Klassen\n",
    "conf_matrix = np.zeros((num_classes, num_classes), dtype=int)\n",
    "for t, p in zip(true_labels, pred_labels):\n",
    "    conf_matrix[t, p] += 1\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Precision und Recall pro Klasse berechnen\n",
    "precisions = np.zeros(num_classes)\n",
    "recalls = np.zeros(num_classes)\n",
    "for i in range(num_classes):\n",
    "    TP = conf_matrix[i, i]\n",
    "    FP = np.sum(conf_matrix[:, i]) - TP  # Fälschlicherweise als i klassifizierte Beispiele\n",
    "    FN = np.sum(conf_matrix[i, :]) - TP  # Beispiele der Klasse i, die fälschlicherweise anders klassifiziert wurden\n",
    "    precisions[i] = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "    recalls[i] = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "\n",
    "print(\"\\nPrecision pro Klasse:\")\n",
    "for i in range(num_classes):\n",
    "    print(f\"Klasse {i}: {precisions[i]:.4f}\")\n",
    "\n",
    "print(\"\\nRecall pro Klasse:\")\n",
    "for i in range(num_classes):\n",
    "    print(f\"Klasse {i}: {recalls[i]:.4f}\")\n",
    "\n",
    "# Optional: Berechnung des durchschnittlichen (Macro-)Precision und -Recall\n",
    "macro_precision = np.mean(precisions)\n",
    "macro_recall = np.mean(recalls)\n",
    "print(f\"\\nMacro Precision: {macro_precision:.4f}\")\n",
    "print(f\"Macro Recall: {macro_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vta_mc_1-eS1Wwm3t",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
