{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Mini Challenge: Gradient Descent – Notebook\n",
    " \n",
    " **Inhalt:**\n",
    " \n",
    " 1. **Aufgabe 1:** Laden und Erkunden des MNIST-Datensatzes  \n",
    "    - Verwendung von `torchvision` und `matplotlib` zur Visualisierung und Analyse der Daten.\n",
    " \n",
    " 2. **Aufgabe 2:** Implementierung eines linearen Layers  \n",
    "    - Erstellung einer Klasse `LinearLayer` mit Methoden für Forward-Pass, Backward-Pass und Parameter-Update.  \n",
    "    - Unittests anhand eines kleinen Beispiels mit handberechneten Ergebnissen.\n",
    " \n",
    " 3. **Aufgabe 3:** Aufbau eines einfachen neuronalen Netzwerks (mit einem Hidden Layer) zur binären Klassifikation  \n",
    "    - Ziel: Erkennung einer bestimmten Ziffer (z.B. 7).  \n",
    "    - Implementierung der Aktivierungsfunktionen (ReLU, Sigmoid), der Kostenfunktion (Binary Cross Entropy) sowie des Trainingsloops.\n",
    " \n",
    " 4. **Aufgabe 4:** Training des Netzwerks für verschiedene Hyperparameter  \n",
    "    - Variation von Lernraten und Hidden-Layer-Größen und Auswertung anhand von Kosten- und Evaluationsmetriken.\n",
    " \n",
    " 5. **Aufgabe 5:** Erweiterung des Netzwerks auf drei Hidden Layers für die Mehrklassenklassifikation  \n",
    "    - Implementierung eines Netzwerks mit Mini-Batch Training, Softmax-Aktivierung, Cross-Entropy-Kostenfunktion und Experimenten zu Lernraten und Layer-Größen.\n",
    " \n",
    " *Hinweis:* Für Aufgabe 1 wird `torchvision` verwendet – in den folgenden Aufgaben kommen nur noch `numpy`, `matplotlib` und Python Built-ins zum Einsatz.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 1: MNIST-Datensatz laden und erkunden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainingsdaten:  60000\n",
      "Testdaten:  10000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Für Aufgabe 1: Verwendung von torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Definieren des Transforms: Wir wandeln das Bild in einen Tensor um.\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# MNIST-Datensatz laden (Training und Test)\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset  = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "print(\"Trainingsdaten: \", len(train_dataset))\n",
    "print(\"Testdaten: \", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x300 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualisierung einige Beispiele aus dem Trainingsdatensatz\n",
    "fig, axes = plt.subplots(1, 5, figsize=(12,3))\n",
    "for i, ax in enumerate(axes):\n",
    "    img, label = train_dataset[i]\n",
    "    ax.imshow(img.squeeze(), cmap='gray')\n",
    "    ax.set_title(f\"Label: {label}\")\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsAAAAHWCAYAAAB5SD/0AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAeqdJREFUeJzt3QmcTfX7B/BnFmbsjHUsMwxlyZJsoRWR1C8h7Unklwgttj8pVGgRaVEpyq8FFZUWpCIikX3NkiHL2AfDYOb+X5+H7+3cM3fGLHfuHfd83r1uY773zLnnuWe5z/2e53xPiMvlcgkRERERkUOEBnoBiIiIiIj8iQkwERERETkKE2AiIiIichQmwERERETkKEyAiYiIiMhRmAATERERkaMwASYiIiIiR2ECTERERESOwgSYiIiIiByFCTCRQ02ZMkVCQkLk77//drfdcMMN+rgUPffccxrPpeaXX37R5f78889zdd1m16W8TRARpYcJMJEP/Oc//5GCBQvK8ePH053mvvvuk/z588uhQ4dy/Hp79uzRhG/VqlU5nhdlP9k+ePBgoBclaDz00EP6nhYtWlROnTqV5vm//vpLn8fjlVdeSfMFAo8VK1Z4nW/hwoU92pDQ165d26PtzJkzMn78eKlfv74uQ/HixeWKK66QHj16yKZNm3Qa8zoXe2CZvMHrmmlCQ0P1dapXry4PPPCAzJs3T3Lirbfe0i8+l5pPPvlExo0bF+jFIAcKD/QCEAUDJLfffPONzJw5Ux588ME0zyclJclXX30lN998s5QsWdInCfDw4cOlcuXKcuWVV2ZrHvjQvfvuuyUiIiLHy0PBa+7cuX57rfDwcN1XsC917tzZ47mPP/5YIiMj5fTp0xl+McHfZkfHjh3l+++/l3vuuUceeeQROXv2rCa+s2fPlmbNmkmNGjVk6tSpHn/z0UcfaeJqb69Zs2a6r1OxYkUZNWqU/vvkyZOydetW+fLLL+V///ufxoyf+fLly1YCXKpUKU34L7UEeN26ddKvX79ALwo5DBNgIh/1ABcpUkQP5t4SYCS/+LBDopwT586dk9TUVPGFsLAwfZB3SMTQq+90OGvhL/gy1rx5c/n000/TJMDYt9q1aydffPGF17/FF0Ekq3/++adcddVVWXrdP/74Q//2hRdekP/7v//zeO6NN96Qo0eP6r/vv/9+j+eWLl2qCbC9PSPFihVLM/3o0aOlT58+msTiS+2YMWOytPxElHUsgSDygQIFCkiHDh1k/vz5kpCQkOZ5fHgjQUaiDPhARY9HpUqV9EO/WrVq+qFnTW5Rv2lO9+IUYdWqVXVafEg2atRIp+natav7lKr19Ofvv/+uvc34sEUSd/3118vixYuzXCea3jTmtLP1VK85rbxhwwa58cYb9XUrVKggL730Upr57ty5U9+LQoUKSZkyZeSJJ56QOXPmZHj62GrRokX6HqBHEO/LO++8k+606FFr0KCBrqOoqCjt9d61a5fHNGbZcQr9uuuu02W3J0JZdfjwYXn66aelTp06egoep7vbtm0rq1ev9jp9SkqKvma5cuX0fcH7Y1/OzK5bb5YvXy5t2rTRXkK8F1WqVJGHH344yzXAZt1Pnz5dz0JgHWPb7tSpkxw7dkySk5N128Z6RdzYRtGWWffee6/2xJqk0ySoKIHAc+l5/PHHpUSJEtoLnFXbtm3Tn0i+7fAl0RdnbTKC13j99delVq1amnDjfTQmT54sLVq00PcT+z+mefvttz3+Hknz+vXrZcGCBe7jgXWdZfV48+6777qPN9jP8P5b7du3T9crerMxTXR0tNx+++0exwl86ccXlvLly+s0mN/IkSN1OzewjN9++60eD8xyIxYTU1ZLTIiygj3ARD6C3t0PP/xQE4PevXt7JEJI7nBqFYkHehaRtPzzzz/y3//+V2JiYuS3336TwYMHy969e9PUw+EDEKd9UYuID5I77rhDa42HDRumbddee61Oh9O08NNPP2mihaTv2Wef1VpD8yH666+/SuPGjXPtPThy5IgmZ/gygB48XNg1cOBATQKxTICecCwLYu3bt68mfPiC8PPPP2fqNdauXSutW7eW0qVLa7KDXnHEWbZs2TTTokfvmWee0WXp3r27HDhwQCZMmKBJ7sqVK7XO00BtNpYRCTJ66LzNLyu2b98us2bNkjvvvFOTzf3792uijnWPLwlIDOzLig93vF/4EoXtoFWrVlrnje0mJ+sW8zPv2aBBgzRuJCs49Z5dOI2P5cL8cBof7ytO3WOZsB1g3aCHFF+iED+218zAtvPoo4/qspkEHdsHShAy6tnFFwx8kcLrZLUXODY21l1mgSQYpRj+hiQYxwhsr/iCh+QRkOyiFhlfiLBcKPF47LHHNHnt1auXToNtBV8A8IVjyJAh2ma236web/Be4/iCabE94gss1gm2Z1OagXIRJNx4TSSq2L7QEx4fH+9OYLHesTxPPvmk/sS2i3WTmJgoL7/8sk6DZUWyv3v3bnnttde0zdRrY7lOnDjhsWyYBvtDbn8hIYdwEZFPnDt3zhUdHe1q2rSpR/vEiRNd2NXmzJmjv48cOdJVqFAh15YtWzymGzRokCssLMwVHx+vv+/YsUP/rmjRoq6EhASPaf/44w99bvLkyR7tqamprssuu8zVpk0b/beRlJTkqlKliuumm25yt+FvMQ+8jnH99dfrI6Np4Oeff9Z2/LT+Ldo++ugjd1tycrKrXLlyro4dO7rbXn31VZ1u1qxZ7rZTp065atSokWae3rRv394VGRnp2rlzp7ttw4YN+t5ZD2l///23tr3wwgsef7927VpXeHi4R7tZdqyrzHj22Wd1+gMHDqQ7zenTp10pKSkebXgfIyIiXCNGjEjzXlaoUMGVmJjobp8+fbq2jx8/PsfrdubMmfo7tpussm8TZnlr167tOnPmjLv9nnvucYWEhLjatm3r8ffYH2JjYy/6Ol26dNH9Ajp16uRq2bKl/hvvIbah4cOHu/eJl19+Oc3yzJgxw3X06FFXiRIlXP/5z3+8ztca0xVXXOH+He+n2QbKli2rsbz55pse25g3vXr18tjmLsb+unZmPZl1btavHbaBuLg4jzbM17qejKweb0qWLOk6fPiwe7qvvvpK27/55hv9/ciRI2nWgTfelvu///2vq2DBgrpvGO3atcvU9mH2B+u+Q5QTLIEg8mEPDnoPlyxZ4nEqED0q6I1p2bKl/j5jxgzttcXpWowiYB7o7cPpwYULF3rMF70t6LnLDPSOmFPF6NE080avK14f8/ZVDbE36L2x1jeifhS9kug9Mn744Qc9bW7KQQClDLjw6GLw/qA3vX379tqTZb3oCKf3rdCDiFjR+2t9n9HjfNlll6XpcUbvOk7r+grmh95Qs9xYH3h/cNU/eijtUDuOUgIDJQU4tfzdd9/leN2anm7UueLiLl/A8lov1mrSpAkywTRlFWhHKQd66jMLMeI0N061o+cQPzMqfzBQFoJT/V9//bX28GcWejqxXT3//PO6X6IGGb2r6Bm+6667PMoxcpPp/bSOJmN6/wG9pVjn6NHFPmUtlUhPVo83iBfTGuYMk9mHsTzYr7F+0NOfHutyIx68JuaFHmkzqkZm4YwJtiuUWQwdOjRLf0uUHpZAEPm4DAKn6ZD0op4Tp/ZwahoXuJgLzpDErFmzJt2k1l5DjNPHmYV5Q5cuXdKdBh+a1g84X0JNoH0sXrwW4jVQ74d6QPt0qEu8GJQwYIgsJLB2SCxNsmjeCyRk3qYF+5X2SMp9ecEXklEMq4Wa7R07dnjUPno7hWtfTrw/eE/Ml6mcrFskTPgihZpdbJ+ovcSXCCSV2R0FxPoFxCSfgDpTezveCyxbZk9d33LLLfplYNq0aZr4ow7V+l5kBGU1iBElGKhDzSy8DzgljwdKA1BPi/WHkiZsK6glzwyctreeusd+n9kvsObvrF+EUN+Nchd8sUbyaIX31Lzv6cnq8ca+Xs32ZJJdvE+oH37qqaf0i/3VV18tt956q34hwpdLAyUSSFbxBQZlD/blziz8LUowsH9i1I1LcaxvypuYABP5EGozUauIHiQkwPiJJMw6+gOSgZtuukkGDBjgdR6XX355uj0pF2N6AFFjl97waPYxUTOS3oeNNZmzSm9UCbwH/ob3AsuPC6q8LZf9fcjK+5wZL774otZzoucKF//gAjz0CKOHMju98DlZt+ZGG6jJRQ0pejuxXK+++qq2ZWWbuNi69sU2gCQLSQ9q6tHzmJUL20wvMP4mK73AVuh5x9kcfGlA/S2SYNS0ZqY2GBeR4YuGgV7kzN6QBMOBWb8M4uI89O7jmDJ27Fj9coEvafiihyQ/M9tRVo83mVl/eH9vu+02rXHHtoTtHDXhSHYxjjJ6zPGlC3XZI0aM0C+8OMuDMx+occ/K9o9h3TDs47Jly3R+RL7CBJjIx5Ds4gMBvS7oCUbPnhm1AfBhgJ4enILMrvQSU8wb8EGRk/nbe3/sp4DRi5tdSAhwShMfqNY4cCHVxaAXC4mq6Q212rx5c5r3Aq+BHnT7h7w/IOHEaBjvv/++RzveS4zEYGePCcuO96Ru3bo+W7forcMDF9xh28S2+tlnn+kFgnkNeqc/+OAD/dKAZDQrkKDhIiokotYLHbMKPb94/7FuTPnMxaAn9JprrsnyFyt8qcQ6wcge5u/xZQUjaKCkw9oz6+2C0YyOCTk93qQ3X/QC44H3B1/K8IUKPeUoj0CZDsqQcMGpgTMhmV1uMzwckmzMB18CiHyJNcBEPmZ6e3HFM07f2sf+RU0qTmei58QOyVFmaiUxTJaZ3t4DjQ8m9ELZr6A2JQRZYZIua50gPqgxTFJ2oVYXV6TjQ93AKBfvvffeRf8WvVP4e3wo4opzY+PGjWneT/QgYnokQfbeR/zuizvyXWxZ7a+LekzE7g1O71prP5FA41S8GT0jJ+sWp6/ty2J6kbMyRJk/4csDes4xLFhmEk9vvcAogcjM3RKRwFm3JwP7F/ZVfBHMbBlDXFycJpvm4W1oNTvsUyiTwnaMn6an0/TGWtcdygcw8oe3Y4K3WmVfHG+sUIZhvxkJtkuUbZhtydty4057KAfyttzeSiJ+/PFHLaFASQrKdYh8jT3ARD6GHkcMSWbqD+0JcP/+/TX5Q90cTu8hscGFTBjeC0kPTpd66yG0f+CgZ2vixIn6wYMPEVxshNeeNGmSJk04dYuLulA7h6QLvUb4YM3KnbIwD/QYYsgkDOeG0/joMczqh6YVhldCUoMhn1CvidPN5i5fcLEaPyS0uJAOF9RgOCgsC4bgwrJaa43xHuGiJiw73lN8iOK9Qi8U7tiHIeQwTm9O4LS0/WYZ6LFE+QvWL07/Yh1ge8D6RZxIkLzBe4ueP0yPIdPQg4lT4ebiQMw3u+sWpQRIPjCEHt4XJNr4woG/Qb1tXoR4c3LBk6kFxrjL5gtjejANepzx3mK7wrrA+4r3DaffsS58ddMYJHumnhjJpLkTHMod0NONpN/A0HUoeUC5AfYbfPHBesOYwPhyZIXjCIZMwzaP7QbTYHg8XxxvrLZs2aJlGUisMSYxykKwP2GbNT312N7xpQH16kjosU/jbnneymCwPKj1xnBpOFOGchzEi+MDvnTgDJq9/holHTkdppCIw6AR5QIMoYTdq3Hjxl6fP378uGvw4MGuatWqufLnz+8qVaqUq1mzZq5XXnnFPbSUtyGfrDA8Ua1atXRIL/uQaCtXrnR16NBBhzTCsFsYZqhz586u+fPnZ2kYNNi2bZurVatWOh8MEfV///d/rnnz5nkdBs3bEE8Yhso+zNH27dt1+KMCBQq4Spcu7XrqqadcX3zxhc5z6dKlF31/FyxY4GrQoIG+dxgOCsOXmaHJ7DDfa665RoeCwgPDrWH4qs2bN1902dNjXsvbA0NLAYZ6QlwYGg9xNm/e3LVkyZJ0hxX79NNPdZsoU6aMTo/3x9swXNlZt3/++acO7RUTE6N/g9e49dZbXcuXL79orOktL4YdszKvaR9qLTNDxqU3XJndxYZBszOvfbFh0Pbv3+8aPXq0tmN9YZ/CcGotWrRwff755z4dBs26rRQuXFiHtrv//vtdc+fO9fo3X3/9tatu3bo69F/lypVdY8aMcX3wwQdp9t19+/bpNlOkSBF9zrrOcnq8QTveSzh48KDGjf0I72uxYsVcTZo00WHKrBYvXuy6+uqrdVsuX768a8CAAToUpP24ceLECde9997rKl68uD5njhXp7V+ZGSqRKDNC8L9AJ+FEROhlw40MMHIGejaJiIhyCxNgIvI7DGVmvTgINYW4ehy1kDjFSkRElJtYA0xEfocL1HBVOy7EMjWRGBwfNbJERES5jQkwEfkdRnLABV1IeNHri4tpcHEd7kJFRESU21gCQURERESOwnGAiYiIiMhRmAATERERkaOwBjgTcN9yDIaOQfQvNkg/EREREfkfqnpxo5/y5cvrzXQuNnHAYMBrb4NcP/bYY/r8qVOn9N9RUVE64DYGf8dg31YYKP6WW25xD6j/9NNPu86ePesxDQbNrl+/vg4AXrVqVY8bBmTGrl27MhyUmw8++OCDDz744IMPyRMP5G0XE9Ae4D/++EOvADfWrVuntzi888479XcMiv/tt9/KjBkz9N7uvXv31uGTFi9erM/jb9u1a6f3if/tt9/01pAPPvig5MuXT1588UWdBrc9xTSPPvqoXnE+f/586d69u95+FVeiZwZ6fmHXrl3ue7QTERERUd6RmJgolSpVcudtl8woEP369ZPZs2fLX3/9pUHgPuCffPKJdOrUSZ/HOKE1a9aUJUuWyNVXXy3ff/+93t8c5QnmvuATJ06UgQMHyoEDB/Qe6vg3kmgk1wbuV3706FH54YcfMrVcWBYk4BivlAkwERERUd6TlXwtz9QAnzlzRgfDf/LJJ7XOdsWKFXL27Flp1aqVe5oaNWro4PkmAcbPOnXquJNfQK9uz549Zf369XpnKUxjnYeZBsl2epKTk/VhfUPh3Llz+gDUluCB+mA8DNOO3mnrd4v02sPCwjReM19rO1h7yDNqDw8P1/la2zFfTG9fxvTaGRNjYkyMiTExJsbEmC7VmOzTXxIJ8KxZs7RX9qGHHtLf9+3bpz24xYsX95gOyS6eM9NYk1/zvHkuo2mQ1Npvx2qMGjVKhg8fnqZ95cqVUqhQIf03eqerVq2qJRbobTYqVqyoD9zOFd9AjLi4OClTpoz2RON1rUk9YsS8rSu0bt26Gv/y5cs9lqFhw4b6ZWHNmjUeK75Ro0b6euglNxBbvXr15ODBg7J9+3Z3O74doScdPee7d+92tzMmxsSYGBNjYkyMiTFdqjFh+szKMyUQ6JVFUN98843+jtKHrl27evTEQuPGjeXGG2+UMWPGSI8ePWTnzp0yZ84c9/NJSUmapH733XfStm1bufzyy3U+gwcPdk+D51AXjGm9JcDeeoBRU3Lo0CF3l3qgv+UE4zc3xsSYGBNjYkyMiTExprBsxnTkyBEpWbLkpVMCgST2xx9/lC+//NLdhgvbkM2jV9jaC7x//359zkyzbNkyj3nhefOc+WnarNPgjfGW/EJERIQ+7LBx4GFlVp6dWRmZbbfPNzvt2Fi8tae3jFltZ0yMKb12xsSYMlp2xuTMmJDQIIE5ffq01+lQ5piVdl9I7xS5r9oZk29ktIwY6CCr26TXaSUPmDx5snaBo1fWaNCggQaJURs6duyobZs3b5b4+Hhp2rSp/o6fL7zwgiQkJOjfw7x58zS5rVWrlnsa9PhaYRozDyIiIvItdGBhZCacaSXyJXxBRDlF4cKFczSfgCfA6CJHAtylSxePzB31Jd26ddOL4qKiojSpffzxxzVxxQVw0Lp1a010H3jgAXnppZe03nfo0KHSq1cvdw8uhj974403ZMCAAfLwww/LTz/9JNOnT9eRIYiIiMj3n+uoAUWPMm5IgPJG3kSKfAFnFVBbjLrjyy67LN2zFpdEAozSB/TqIjm1e+211/T0DXqAUZOLOuG33nrL/TwCx7BpGPUBiTFqf5FIjxgxwj1NlSpVNNnFmMLjx4/Xbw2TJk3K9BjARERElLXeXyTBuHamYMGCgV4cCjKlS5eWv//+W0swcpIA55mL4PIyjgNMRESUOaj5RQ8wOqAiIyMDvTjkoO0rMQv52kVulExEREREFFyYABMRERGRowS8BpiIiIicAdf84KYK/lCqVCm9e2ygVa5cWe8+m9EdaK1++eUXvd8BxrS13wzMnx566CEdihY3KgtGTICJiIjIL8kv7vBlveNXbsJY/7irWWaT4IuNVPHss8/Kc889l+Xl+OOPP9x3kc2MZs2a6RByqGW9lPz9999al4u7sV155ZWS1zEBJiIiolyHnl8kv6263y0los+P3Z9bjuxNkB8nfaavmdkEGEmnMW3aNBk2bJjef8Cwjjtr7piWmRsvYNSCrMCwceZmXpR7WANMREREfoPkt3RshVx9ZCfBRtJpHuh9RY+w+R09yUWKFJHvv/9eb9SFew0sWrRItm3bJrfffruULVtWE+RGjRrp8K72Eohx48a5f8d8MRzrHXfcocPEYTzbr7/+2qMEAtOg/ACmTJmipRBz5syRmjVr6uvcfPPNHgk77pDWp08fnQ63Ah44cKAOC9u+fXv3NJ9//rnUqVNHe8ZLliwprVq1kpMnT+pzSOZx3wXz97h3gn2QsB9++EGuueYa9zS33nqrxm+g9xfq16+vy3/DDTe4n0O8WHaM2oCzANYhbdFzjOlxN2CUfuA9qVevnixZskRyE3uAKWjrwfJK/RcREQWHQYMGySuvvCJxcXFSokQJ2bVrl9xyyy16V1okxR999JHcdttt2nOc0efP8OHD9QZeL7/8skyYMEHuu+8+2blzp974yxvcUQ+vO3XqVL0/wv333y9PP/20fPzxx/r8mDFj9N+4sRgSTdz3ALW7SCgByfI999yjr4nE+/jx4/Lrr7+6k9xXX31VE+0PPvhA/x6/z5w5U1q0aOFeBiTLSJLr1q0rJ06c0B5yzGvVqlW6TMuWLZPGjRvrF4ArrrhCe7IBy4VpcVMyJMcokXjkkUfc924whgwZojHiCwH+jeXdunVrlm5vnBVMgClo68GyWv9FRESUEdxo66abbnL/joQVvZXGyJEjNXFEj27v3r0zvMAMCR68+OKL8vrrr2sCiZ5db3DTh4kTJ0rVqlX1d8zbetMvJNGDBw/WhBSQbH733Xfu55EAo5e4Q4cOEhsbq23oDTbQQ42/x/OA10KPsxVuSmaFZBnlHRs2bJDatWu7Sz3QO2wt4UDtNBJqM2/0FONv3nnnHY8EGAl9u3bt3F8QkEQjAUaekBuYAFNQ1oNlp/6LiIgoIw0bNvT4HT2huDAOd5w1SSY+19DBkxH0ohroCcVNGxISEtKdHmUBJvmF6Oho9/S46cP+/fu199XAHdJQqoE78gGS9JYtW2rSizvhtm7dWjp16qS92Ph7LHuTJk3cf49eV8RqLYP466+/tCf3999/189WM2/EigTYG/Qao0yiW7du2utr4H2yX+RnfU8QHyBGJsAU9PVgREREeZl9NAf0Ws6bN09P3VerVk3PPCKxxO2gM5IvXz6P31EDaxLKzE6flRv5IiHGcv72228yd+5c7TFGmQGS2fTKLuxQ2oHe4/fee0/Kly+vy4vEN6NY8QUB8DfWBNssU3oxmhE5MnpPcooXwRERERFlw+LFi7WcAaUH6F3FqX9c1OVP6EnFRXgYbs3ARW1//vmnx3RIKps3b67lBStXrtQaXZRr4O/R44pk2NpDu2LFCvfvhw4d0rrmoUOHak8y6oQxTrGVqfnFaxtYLiTL27dv1y8I1oe5aC5Q2ANMREREfoMStWB4DcAFWxi9AL2jSDCfeeaZXO21TM/jjz8uo0aN0sQSJQPo4UWCanpSkdzOnz9fSx/KlCmjvx84cEATWejbt6+MHj1a48Hfjx071j0KBaBUArW97777ribLKHvABYFWmC96wDFaRMWKFXXEByTXSLgxQgX+jRrn5ORkWb58uS4fLqoLFCbARERElOswMg8SJFyf4Q94LbxmbkKi+PDDD+vNK/BaGH4sMTFR/A2vu2/fPnnwwQe1tKBHjx5a62vKDFBjvHDhQr3YDcsXGxurF6a1bdtWn3/qqae0DhgXpWFEB8SEXm3UBwPaPvvsM01kUfZQvXp1vXDPOtQZ6obRhovzUCt87bXX6pBu3bt31xpmjHjRv39/LSNBb3lm74yXW0JcWSkicShsLPjmgg0BGxH5Bk7PoEj/zmf6+LwG+MDOf2TGyNf1FM5VV13l03kTEVH6Tp8+LTt27NBT3OgFdPqtkAMBvdDo3e3cubOOTOGU7SsxC/kae4CJiIjIL5CQOjUpzU0YQxgXt11//fVaYoBh0JAk3nvvvYFetDyLF8ERERERXcJQooAbWeBOdLjQbe3atXpDClPjS2mxB5iIiIjoElapUiUdkYIyjz3AREREROQoTICJiIiIyFGYABMRERGRozABJiIiIiJHYQJMRERERI7CUSCIiIjIL3gjDMormAATERGRX5LfmjVqSNKpU355vYIFCsjGTZsuqST4ueeek1mzZsmqVasCvShBjwkwERER5Tr0/CL5faN7c6kWnfFtanNq695E6T1psb5mZhPgkJCQDJ9/9tlnNUHNDsx75syZ0r59+2z9faDmHcyYABMREZHfIPmtG1tS8pq9e/e6/z1t2jQZNmyYbN682d1WuHDhAC0Z5QZeBEdERESOV65cOfejWLFi2rNqbfvss8/01sKRkZFSo0YNeeutt9x/e+bMGendu7dER0fr87GxsTJq1Ch9rnLlyvrzjjvu0Hma32H06NFStmxZKVKkiHTr1k1Onz7tsUx//PGH3HTTTVrPjGW6/vrr5c8//3Q/n9G8v/rqK7nqqqt0eeLi4mT48OFy7tw59/MhISEyadIk/duCBQvKZZddJl9//bU4BRNgIiIiogx8/PHH2iP8wgsvyMaNG+XFF1+UZ555Rj788EN9/vXXX9fkcfr06dprjOlNMookFiZPnqy9zOZ3TIuSCsxr+fLlmjxbk2o4fvy4dOnSRRYtWiRLly7VJPWWW27R9ozm/euvv8qDDz4offv2lQ0bNsg777wjU6ZM0eW3Gj58uHTu3FnWrFmj873vvvvk8OHD4gQsgSAiIiK6SP3vq6++Kh06dNDfq1Sp4k4skaDiAj8kp9dcc432rKIH2ChdurT+LF68uPYkG+PGjdNeXzzg+eeflx9//NGjF7hFixYey/Huu+/qfBYsWCC33npruvNGYjto0CBdNkAP8MiRI2XAgAEai/HQQw/JPffco/9GIo5EftmyZXLzzTdLsGMPMBEREVE6Tp48Kdu2bdNEFXXA5oGEFe0mkcTIDdWrV5c+ffrI3LlzLzpf9CQ3adLEo61p06Yev+/fv18eeeQRTa5RAlG0aFE5ceKEJtwZWb16tYwYMcJjeTEf9BInJSW5p6tbt67734UKFdL5JyQkiBOwB5iIiIgoHUg44b333kuTsIaFhelP1Nru2LFDvv/+e+3FRVlBq1at5PPPP8/Ra6MH99ChQzJ+/HjtVY6IiNAkGTXHF1tm9AKbHmsr1AQb+fLlEyv0XqempooTMAEmIiIiSgcuUitfvrxs375da2TTg97Tu+66Sx+dOnXSMgLU00ZFRWmimZKS4jE9Lqj7/ffftVbXQJ2v1eLFi7UuGPW5sGvXrjQ3EvE2byTkqEWuVq1ajmIPZkyAiYiIglRu3nktu3dawxi9uc3Xr4HeVJQ2oAwBiW1ycrJeuHbkyBF58sknZezYsXoRW/369SU0NFRmzJihNbmozQVcEDd//nxp3ry59uKWKFFCL1BD6UTDhg21HRfOrV+/Xut1DZQ+TJ06VadJTEyU/v37S4ECBTyWzdu8ccEeaoSxfpCMY5lQFrFu3Tot3SAmwEREREGb/GK4rlO5dOc1JGKbsnCnNSTMuDsbblDhD3gtvKYvdO/eXYcKe/nllzUJRb1snTp1pF+/fvo8hjF76aWX5K+//tKyiEaNGsl3332niSfgAjokyiijqFChgvz999/aU4waYlyYhgvfOnbsKD179pQ5c+a4X/f999+XHj16aI9upUqV9EK1p59+2mPZvM27TZs2Mnv2bK0DHjNmjPYSY1tAHHReiMvlcl34N6UD37rwre/YsWN6ioN8A2MZNmjQQO58po+Ujq3g03kf2PmPzBj5uqxYsUIPHERETj3Gtup+t5SILuPTeR/ZmyA/TvrM6zEWyRzqYTFSgrXeNLd7pH3VQ015W0bbV1byNfYAExERBTEkv77uZMguJKRMSikv4DBoREREROQoTICJiIiIyFGYABMRERGRo7AGmIiCdrglIgocXmNPeXm7YgJMREE33BIRBY65uxhuuWsfs5Yop8xd8Mxd+LKLCTAR5Rh6fpH85uZwS3gNJsBEeR8SE9wAIiEhQX/H+Lm4xS5RTuE2zQcOHNBtKjw8ZyksE2AiCsrhlogocHAXNDBJMJGv4OYi6AzJ6ZcqJsBERETkU0hOcGvgMmXKyNmzZwO9OBRE8ufP777D3iWdAP/zzz8ycOBA+f7777VeqFq1ajJ58mS977Updn722Wf1Fn9Hjx7Ve12//fbben9s4/Dhw/L444/LN998o28Kbic4fvx4KVy4sHuaNWvWSK9eveSPP/6Q0qVL6/S4/SARERHlXjlETms1iYJuGLQjR45oQouCeSTAGzZs0HtalyhRwj0N7q39+uuvy8SJE+X333/X+2/jHte4FZ5x3333yfr162XevHl67+uFCxfqvbOtt8Zr3bq1xMbG6m0bcS/v5557Tt59912/x0xEREREgRXQHuAxY8ZIpUqVtMfXwL2dDfT+jhs3ToYOHSq33367tn300UdStmxZmTVrltx9992yceNG+eGHH7Rn1/QaT5gwQW655RZ55ZVXpHz58vLxxx/rVYMffPCBdp1fccUVsmrVKhk7dqxHomwkJyfrw5pAw7lz5/QB6GnGAwXZeBimPSUlxWOojvTa8c0Yp4rMfK3tgOkz045icMzX2o75Ynr7MqbX7u+Y8BpYH+75iWc9T6q40m1HS0gG7WEhoTpvs1xcT7kfE95vvO9mfWVmPRku/U+8tgO+JCNW+/7H9cSYGFP6Mdn3S7Of2Y+p6bVndAwGzNu6X3I9MabQAMdknz7PJsBff/219ubeeeedsmDBAqlQoYI89thj8sgjj+jzO3bskH379kmrVq3cf1OsWDFp0qSJLFmyRBNg/MTVpib5BUyPNxQ9xnfccYdOc91113kkW3hdJODohbb2OMOoUaNk+PDhaZZ35cqV2gMNKKOoWrWqLiOuSDQqVqyojy1btsixY8fc7XFxcVoLtW7dOo+hojB0FJYf87au0Lp16+ryLl++3GMZECeSeZR0WFd8o0aN9PUwVJSB4Wfq1aunV89v377d4z2sWbOm7NmzR3bv3u1u93dMmFf//v1lW8hJiQwNl5qFy7qnTXGlyprje6VIWIRUK1TK3X465axsPJkgUfkKSkyBf9db4rnTsi3pkJSNKCLREUUlOS5KqvbvLydOnNDnuZ5yNybU+GFdRsdVkYiCkZleT8ahMycl/vRRqRRZXErmP7+Pwd7kRMElNJ06dZJDhw65l4nriTExpovHhOOfdb/E/rQv+bhUKRglRcMj3dPHnzoih84mSfVCpSUy7PwQZrD15EE5npIstYuU0yTa2Hhiv/6OeVv3S64nxlQ6wDFh+swKcQVwpOrIyPM74JNPPqlJMHpx+/btq+UOXbp0kd9++01LJPAGo5je6Ny5s34zmDZtmrz44ovy4YcfyubNmz3mjTcUSWzPnj21/AE9y++88477eZRboCcYP7ESL9YDjJ5q7OhFixbNE99yguGbG3rhsX5vH/iojhzgyx7gg/F75MvRb+k21KBBA66nXI4JpUXNmjWTDoMek1Ix5dOsj5z0ACfs/EdmjX5b1+WVV17pt5iCcT0xJmfFZN8vfdkDfGDnP/LVmImyePFi937J9cSYQgMcEzo1S5YsqUm2ydfyZA8w3hxk7UhioX79+votwCTAgRIREaEPO2wc9nHnzMqzS6/oP7329Mazy0o7NhZv7ektY1bbfR0TXsMMaG092Np5a3dZTo97a0cPMuZthkkJ9Hry513SArXt4f3G+25dXxdbT5ltRw8z1p/9tbk/MSbGlH57evtlVo61GbVj3t72S66n3InJH58jIUG4nvJkAoxe3Vq1anm0oTf2iy++8BhHcP/+/R49wPjdfOPENPZxBvGNASNDmL/HT/yNlfndTEOUW3iXNCIiygl+jvheQBNgnP62ly6gPgSjNQDKFpCgzp8/353wohwBtb0obYCmTZvq8Gg41YNT3fDTTz9p7zJqhc00Q4YM0V4kc4tGjBhRvXr1NPW/eYk/ew0p9/AuaUR5E4+xdKng50iQJcBPPPGE1iehBAJ1vcuWLdOhyczwZOiK79evnzz//PM67i8S4meeeUZHdmjfvr27x/jmm2/WC+dQOoEkt3fv3nqBHKaDe++9V+uBu3XrpmMOo8wC4wS/9tprklfx217w4V3SiPIOHmPpUsTPkSBJgHHF4cyZM2Xw4MEyYsQITXAx7BnG9TVws4qTJ0/qcGXo6b3mmmt02DNzAR1gmDMkvS1btnTfCANjB1uvVpw7d67eCAO9xPhmPmzYMK9DoOUV/LZHRJR7eIwlcraA3wnu1ltv1Ud60AuM5BiP9ERFRcknn3yS4etg6Ixff/1VLjX8tkdElHt4jCVypoAnwERElxLWjRLlPdwvKauYABMRZRLrRonyHu6XlB1MgImIMol1o0R5D/dLyg4mwEREWcS6UaK8h/slZUXa23QQEREREQUxJsBERERE5ChMgImIiIjIUZgAExEREZGjMAEmIiIiIkdhAkxEREREjsIEmIiIiIgchQkwERERETkKE2AiIiIichQmwERERETkKEyAiYiIiMhRmAATERERkaMwASYiIiIiR2ECTERERESOwgSYiIiIiByFCTAREREROQoTYCIiIiJyFCbAREREROQoTICJiIiIyFHCA70ARESUt8THx8vBgwdzZd6lSpWSmJiYXJk3EVFmMQEmIiKP5LdGjRpy6tSpXJl/gQIFZNOmTUyCiSigmAATEZEben6R/LbqfreUiC7j03kf2ZsgP076TF+DCTARBRITYCIiSgPJb+nYCoFeDCKiXMGL4IiIiIjIUZgAExEREZGjMAEmIiIiIkdhAkxEREREjsIEmIiIiIgchQkwERERETkKE2AiIiIichQmwERERETkKEyAiYiIiMhRmAATERERkaMwASYiIiIiR2ECTERERESOwgSYiIiIiByFCTAREREROQoTYCIiIiJyFCbAREREROQoAU2An3vuOQkJCfF41KhRw/386dOnpVevXlKyZEkpXLiwdOzYUfbv3+8xj/j4eGnXrp0ULFhQypQpI/3795dz5855TPPLL7/IVVddJREREVKtWjWZMmWK32IkIiIiorwl4D3AV1xxhezdu9f9WLRokfu5J554Qr755huZMWOGLFiwQPbs2SMdOnRwP5+SkqLJ75kzZ+S3336TDz/8UJPbYcOGuafZsWOHTnPjjTfKqlWrpF+/ftK9e3eZM2eO32MlIiIiosALD/gChIdLuXLl0rQfO3ZM3n//ffnkk0+kRYsW2jZ58mSpWbOmLF26VK6++mqZO3eubNiwQX788UcpW7asXHnllTJy5EgZOHCg9i7nz59fJk6cKFWqVJFXX31V54G/R5L92muvSZs2bfweLxERERE5PAH+66+/pHz58hIZGSlNmzaVUaNGSUxMjKxYsULOnj0rrVq1ck+L8gg8t2TJEk2A8bNOnTqa/BpIanv27Cnr16+X+vXr6zTWeZhp0BOcnuTkZH0YiYmJ+hOlFaa8IjQ0VB+pqan6MEw7eqddLtdF28PCwrT0w162YaYJCwmVUAlxt6fK+XZr28Xa0RJiaQ8NCXW/hvV1czsmtAOm12VLTdUvKe755SAmezveN8zbLJe/YvLWbuLEMmUU68ViMlz6H9rObx+Yt4nPXzHZv8RiviZGE1t2Y7K3Q758+TQ++/6X2zFZ2zFf89re9susxOTysg2Yaez7ZW7HhOmt+wd+4v0+v0w5i8nebt8v/RWTt3azX4ZeWK/Zjcnb8cq6X5pY/RGTYfYP+36Zk5i8tZsYzXr0R0ze2gHbrHW/zG5M9nazTNY4/RGT/fhmju9ZWfbMtodd2F71ffNjTBm1Z/e4Z58+zybATZo00ZKF6tWra/nD8OHD5dprr5V169bJvn37dIUUL17c42+Q7OI5wE9r8mueN89lNA2S2lOnTkmBAgXSLBeScCyL3cqVK6VQoUL679KlS0vVqlW1xOLAgQPuaSpWrKiPLVu2aC+2ERcXpzXKiA2va03qESPmbV2h2CAQ/zVxtSWiYKS7fXXiHskfGiY1C/8bU4orVdYc3ytFwiKkWqFS7vbTKWdl48kEicpXUGIKlHC37yuXT6aJSFJSkixfvtzdntsx1a1bV2Myr4l5oWZ7W8hJiQwNz1FMiedOy7akQ1I2oohERxSV5Lgoqdq/v5w4cUKf91dMRsOGDbU0Z82aNe44y1SOkc2pR7Mdk3HozEmJP31UKkUWl5pxJTXOQ4cOaYmQv2KyHnQaNWqkX1YRY3RcFd1ecxJTyfzn9zHYm5woCSLSqVMnjdEskz9iwnu4adMmd7s5VmBe1v0yOzHtSz4uVQpGSdHwf/ftNUWS9OeRI0c8ljO3Y6pXr54cPHhQtm/frm2YBu83PkZyGlP8qSNy6GySVC9UWiLD8rn3S2wv4K+YoFixYnoGEPvJ7t273ftlSKliclhc2Y7J2HryoBxPSZbaRcrJuQtxYpvF9umvmOzHchz/rPtlTmKyfoHfeGK//o55W/dLf8Tk7VgO2GabW/bL7MR0JjVF6hUt77Ge5ofs0euQrHH6Iyb7sRw/sX9ATmOy5xHJcVFSqW9f9+v4K6bcOJZj+swKcVlT7gA7evSoxMbGytixY/VN79q1q0dPLDRu3FjreceMGSM9evSQnTt3etTzIqlDkvrdd99J27Zt5fLLL9f5DB482D0NnkNdMKb1lgB76wGuVKmS7gBFixb1y7ec1atX6wq9e1g/KRVT3qc9wAfi98i0EeN0A8KGHahvbqjJbt68udw+8FEpHVvBpz3AB+P3yJej39La8AYNGgS0B9jE2WHQYxIVE53tmLz1wh2K36txLl68WC/0DFQPMM7YNGvWTGM026uveoATdv4js0a/resSZU7+islbTwj2S3zwdR7aJ81+mdPeUsQ5feT4NPulv3uAsb1iXbYf1FPKxFbwaQ+wfb8MZA+w2S/vGNRTSsaU92kPsIkT+yXi9FdMhtk/7PulL3uAD+z8R74aM1FjNPtloHqAsS5xVrjj4F7u/dJXPcD7d+6Wz5+fIMuWLXPH6Y+Y7Mc3xIhOw45DekvZ2Io5isnefvDC9ooz5jhzfin3AKMDAV9YkGSbfC3PlkBYIdtHwrp161a56aabNJtHUmztBcYoEKZmGD+xUVqZUSKs09hHjsDveGO8Jb+A0SLwsMMGj4eVWXl2ZmVktt0+X3O6FT2hZmO18taWXrvLcipZp3Glul/D/rq5GZO9Ha+BdZzRsmc2Jns73jfM23ra2h8xeWs3cWKZchKTt3YTpzU+f8Rkh/fZxGiNLTsxeWtHjyHis792bsfkrR0HdW/7ZVZjSvv3rgxfNzdjsm4/+Gl6aHMak73dvl/6KyZv7Wa/TL3woZvdmLy1W/fL7MSa3Zgyu19mJyZvTIz2Zc3NmNJrxzbrbb/0RaxIzLzFmdsxWY9vJqn0VUze9kt/x5SZdl8cI/LsKBBWOF2zbds2iY6O1m/NqOmZP3+++/nNmzfrsGeoFQb8XLt2rSQk4CTpefPmzdPktlatWu5prPMw05h5EBEREZGzBDQBfvrpp3V4s7///ltPid1xxx36LeCee+7R+pJu3brJk08+KT///LOeykEpAxJXnOqA1q1ba6L7wAMP6KlJlEIMHTpUxw42PbiPPvqo1q0MGDBA61reeustmT59ug6xRkRERETOE9ASCBROI9lFbS0Kp6+55hod4gz/BgxVhi503AADNbkYvQEJrIFkefbs2TrqAxJj1P526dJFRowY4Z4GQ6B9++23mvCOHz9ei7AnTZrEIdCIiIiIHCqgCfBnn32W4fMYGu3NN9/UR3pw0RwuasvIDTfckKUrA4mIiIgoeOWpGmAiIiIiotzGBJiIiIiIHIUJMBERERE5ChNgIiIiInIUJsBERERE5ChMgImIiIjIUZgAExEREZGjMAEmIiIiIkdhAkxEREREjsIEmIiIiIgchQkwERERETkKE2AiIiIichQmwERERETkKFlKgFNSUmThwoVy9OjR3FsiIiIiIqK8kgCHhYVJ69at5ciRI7m3REREREREeakEonbt2rJ9+/bcWRoiIiIioryWAD///PPy9NNPy+zZs2Xv3r2SmJjo8SAiIiIiysvCs/oHt9xyi/78z3/+IyEhIe52l8ulv6NOmIiIiIgoaBLgn3/+OXeWhIiIiIgoLybA119/fe4sCRERERFRXh0H+Ndff5X7779fmjVrJv/884+2TZ06VRYtWuTr5SMiIiIiCmwC/MUXX0ibNm2kQIEC8ueff0pycrK2Hzt2TF588UXfLh0RERERUV4YBWLixIny3nvvSb58+dztzZs314SYiIiIiCioEuDNmzfLddddl6a9WLFivEMcEREREQVfAlyuXDnZunVrmnbU/8bFxflquYiIiIiI8kYC/Mgjj0jfvn3l999/13F/9+zZIx9//LHeHKNnz565s5RERERERIEaBm3QoEGSmpoqLVu2lKSkJC2HiIiI0AT48ccf99VyERERERHljQQYvb5DhgyR/v37aynEiRMnpFatWlK4cOHcWUIiIiIiokAmwEb+/Pk18SUiIiIiCroEuEOHDpme4ZdffpmT5SEiIiIiCvxFcBjizDyKFi0q8+fPl+XLl7ufX7FihbbheSIiIiKiS74HePLkye5/Dxw4UDp37qw3wwgLC9O2lJQUeeyxxzQ5JiIiIiIKqmHQPvjgAx3xwSS/gH8/+eST+hwRERERUVAlwOfOnZNNmzalaUcbhkcjIiIiIgqqUSC6du0q3bp1k23btknjxo21DTfFGD16tD5HRERERBRUCfArr7yit0N+9dVXZe/evdoWHR2t4wI/9dRTubGMRERERESBS4BDQ0NlwIAB+khMTNQ2XvxGREREREF/Iwxg4ktEREREQZkA169fX2+BnBl//vlnTpeJiIiIiCiwCXD79u1zbwmIiIiIiPJaAvzss8/m/pIQEREREeXFcYCJiIiIiIK+BzgqKkq2bNkipUqVkhIlSmRYD3z48GFfLh8RERERkf8T4Ndee02KFCmi/x43bpzkBtxIY/DgwdK3b1/3a5w+fVrHFv7ss88kOTlZ2rRpI2+99ZaULVvW/Xfx8fHSs2dP+fnnn6Vw4cLSpUsXGTVqlISH/xvaL7/8ordqXr9+vVSqVEmGDh0qDz30UK7EQURERERBkAAjqfT2b1/5448/5J133pG6det6tD/xxBPy7bffyowZM6RYsWLSu3dv6dChgyxevFifT0lJkXbt2umNOX777Te9MceDDz4o+fLlkxdffFGn2bFjh07z6KOPyscffyzz58+X7t276807kFATERERkbPkqAbY5XLJTz/9pEnqkSNHsjWPEydOyH333SfvvfeellcYx44dk/fff1/Gjh0rLVq0kAYNGsjkyZM10V26dKlOM3fuXNmwYYP873//kyuvvFLatm0rI0eOlDfffFPOnDmj00ycOFGqVKmid66rWbOmJtGdOnXSXm0iIiIicp5M3wjj6NGjWp6AcX6vvvpqTShvueUWTUihTJkympDae3EvplevXtpD26pVK3n++efd7StWrJCzZ89qu1GjRg2JiYmRJUuW6DLgZ506dTxKItCri5IIlDtg/GJMY52HmaZfv37pLhPKLfAwzB3vzp07pw9zRzw8UlNT9WGYdvRO4wvCxdrDwsK0ptrM1zDThIWESqj8W3OdKufbrW0Xa0dLiKU9NCTU/RrW183tmNAOmF6XLTVV8ufP/+/8chCTvR3vG+ZtlstfMXlrN3FimTKK9WIxGS79D23ntw/M28Tnr5isUG6E+ZoYTWzZjcneDjirg/js+19ux2RtN9c+4HW97ZdZicnlZRsw09j3y9yOCdNb9w/8xPt9fplyFpO93b5f+ismb+1mvwy9sF6zG5O345V1vzSx+iMmw+wf9v0yJzF5azcxmvXoj5i8tQO2Wet+md2Y7O1mmaxx+iMm+/HNHN+zsuyZbQ+7sL3q++bHmDJqz+5xzz69TxLgp59+WpNJlEB88803cvPNN+vCos3cHnnIkCH6XGahthcJNUog7Pbt26crpHjx4h7tSHbxnJnGmvya581zGU2DpPbUqVNSoECBNK+NGuLhw4enaV+5cqUUKlRI/126dGmpWrWqllgcOHDAPU3FihX1gYsG0YttxMXF6ZeEdevW6etak3rEiHlbVyjeU8R/TVxtiSgY6W5fnbhH8oeGSc3C/8aU4kqVNcf3SpGwCKlWqJS7/XTKWdl4MkGi8hWUmAL/9q7vK5dPpolIUlKSLF++3N2e2zHhyxFiMq+JefXv31+2hZyUyNDwHMWUeO60bEs6JGUjikh0RFFJjouSqv376xkG8FdMRsOGDfUsxJo1a9xxlqkcI5tTj2Y7JuPQmZMSf/qoVIosLjXjSmqchw4dkj179vgtJutBp1GjRvplFTFGx1XR7TUnMZXMf34fg73JiZIgomdtEKNZJn/EhPdw06ZN7nZzrMC8rPtldmLal3xcqhSMkqLh/+7ba4ok6U+cTbMuZ27HVK9ePTl48KBs375d2zAN3m98jOQ0pvhTR+TQ2SSpXqi0RIblc++X2F7AXzEByuhwFhD7ye7du937ZUipYnJYXNmOydh68qAcT0mW2kXKybkLcWKbxfbpr5jsx3Ic/6z7ZU5isn6B33hiv/6OeVv3S3/E5O1YDthmm1v2y+zEdCY1ReoVLe+xnuaH7JGSJUt6xOmPmOzHcvw0HYw5jcmeRyTHRUmlvn3dr+OvmHLjWI7pMyvEZU25M1ChQgX55JNP5Prrr5d//vlHLyZD+cMNN9ygzy9btkz+85//uBPPi9m1a5cu8Lx589wrFfNCKQMugsNrde3a1aMnFho3biw33nijjBkzRnr06CE7d+6UOXPmuJ9HUock9bvvvtOSiMsvv1zngwvsDDyHXmdM6y0B9tYDjHixA5jbP+f2t5zVq1fr+3P3sH5SKqa8T3uAD8TvkWkjxukGhA07UN/cVq1aJc2bN5fbBz4qpWMr+LQH+GD8Hvly9Ft6hgLlM4HsATZxdhj0mETFRGc7Jm+9cIfi92qcqIu/6qqrAtYDjDM2zZo10xjN9uqrHuCEnf/IrNFv67rE8cFfMXnrCcF+iQ++zkP7pNkvc9pbijinjxyfZr/0dw8wtlesy/aDekqZ2Ao+7QG275eB7AE2++Udg3pKyZjyPu0BNnFiv0Sc/orJMPuHfb/0ZQ/wgZ3/yFdjJmqMZr8MVA8w1iXOCncc3Mu9X/qqB3j/zt3y+fMTNMcxcfojJvvxDTE2adJEOg7pLWVjK+YoJnv7wQvbKzo0ceb8Uu4BRgcCvrAgyTb5Wo57gPfv36/JpEmGIyMjNSk0UJpgzfYvBjtmQkKCfmgbCGjhwoXyxhtvaFKLbB6lF9ZeYCwHLnoD/MRGaV9O85z5adqs0+CN8Zb8QkREhD7ssMFbR5ewrjw7szIy226frzndip5Qs7FaeWtLr91lOZWs07hS3a9hf93cjMnejtcwtdrpLXtmY7K3433DvK2nrf0Rk7d2EyeWKScxeWs3cVrj80dMdnifTYzW2LITk7d29BgiPvtr53ZM3tpxUPe2X2Y1prR/78rwdXMzJuv2g5+mhzanMdnb7fulv2Ly1m72y9QLH7rZjclbu3W/zE6s2Y0ps/tldmLyxsRoX9bcjCm9dmyz3vZLX8SKxMxbnLkdk/X4ZpJKX8Xkbb/0d0yZaffFMSLHF8HhjbcuoMnOjYzGBvamZcuWsnbtWv1WYx7o8cQFcebfqOnBqA3G5s2bddizpk2b6u/4iXkgkTbQo4zktlatWu5prPMw05h5EBEREZGzZD5VFpFJkybpWLuAbukpU6bozTHg+PHjWXphjCtcu3ZtjzaULqDr2rR369ZNx+/FjTiQ1D7++OOauOJUB7Ru3VoT3QceeEBeeuklLb/AGL+4sM704GL4M/Qoo0b54Ycf1rKN6dOn68gVREREROQ8mU6AUeKAocoMlBZMnTo1zTS+hKHK0IXesWNHjxthWHuhZ8+eraM+IDFGAo2L9EaMGOGeBkOgIdnFmMLjx4/XImwk8hwDmIiIiMiZMp0A//3337m7JBfu2GaFOmOM6YtHemJjY/Witozg4rqsXBlIRERERMErRzfCICIiIiK61DABJiIiIiJHYQJMRERERI7CBJiIiIiIHCVLCTCGPvvoo4/S3FiCiIiIiCgoE2DcYQPj6p4+fTr3loiIiIiIKC+VQDRu3Fjv1EZEREREFPR3goPHHntM7862a9cuadCggd58wqpu3bq+XD4iIiIiosAmwHfffbf+7NOnj7stJCREXC6X/kxJSfHtEhIRERERBTIB3rFjhy9fn4iIiIgobyfAuPUwEREREZGjxgGeOnWqNG/eXMqXLy87d+7UtnHjxslXX33l6+UjIiIiIgpsAvz222/rRXC33HKLHD161F3zW7x4cU2CiYiIiIiCKgGeMGGCvPfeezJkyBAJCwtztzds2FDWrl3r6+UjIiIiIgpsAoyL4OrXr5+mPSIiQk6ePOmr5SIiIiIiyhsJcJUqVbzeCOOHH36QmjVr+mq5iIiIiIjyxigQqP/t1auX3g4ZY/8uW7ZMPv30Uxk1apRMmjQpd5aSiIiIiChQCXD37t2lQIECMnToUElKSpJ7771XR4MYP368+yYZRERERERBkwDDfffdpw8kwCdOnJAyZcr4fsmIiIiIiPJKAmwULFhQH0REREREQZUAY9SHkJCQTM3wzz//zOkyEREREREFNgFu3769+9+4+O2tt96SWrVqSdOmTbVt6dKlsn79ennsscdyb0mJiIiIiPyVAD/77LMeF8H16dNHRo4cmWaaXbt2+WKZiIiIiIjyzjjAM2bMkAcffDBN+/333y9ffPGFr5aLiIiIiChvJMAYAm3x4sVp2tEWGRnpq+UiIiIiIsobo0D069dPevbsqRe7NW7cWNt+//13+eCDD+SZZ57JjWUkIiIiIgpcAjxo0CCJi4vTG1/873//0zbcAnny5MnSuXNn3y0ZEREREVFeGQcYiS6TXSIiIiJyRA0wEREREdGljAkwERERETkKE2AiIiIichQmwERERETkKEyAiYiIiMhRMjUKxJNPPpnpGY4dOzYny0NEREREFPgEeOXKlZmaWUhISE6Xh4iIiIgo8Anwzz//nLtLQURERETkJ6wBJiIiIiJHyfKd4E6ePCmjR4+W+fPnS0JCgqSmpno8v337dl8uHxERERFRYBPg7t27y4IFC+SBBx6Q6Oho1v0SERERUXAnwN9//718++230rx589xZIiIiIiKivFQDXKJECYmKisqdpSEiIiIiymsJ8MiRI2XYsGGSlJSUO0tERERERBToEoj69et71Ppu3bpVypYtK5UrV5Z8+fJ5TPvnn3/6fimJiIiIiPyZALdv395Xr0dERERElPcT4GeffTZXXvztt9/Wx99//62/X3HFFVpe0bZtW/399OnT8tRTT8lnn30mycnJ0qZNG3nrrbe099mIj4+Xnj176s06ChcuLF26dJFRo0ZJePi/of3yyy96O+f169dLpUqVZOjQofLQQw/lSkxERERElLcF9EYYFStW1DGFV6xYIcuXL5cWLVrI7bffrokqPPHEE/LNN9/IjBkzdOi1PXv2SIcOHdx/n5KSIu3atZMzZ87Ib7/9Jh9++KFMmTJFk2hjx44dOs2NN94oq1atkn79+ulQbnPmzAlIzERERER0iQ2DhqTztddek+nTp2vvK5JPq8OHD2d6XrfddpvH7y+88IL2CC9dulST4/fff18++eQTTYxh8uTJUrNmTX3+6quvlrlz58qGDRvkxx9/1F7hK6+8Ui/SGzhwoDz33HOSP39+mThxolSpUkVeffVVnQf+ftGiRRoDepSJiIiIyFmynAAPHz5cJk2apKUJKCUYMmSIljDMmjXLo+c1O4k1enpxp7mmTZtqr/DZs2elVatW7mlq1KghMTExsmTJEk2A8bNOnToeJRFIalESgV5kXLyHaazzMNOgJzg9KLfAw0hMTNSf586d0weEhobqA3fCs94Nz7QjHpfLddH2sLAwvcDQzNcw04SFhEqo/HsBYqqcb7e2XawdLSGW9tCQUPdrWF83t2NCO2B6XbbUVP2S4p5fDmKyt+N9w7zNcvkrJm/tJk4sU0axXiwmw6X/oe389oF5m/j8FZMVyo0wXxOjiS27MdnbARfbIj77/pfbMVnbzYXAeF1v+2VWYnJ52QbMNPb9MrdjwvTW/QM/zcXNOY3J3m7fL/0Vk7d2s1+GXliv2Y3J2/HKul+aWP0Rk2H2D/t+mZOYvLWbGM169EdM3toB26x1v8xuTPZ2s0zWOP0Rk/34Zo7vWVn2zLaHXdhe9X3zY0wZtWf3uGef3qcJ8McffyzvvfeelhWgl/Wee+6RqlWrSt26dbVntk+fPlma39q1azXhRb0vanhnzpwptWrV0nIFrJDixYt7TI9kd9++ffpv/LQmv+Z581xG0yCpPXXqlBQoUCDNMqGGGIm+3cqVK6VQoUL679KlS2vcKLE4cOCAexr0XOOxZcsWOXbsmLs9Li5OypQpI+vWrdPXtSb1iBHztq5QbBCI/5q42hJRMNLdvjpxj+QPDZOahf+NKcWVKmuO75UiYRFSrVApd/vplLOy8WSCROUrKDEFSrjb95XLJ9NEdCg7lJ4YuR0TthHEZF4T8+rfv79sCzkpkaHhOYop8dxp2ZZ0SMpGFJHoiKKSHBclVfv3lxMnTujz/orJaNiwoZ4dWbNmjTvOMpVjZHPq0WzHZBw6c1LiTx+VSpHFpWZcSY3z0KFDWiLkr5isB51GjRrpl1XEGB1XRbfXnMRUMv/5fQz2JidKgoh06tRJYzTL5I+Y8B5u2rTJ3W6OFZiXdb/MTkz7ko9LlYJRUjT83317TZHzQ0seOXLEYzlzO6Z69erJwYMH3bexxzR4v/ExktOY4k8dkUNnk6R6odISGZbPvV9iewF/xQTFihXTM4DYT3bv3u3eL0NKFZPD4sp2TMbWkwfleEqy1C5STs5diBPbLLZPf8VkP5bj+GfdL3MSk/UL/MYT+/V3zNu6X/ojJm/HcsA229yyX2YnpjOpKVKvaHmP9TQ/ZI+ULFnSI05/xGQ/luMn9g/IaUz2PCI5Lkoq9e3rfh1/xZQbx3JMn1khLmvKnQlIADdu3Kg9sbgVMu4Kd9VVV+kbgx5Xa2CZgQVGKQX+7vPPP9feZdT7IgHu2rWrR08sNG7cWOt5x4wZIz169JCdO3d61PMiqcMyfvfdd3ox3eWXX67zGTx4sHsaPIcEHtN6S4C99QDj4jnsAEWLFvXLt5zVq1frCr17WD8pFVPepz3AB+L3yLQR43QDwoYdqG9uWMe4o+DtAx+V0rEVfNoDfDB+j3w5+i2tDW/QoEFAe4BNnB0GPSZRMdHZjslbL9yh+L0a5+LFi3U/DFQPMM7YNGvWTGM026uveoATdv4js0a/resSZU7+islbTwj2S3zwdR7aJ81+mdPeUsQ5feT4NPulv3uAsb1iXbYf1FPKxFbwaQ+wfb8MZA+w2S/vGNRTSsaU92kPsIkT+yXi9FdMhtk/7PulL3uAD+z8R74aM1FjNPtloHqAsS5xVrjj4F7u/dJXPcD7d+6Wz5+fIMuWLXPH6Y+Y7Mc3xNikSRPpOKS3lI2tmKOY7O0HL2yvOGOOPO5S7gFGBwK+sCCnNPmaz3qAkcHv3btXE2Bk+ajDxQfvH3/8IREREVmdnWby1apV03/jQIH5jB8/Xu666y5Njo8ePerRC7x//34pV66c/hs/sVFa4XnznPlp2qzT4I3xlvwC4vAWCzZ46+gS1pVnZ1ZGZtvt8zWnW9ETajZWK29t6bW7LKeSdRpXqvs17K+bmzHZ2/Ea1hrynMRkb8f7hnlbT1v7IyZv7SZOLFNOYvLWbuK0xuePmOzwPpsYrbFlJyZv7egxRHz2187tmLy146Dubb/Makxp/96V4evmZkzW7Qc/TQ9tTmOyt9v3S3/F5K3d7JepFz50sxuTt3brfpmdWLMbU2b3y+zE5I2J0b6suRlTeu3YZr3tl76IFYmZtzhzOybr8c0klb6Kydt+6e+YMtPui2OEz0aBuOOOO2T+/Pn678cff1yeeeYZueyyy+TBBx+Uhx9+WHIKKxi9r0iGUdNjXgs2b96svcUomQD8RAlFQgJOkp43b948TW5RRmGmsc7DTGPmQURERETOkuUeYAxbZqCXNjY2Vk9nIQm2j+pwMShLQJkCepOPHz+uIz5gzF6UNKC+pFu3bjp+b1RUlCa1SLiRuOJUB7Ru3VoT3QceeEBeeuklrffFhXm9evVy9+A++uij8sYbb8iAAQM0Qf/pp590BAuUbhARERGR82Q5AbZDMmoS0qxCzy16jlFSgYQXxc1Ifm+66SZ9HkOVoQu9Y8eOHjfCsHaZz549W0d9QGKM2l/cCGPEiBHuaTAEGpJdjCmM0gqUcKDOmEOgERERETlTlhNgJJ3XXXedfPHFF9oza62rLV++fJrC5IxgnN+MREZGyptvvqmP9KAHGhe1ZeSGG27I0pWBRERERBS8slwDjGJw9MZihAJzxzbrc0REREREQZUA4wpB9P6i3hdlB1999ZXHc0REREREQdcDjDII1NO+8soreiHc888/z95fIiIiIgr+i+BwIwqM/nDnnXfKwoULfbdURERERER5pQcYF51ZByzGXdlwC+Rdu3b5etmIiIiIiALfA4z7O9vhTm4YZcF+xzUiIiIioqApgcBt8zCOr/3e0EREREREQZUAb9myRe/Qhru/WeEiOCTAWRkHmIiIiIgozyfAXbt2lfDwcL0DW3R0NHt9iYiIiCi4E+BVq1bJihUrpEaNGrmzREREREREeWkUiFq1asnBgwdzZ2mIiIiIiPJaAjxmzBgZMGCA/PLLL3Lo0CFJTEz0eBARERERBVUJRKtWrfRny5YtPdp5ERwRERERBWUC/PPPP+fOkhARERER5cUE+Prrr0/3uXXr1uV0eYiIiIiI8lYNsN3x48fl3XfflcaNG0u9evV8s1RERERERHktAV64cKF06dJFxwJ+5ZVXpEWLFrJ06VLfLh0RERERUSBLIPbt2ydTpkyR999/X0d86Ny5syQnJ8usWbN0eDQiIiIioqDpAb7tttukevXqsmbNGhk3bpzs2bNHJkyYkLtLR0REREQUqB7g77//Xvr06SM9e/aUyy67zNfLQURERESUt3qAFy1apBe8NWjQQJo0aSJvvPEG7whHRERERMGbAF999dXy3nvvyd69e+W///2vfPbZZ1K+fHlJTU2VefPmaXJMRERERBR0o0AUKlRIHn74Ye0RXrt2rTz11FMyevRoKVOmjPznP//JnaUkIiIiIsoL4wDjoriXXnpJdu/eLZ9++qmvlomIiIiIKO/eCAPCwsKkffv28vXXX/tidkREREREeTsBJiIiIiK6VDABJiIiIiJHYQJMRERERI7CBJiIiIiIHIUJMBERERE5ChNgIiIiInIUJsBERERE5ChMgImIiIjIUZgAExEREZGjMAEmIiIiIkdhAkxEREREjsIEmIiIiIgchQkwERERETkKE2AiIiIichQmwERERETkKEyAiYiIiMhRmAATERERkaMwASYiIiIiRwloAjxq1Chp1KiRFClSRMqUKSPt27eXzZs3e0xz+vRp6dWrl5QsWVIKFy4sHTt2lP3793tMEx8fL+3atZOCBQvqfPr37y/nzp3zmOaXX36Rq666SiIiIqRatWoyZcoUv8RIRERERHlLQBPgBQsWaHK7dOlSmTdvnpw9e1Zat24tJ0+edE/zxBNPyDfffCMzZszQ6ffs2SMdOnRwP5+SkqLJ75kzZ+S3336TDz/8UJPbYcOGuafZsWOHTnPjjTfKqlWrpF+/ftK9e3eZM2eO32MmIiIiosAKD+SL//DDDx6/I3FFD+6KFSvkuuuuk2PHjsn7778vn3zyibRo0UKnmTx5stSsWVOT5quvvlrmzp0rGzZskB9//FHKli0rV155pYwcOVIGDhwozz33nOTPn18mTpwoVapUkVdffVXngb9ftGiRvPbaa9KmTZuAxE5EREREDkyA7ZDwQlRUlP5EIoxe4VatWrmnqVGjhsTExMiSJUs0AcbPOnXqaPJrIKnt2bOnrF+/XurXr6/TWOdhpkFPsDfJycn6MBITE/UnyipMaUVoaKg+UlNT9WGYdvRMu1yui7aHhYVJSEhImpINM01YSKiESoi7PVXOt1vbLtaOlhBLe2hIqPs1rK+b2zGhHTC9Lltqqn5Bcc8vBzHZ2/G+Yd5mufwVk7d2EyeWKaNYLxaT4dL/0HZ++8C8TXz+iskqPDxc52tiNLFlNyZ7O+TLl0/js+9/uR2TtR3zNa/tbb/MSkwuL9uAmca+X+Z2TJjeun/gJ97v88uUs5js7fb90l8xeWs3+2XohfWa3Zi8Ha+s+6WJ1R8xGWb/sO+XOYnJW7uJ0axHf8TkrR2wzVr3y+zGZG83y2SN0x8x2Y9v5vielWXPbHvYhe1V3zc/xpRRe3aPe/bpL4kEGG8UEtLmzZtL7dq1tW3fvn26UooXL+4xLZJdPGemsSa/5nnzXEbTILE9deqUFChQIE1t8vDhw9Ms48qVK6VQoUL679KlS0vVqlW1vOLAgQPuaSpWrKiPLVu2uBN6iIuL097tdevW6WtaE3rEh3lbVyg2CMR+TVxtiSgY6W5fnbhH8oeGSc3C/8aT4kqVNcf3SpGwCKlWqJS7/XTKWdl4MkGi8hWUmAIl3O37yuWTaSKSlJQky5cvd7fndkx169bVmMxrYl6o194WclIiQ8NzFFPiudOyLemQlI0oItERRSU5Lkqq9u8vJ06c0Of9FZPRsGFDLctZs2aNO84ylWNkc+rRbMdkHDpzUuJPH5VKkcWlZlxJjfPQoUNaHuSvmKwHHdTx44sqYoyOq6Lba05iKpn//D4Ge5MTJUFEOnXqpDGaZfJHTHgPN23a5G43xwnMy7pfZiemfcnHpUrBKCka/u++vaZIkv48cuSIx3Lmdkz16tWTgwcPyvbt27UN0+D9xsdITmOKP3VEDp1NkuqFSktkWD73fontBfwVExQrVkzP/mE/2b17t3u/DClVTA6LK9sxGVtPHpTjKclSu0g5OXchTmyz2D79FZP9WI7jn3W/zElM1i/wG0/s198xb+t+6Y+YvB3LAdtsc8t+mZ2YzqSmSL2i5T3W0/yQPXoNkjVOf8RkP5bjJ/YPyGlM9jwiOS5KKvXt634df8WUG8dyTJ9ZIS5ryh1A6LH9/vvvtTTBbNAofejatatHbyw0btxY63nHjBkjPXr0kJ07d3rU8yKxQ6L63XffSdu2beXyyy/X+QwePNg9DZ5DXTCmtSfA3nqAK1WqpDtA0aJF/fItZ/Xq1bpC7x7WT0rFlPdpD/CB+D0ybcQ43YCwYQfqmxvqsfGF5/aBj0rp2Ao+7QE+GL9Hvhz9ltaFN2jQIKA9wCbODoMek6iY6GzH5K0X7lD8Xo1z8eLFepFnoHqAcbamWbNmGqPZXn3VA5yw8x+ZNfptXZcocfJXTN56QrBf4oOv89A+afbLnPaWIs7pI8en2S/93QOM7RXrsv2gnlImtoJPe4Dt+2Uge4DNfnnHoJ5SMqa8T3uATZzYLxGnv2IyzP5h3y992QN8YOc/8tWYiRqj2S8D1QOMdYkzwh0H93Lvl77qAd6/c7d8/vwEWbZsmTtOf8RkP74hxiZNmkjHIb2lbGzFHMVkbz94YXvF2XKcNb+Ue4DRgYAvLEiyTb6Wp3uAe/fuLbNnz5aFCxe6k18oV66cZvRHjx716AXGKBB4zkyDDdPKjBJhncY+cgR+x5tjT34BI0XgYYcNHg8rs/LszMrIbLt9vuZ0K3pCzcZq5a0tvXaX5VSyTuNKdb+G/XVzMyZ7O14D6zejZc9sTPZ2vG+Yt/W0tT9i8tZu4sQy5SQmb+0mTmt8/ojJDu+zidEaW3Zi8taOHkPEZ3/t3I7JWzsO6t72y6zGlPbvXRm+bm7GZN1+8NP00OY0Jnu7fb/0V0ze2s1+mXrhQze7MXlrt+6X2Yk1uzFldr/MTkzemBjty5qbMaXXjm3W237pi1iRmHmLM7djsh7fTFLpq5i87Zf+jikz7b44RuTJUSCwUSH5nTlzpvz00096oZoVvjmjrmf+/PnuNgyThmHPmjZtqr/j59q1ayUhASdKz8OIEkhua9Wq5Z7GOg8zjZkHERERETlHQHuAMQQayhy++uorHQvY1OyitgQ9s/jZrVs3efLJJ/XCOCS1jz/+uCauON0BGDYNie4DDzwgL730ks5j6NChOm/Ti/voo4/KG2+8IQMGDJCHH35Yk+3p06fLt99+G8jwiYiIiCgAAtoD/Pbbb2udxg033CDR0dHux7RpuETrPAxVduutt+oNMDA0GsoZvvzyS49uc5RP4CcS4/vvv18efPBBGTFihHsa9Cwj2UWvL2rrMBzapEmTOAQaERERkQMFtAc4M9ffRUZGyptvvqmP9MTGxupFbRlBkp2VqwOJiIiIKDgFtAeYiIiIiMjfmAATERERkaMwASYiIiIiR2ECTERERESOwgSYiIiIiByFCTAREREROQoTYCIiIiJyFCbAREREROQoTICJiIiIyFGYABMRERGRozABJiIiIiJHYQJMRERERI7CBJiIiIiIHIUJMBERERE5ChNgIiIiInIUJsBERERE5ChMgImIiIjIUZgAExEREZGjMAEmIiIiIkdhAkxEREREjsIEmIiIiIgchQkwERERETkKE2AiIiIichQmwERERETkKEyAiYiIiMhRmAATERERkaMwASYiIiIiR2ECTERERESOwgSYiIiIiByFCTAREREROQoTYCIiIiJyFCbAREREROQoTICJiIiIyFGYABMRERGRozABJiIiIiJHYQJMRERERI7CBJiIiIiIHIUJMBERERE5ChNgIiIiInKU8EAvAFFu2rhxY67Mt1SpUhITE5Mr8yYiIqLcxQSYglLSseMSGiJy//3358r8CxYoIBs3bWISTEREdAliAkxBKTnplKS6RN7o3lyqRRf16by37k2U3pMWy8GDB5kAExERXYKYAFNQQ/JbN7ZkoBeDiIiI8pCAXgS3cOFCue2226R8+fISEhIis2bN8nje5XLJsGHDJDo6WgoUKCCtWrWSv/76y2Oaw4cPy3333SdFixaV4sWLS7du3eTEiRMe06xZs0auvfZaiYyMlEqVKslLL73kl/iIiIiIKO8JaAJ88uRJqVevnrz55pten0ei+vrrr8vEiRPl999/l0KFCkmbNm3k9OnT7mmQ/K5fv17mzZsns2fP1qS6R48e7ucTExOldevWEhsbKytWrJCXX35ZnnvuOXn33Xf9EiMRERER5S0BLYFo27atPrxB7++4ceNk6NChcvvtt2vbRx99JGXLltWe4rvvvluv8P/hhx/kjz/+kIYNG+o0EyZMkFtuuUVeeeUV7Vn++OOP5cyZM/LBBx9I/vz55YorrpBVq1bJ2LFjPRJloksZR7sgokDgsYcuVXm2BnjHjh2yb98+LXswihUrJk2aNJElS5ZoAoyfKHswyS9g+tDQUO0xvuOOO3Sa6667TpNfA73IY8aMkSNHjkiJEiXSvHZycrI+rL3IcO7cOX0AXgOP1NRUfRimPSUlRZP4i7WHhYVp+YeZr2GmCQsJlVAJcbenyvl2a9vF2tESYmkPDQl1v4b1dXM7JrQDptdlS031WC85icneHh4apvN2XYg1VULEZTnhESKpEiquC/+3vDcXpkzbnqK/pUiYuELCdd5YfhO3iclbrCZOrMuMYr1YTIZL/0ObSPLxJImMyC8PP/ywvhYe+fLl0/ffwHrAMqTXbl0HcPbsWY0L7RjtYuWqVVKxYsU0688IDw93T2/dXrMbk70dsOz4oDXbJeLAA69r3fZMu3X7Ne06P8u0ph0ftBUqVPAakzVWMw9s8972y6zE5PKyDZhp7PtlZveni7WnFxOmt+7zZls5v0w5i8nejvdN98sL68FfMXlrN9t+6IX1mt2YvB2vTJyZPUb4Kib7sdm+X+YkJns7Rtoxxx4DnU1YJrP9WNuxPIjN/Zoulx5r0msvUriwrFm7Vo891pi8xQp4Tet+mZ2YvLWb9xmvad1efbmeMvOZi+dNrDmNyd4edmF7xTE2vWNqTtrNl5ncyCPs7fbpL8kEGMkvoMfXCr+b5/CzTJkyHs9jo4yKivKYpkqVKmnmYZ7zlgCPGjVKhg8fnqZ95cqVWoYBpUuXlqpVq2qifuDAAfc02Fnx2LJlixw7dszdHhcXp8u6bt06OXXqlLu9Ro0amsRj3tYVig0CG+Q1cbUlomCku3114h7JHxomNQv/+76kuFJlzfG9UiQsQqoVKuVuP51yVjaeTJCofAUlpsC/ce4rl0+mYTSDrVt1JAMDNdJFihSR48ePe5SZFCxYUONGPDiQGZgWf4M6bOuyo9wEcdljqlu3rsa0fPly/R3z69+/v2wLOSmRoeE5iinx3GnZlnRIykYUkeiIolK1QSFpULqypJTC8u6VhMiqkpj/320lKnmXlEreJXsK1pCk8OLu9rKntkmxs/slvlA9ORNWwN1e4eQGKZRyVHYUaShn41Klf/+6cujQIV2X1pgMfCnDe4X6cxNnmcoxsjn1aLZjMg6dOSnxp49KpcjiUq1mfanzdFmpHVNCip7cKWFHtsrZ6IbiKvjv/MMS1knY8d1yttI14spf2N0evme5hJ46KGeqtBIJ/fdQkC9+kci503K0wvWyLv6IbN68WfcVa0zueYeFSaNGjfQDCzFGx1XR7TUnMZXMf34fg73JifL3seNy552d9OBsepu+/fZbPZPz3//+Vw+uxqeffirbt2/XZbEm9u+8845+kUW7FUqiypYpIx9+9JFeZ2CNCett06ZN7mnN89iOrftldmLal3xcqhSMkqLh/+7ba4ok6U8cU6z7Jb74Ixa0WT8gcOzCcQLboVXJkiX1wxJf8A18kGCfxLUS9phQhoZ5430DxN2pUyfBx0hOY4o/dUQOnU2S6oVKS2RYPkmOi5Kq/fvr9gIXO0YYGW173taTPSbzPtasWVP27Nkju3fvdu+XIaWKyWFxZTsmY+vJg3I8JVlqFykn5y7EmdljhK9iMsznE66Hse6XOYnJ+gV+44n9knL6jDz1dH899hSKOP83+bb/KBIeKWdjrvk30NRzkn/Hj5JaoJScK/9vZ1XImROSb9ciSSlSUVLK1P63Pemg7PzzJ5m+Jcx97LnYZy5gm21u2S+zE9OZ1BSpV7S8x3qaH7JH9ymsS7MOfb2eMpNH4Cf2D8hpTPY8IimmqFTq21eHDUWucs8997inRSw4fl555ZXSrl07dzviw/EWHYy4xsrAcRnHZ0yLv9H3KzRU7ujQQX/PaW50sWMEps+sEJe9SyRAcICeOXOmtG/fXn//7bffpHnz5rrB4CI4o3PnzjrttGnT5MUXX5QPP/xQdxIrvJlIYHv27Kn1v0iAsQKNDRs2aCkEfmKjzEwPMC6eww6ADxB/9JauXr1aV+jdw/pJqZjyPu0Bjl+3WWaPf19CQsPc35p02gvfcPElwnzThKz2LOYLD5cNGzdqCUpG39ywo2Ad3z7wUSkdW8GnPcBbl62Wn6bMkK8G3ST1Ykr4tAd4XfxhuX30HFm8eLE0aNDAIyZvsZo4Owx6TKJion3aA7xt2ZoLcbaROjHFsx2TFdphTfwxd5w4cGXUY4X6+mbNmmmMZnv1VQ/w5qUrZcGHX8i4rs0krtyFZMyFZXeJKwTLZJm/u9323d51Yf+ytW/bc1gef/83WbZsmftgnVHvDvZLfPB1HtonzX6Z097Sv9duku9e/0DCwrPeU2+VXi9ceFiY7pfW3m5vPVPYXrEu2w/qKWViK/i0B/hg/B75cvRbenzHvhPIHmCzX94xqKeUjCnv0x5gE2dmjxG+isn+eWPfL33ZA7xl6UpZ+NGXeuypHRN1YboLZ/dsx5QwSdE5WdvPH5HP7zk4Nlnb1+08IO1enCtLly5175cZfeZiXV599dXScXAv937pqx7g/Tt3y+fPT5CpU6dK9erVPV7XF72lSIKR7F4sj0CMOAPecUhvKRtbMUcx2du3Xvi8HPtgI6kaXUxEj6uGS0JcWH8h6bSHiliSbj3f6kq9cPY1VLbvS5Qnpixx7/O53QOML/74woIk2+Rrl1wPcLly5fTn/v37PRJg/G52CEyTkJDg8Xd4s9Ajaf4eP/E3VuZ3M41dRESEPuxwYLKeqrGuPDtrYpmZdvt8zQcgekLNxmrlrS29dpflVDIknTh5fozcrlcHZIxcEyveN2uPck5isrefS005nwi4zh9UkPoh1bPDAdib9NpxIA9xnXOf0jPryb7+rLGaOLEucxKTt/Z/4zx3IcbsxeSNNU5rfN5ixftgYrTGlp2YvLUj2atatqDUjSkmvoQYcdC1x6jPhYR4jRUfVN72y6zGZP/70yeTdL98vUvjXNsv8SUeZ2jsrMcx/DQ9tDmNyd6O980k6BfbbzLbnt56Su/YbNrNfpl64UM3uzF5azdxonPG2zL4uj42vVjT2y+zE5M35thjP4Z4O6aEpNvu8tqOBMfbfplerNhmve2XOY0VpR5YxkDcVMmaL5jk39syGtltP3fhc+T8sKHnv8z4ivkcMft8TnOj7LZ7nVbyKPTaIkGdP3++O+FFTyxqe9GzC02bNpWjR4/qt1zzLfunn37SjQTflMw0Q4YM0Z3D9IhgxAh8k/NW/uAkHCOXKO/hfnnp450ogwdvqhS8ApoAoz4JdagG6mDQzY8aXmwM/fr1k+eff14uu+wyTYifeeYZPa1uyiRQvnDzzTfLI488okOlIcnt3bu3XiBnTr/fe++9Wg6B8YEHDhyodSbjx4+X1157LWBxExFR8GLSFHz4xTT4BDQBRhHzjTfe6P79ySef1J9dunSRKVOmyIABA3SsYAxXhp7ea665Roc9w4VXBoY5Q9LbsmVLPUXQsWNHHTvYWnw+d+5c6dWrl/YS49QRbq7BIdCIiALDKUNnMWkiyrsCmgDfcMMNaYYlskLNyIgRI/SRHvQWf/LJJxm+Dq4a/PXXX3O0rERElDMsDSCivCLP1gATEVFwYWkAEeUVTICJiMivWBpARIHm+/FZiIiIiIjyMCbAREREROQoTICJiIiIyFGYABMRERGRozABJiIiIiJHYQJMRERERI7CBJiIiIiIHIUJMBERERE5ChNgIiIiInIUJsBERERE5ChMgImIiIjIUZgAExEREZGjMAEmIiIiIkdhAkxEREREjsIEmIiIiIgchQkwERERETkKE2AiIiIichQmwERERETkKEyAiYiIiMhRmAATERERkaMwASYiIiIiR2ECTERERESOwgSYiIiIiByFCTAREREROQoTYCIiIiJyFCbAREREROQoTICJiIiIyFGYABMRERGRozABJiIiIiJHYQJMRERERI7CBJiIiIiIHIUJMBERERE5ChNgIiIiInIUJsBERERE5ChMgImIiIjIUZgAExEREZGjMAEmIiIiIkdhAkxEREREjsIEmIiIiIgchQkwERERETkKE2AiIiIichQmwERERETkKI5KgN98802pXLmyREZGSpMmTWTZsmWBXiQiIiIi8jPHJMDTpk2TJ598Up599ln5888/pV69etKmTRtJSEgI9KIRERERkR85JgEeO3asPPLII9K1a1epVauWTJw4UQoWLCgffPBBoBeNiIiIiPwoXBzgzJkzsmLFChk8eLC7LTQ0VFq1aiVLlixJM31ycrI+jGPHjunPw4cPy7lz59x/j0dqaqo+rPPFIyUlRVwu10Xbw8LCJCQkxD1f4/jx4/rzyK59knrmrLs9xXX+tcJCPL+7oD0Er2Npx6ukanuIhIaE/BtPwkH9uTb+qJw48++yiKRKiCtVXDoPy/x1HmgPE9FXMe0pEiIuj/a/9x/XeE6cOKHvlxVi1WVNSdGfiYmJki9fPjmw8x85m5yco5js7YkJh3TeiPFk8tkcxeTZHq4xYt5YfrNtmJi8xWrixLpEnNmN6d92l6S6XB5xrtuVKCeSU7IdkwfX+W3x74RT7jixLu3rzwgPD9ft1cSI7TUnMeE/A21H9iboa2iMZnvNZkxia9+x94j+NDFaY8J+ao3VbNf4ad0vsxMT/sM2YFlyObLvgP5cu+uY536ZxZi8tWObBawna5yIBevVehzDe4H4sV+eSz6To5js7e79cueRC/tl9mPSX13nxIU5h4S590vEiBjsx2Z7rGa/PBS/V/fL7Mb0b3uqbgs4jnnul2ezHZNlYglxpWj73wlJ7v3yyJEjadaf9fPGvl/mJCb7sRn7pTtG936ZvZjs7Tv2HtXlt+6XGX3mYr/ENmvdL7MTk3hpR5zgEWc2YjrfHipimf+Ofce87pfe8gW8F9iGsV+mnDmbo5js7Ynu/fKwnEg+l6OY7HmEfb/MaW6U3ueQacc+oUtnmVd6QlyZmeoSt2fPHqlQoYL89ttv0rRpU3f7gAEDZMGCBfL77797TP/cc8/J8OHDA7CkRERERJQTu3btkooVK2Y4jSN6gLMKPcWoFzbwbRPfzkqWLKnfSC51+BZWqVIl3UCKFi0qwcgJMTolTifE6JQ4nRAjOCFOJ8TolDgTgyhG9Omit7l8+fIXndYRCXCpUqW0e3z//v0e7fi9XLlyaaaPiIjQh1Xx4sUl2GBDv9Q39otxQoxOidMJMTolTifE6JQ4nRCjU+IsGiQxFitWLFPTOeIiuPz580uDBg1k/vz5Hr26+N1aEkFEREREwc8RPcCAkoYuXbpIw4YNpXHjxjJu3Dg5efKkjgpBRERERM7hmAT4rrvukgMHDsiwYcNk3759cuWVV8oPP/wgZcuWFadBeQfGQ7aXeQQTJ8TolDidEKNT4nRCjE6J0wkxOiXOCAfE6NhRIIiIiIiIHFUDTERERERkMAEmIiIiIkdhAkxEREREjsIEmIiIiIgchQmww7z55ptSuXJliYyMlCZNmsiyZcskmCxcuFBuu+02vQsM7to3a9YsCTajRo2SRo0aSZEiRaRMmTLSvn172bx5swSbt99+W+rWresenB1jdn///fcSzEaPHq3bbb9+/SSY4PbyiMv6qFGjhgSbf/75R+6//369a2iBAgWkTp06snz5cgkm+Pywr0s8evXqJcEiJSVFnnnmGalSpYqux6pVq8rIkSP1LmPB5vjx43q8iY2N1VibNWsmf/zxhzgBE2AHmTZtmo6HjOFO/vzzT6lXr560adNGEhISJFhgbGfEhUQ/WC1YsEA/bJYuXSrz5s2Ts2fPSuvWrTX2YIL7uCMhXLFihSYRLVq0kNtvv13Wr18vwQgfOu+8844m/cHoiiuukL1797ofixYtkmBy5MgRad68ueTLl0+/qG3YsEFeffVVKVGihATbdmpdjzgGwZ133inBYsyYMfoF/I033pCNGzfq7y+99JJMmDBBgk337t11HU6dOlXWrl2rnyWtWrXSL3NBD8OgkTM0btzY1atXL/fvKSkprvLly7tGjRrlCkbYvGfOnBnoxch1CQkJGuuCBQtcwa5EiRKuSZMmuYLN8ePHXZdddplr3rx5ruuvv97Vt29fVzB59tlnXfXq1XMFs4EDB7quueYal9NgW61ataorNTXVFSzatWvnevjhhz3aOnTo4LrvvvtcwSQpKckVFhbmmj17tkf7VVdd5RoyZIgr2LEH2CHOnDmjPWn4ZmeEhobq70uWLAnoslHOHDt2TH9GRUVJsMIpyc8++0x7uYPx9uXo0W/Xrp3H/hls/vrrLy1NiouLk/vuu0/i4+MlmHz99dd6p1H0hKI0qX79+vLee+9JsH+u/O9//5OHH35YyyCCBcoA5s+fL1u2bNHfV69erWcs2rZtK8Hk3LlzemxFSaQVSiGC7QyNo+8E53QHDx7UDd1+5zv8vmnTpoAtF+VMamqq1m/h1Gvt2rUl2OCUHBLe06dPS+HChWXmzJlSq1YtCSZI7FGSFMx1d7jeYMqUKVK9enU9bT58+HC59tprZd26dVrLHgy2b9+up81RZvZ///d/uj779Okj+fPnly5dukgwwjUWR48elYceekiCyaBBgyQxMVHr1MPCwvSz84UXXtAvbsGkSJEienxFfXPNmjU1H/j000+1U6xatWoS7JgAE13iPYdIIoL12zoSplWrVmkv9+eff66JBGqggyUJ3rVrl/Tt21dr8Oy9MMHE2nOGGmckxLjoZvr06dKtWzcJli+j6AF+8cUX9Xf0AGPfnDhxYtAmwO+//76uW/TsBxNslx9//LF88sknWruOYxA6GhBnsK3LqVOnag9+hQoVNNm/6qqr5J577tEzxsGOCbBDlCpVSjfu/fv3e7Tj93LlygVsuSj7evfuLbNnz9aRL3DBWDBC75npiWjQoIH2qo0fP14vFgsG+JDBRaj40DHQ24R1igtwkpOTdb8NNsWLF5fLL79ctm7dKsEiOjo6zRcz9Kp98cUXEox27twpP/74o3z55ZcSbPr376+9wHfffbf+jtE8EC9G4Am2BLhq1araqYDyMvR6Yzu+6667tFQp2LEG2CGQSCCBQF2TtccCvwdjTWUww/V9SH5RDvDTTz/pUD1OgW0WSWGwaNmypZZ5oIfJPNCLiFOt+HcwJr9w4sQJ2bZtm37YBguUIdmHI0QNKXq6g9HkyZO11hm168EmKSlJr5Gxwr6I40+wKlSokO6PGM1kzpw5OuJOsGMPsIOgNg3fXvEB27hxYxk3bpx+6+vatasE0wertVdpx44dmkjgArGYmBgJlrIHnJr76quvtIZr37592l6sWDG9eCFYDB48WE+vYr1hrErE/Msvv+jBOVhg/dlrt/FBhHFkg6mm++mnn9bxuZEM7tmzR4diREKBU63B4oknntCLp1AC0blzZx1j/d1339VHsEEiiAQYnyfh4cGXRmBbRc0vjj0ogVi5cqWMHTtWSwWCzZw5c7RTBeVm+OxE7zdqn4MpL0hXoIehIP+aMGGCKyYmxpU/f34dFm3p0qWuYPLzzz/rkGD2R5cuXVzBwlt8eEyePNkVTDAMUWxsrG6rpUuXdrVs2dI1d+5cV7ALxmHQ7rrrLld0dLSuywoVKujvW7dudQWbb775xlW7dm1XRESEq0aNGq53333XFYzmzJmjx5zNmze7glFiYqLug/isjIyMdMXFxemwYMnJya5gM23aNI0P+2a5cuV0qNSjR4+6nCAE/wt0Ek5ERERE5C+sASYiIiIiR2ECTERERESOwgSYiIiIiByFCTAREREROQoTYCIiIiJyFCbAREREROQoTICJiIiIyFGYABMRERGRozABJiLyIdyuOSQkRI4ePepumzVrllSrVk1v/9uvX7902/ztmWeekR49evhsflOmTJHixYvneD54//D+XMyZM2ekcuXKsnz58hy/JhE5CxNgIqIsJLbpPW688UadrlmzZrJ3714pVqyY+2//+9//SqdOnWTXrl0ycuTIdNv8ad++fTJ+/HgZMmSIu+2hhx6S9u3by6Uif/788vTTT8vAgQMDvShEdIlhAkxElAkmsbU/3nnnHU2AH3vsMXdSVq5cOW2DEydOSEJCgrRp00bKly8vRYoU8dqWHegBza5JkyZpTLGxsXIpu++++2TRokWyfv36QC8KEV1CmAATEWWCSWytjyNHjmgP5P/93//JnXfemaYEAv82yW2LFi20Pb02QCJ37bXXSoECBaRSpUrSp08fOXnypHsZcLofvcUPPvigFC1aVMsXTNnBnDlzpGbNmlK4cGG5+eabNTnPyGeffSa33XZblt6DsWPHSp06daRQoUK6fEj6kczboXzhsssuk8jISE3y0ctt9dVXX8lVV12lz8fFxcnw4cPl3Llz6Sb5vXv3lujoaJ0eCfuoUaPcz5coUUKaN2+u8RARZRYTYCKibECCe/vtt8sNN9yQbgkDelg3b96s//7iiy80KU2vbdu2bZq4duzYUdasWSPTpk3ThBjJn9Urr7wi9erVk5UrV2oNLyQlJWn71KlTZeHChRIfH6+JeXoOHz4sGzZskIYNG2Yp5tDQUHn99de1t/XDDz+Un376SQYMGOAxDZblhRdekI8++kgWL16s79Pdd9/tfv7XX3/VBL5v3766DOhBRxKPv/EGr/f111/L9OnT9X37+OOP9YuAVePGjXW+RESZ5iIioixJSUlxtW3b1lWzZk1XYmKix3M///yzC4fWI0eO6O/4id/Rbnhr69atm6tHjx4e8/r1119doaGhrlOnTunvsbGxrvbt23tMM3nyZJ3X1q1b3W1vvvmmq2zZsuku/8qVK/Vv4uPjPdq7dOniuv322zP9PsyYMcNVsmTJNMuydOlSd9vGjRu17ffff9ffW7Zs6XrxxRc95jN16lRXdHS0+3dMP3PmTP33448/7mrRooUrNTU13eUYP368q3LlyplebiKi8MynykREBCh5WLJkiSxbtizb9bt2q1ev1p5f9HAayAVTU1Nlx44dWt4A3nptCxYsKFWrVnX/jnIB1Bin59SpU/oTJQVZ8eOPP2r5waZNmyQxMVHLFk6fPq29vlgGCA8Pl0aNGrn/pkaNGlqisXHjRu2pRZzoGbb2+KakpKSZj/XCvJtuukmqV6+uPeS33nqrtG7d2mMalIzgb4mIMosJMBFRFqDWFOUG3377rda5+gpqaTEyBOp+7WJiYtz/Rv2tXb58+Tx+R13x+Y5U70qVKqU/UcNcunTpTC3f33//rclnz549NXmNiorSEo1u3bppna49cc0oTtT8dujQIc1z3hJy1ArjC8D333+vCXjnzp2lVatW8vnnn3uUdGQ2DiIiYAJMRJRJq1at0oRv9OjRenGXLyHRQ00sxgbObegtxkV0eL3LL788U3+zYsUK7Y1+9dVXtRYYUJdrh15hjMuL3l5A3S7qgE0PNuJEW1bixLLedddd+sDQcegJRtKLJBzWrVsn9evXz/T8iIiYABMRZcLBgwd1jFxc9Hb//ffrOLpWuKFFTnohMZbt1VdfrRe9de/eXXt6kaDOmzdP3njjDfElJLDoRUUPrn3c32PHjmmib1WyZElNWM+ePSsTJkzQ0SNQxjBx4kSvvdGPP/64XryGcgjEg7hMQjxs2DDtSUavNpJZLAvKIpDEPv/8815HnkBJBxJcTDtjxgwdgcN6ww1cABeIsZSJ6NLFUSCIiDIBJQ87d+6U7777ThMy+8Na95oddevWlQULFsiWLVt0KDQkfEgWMU5wbkCSjXIO9OpaYUg2vLb1gZIFjDyBZHTMmDFSu3ZtrVW2DkdmoBQCyfy9996rw5NhWDaMaGGg53z27Nkyd+5cfc+QHL/22mvpjkeMGuuXXnpJa58xPUoxsA5MLzRqsZG0I5kmIsqsEFwJl+mpiYgoKODQ36RJE3niiSfknnvukUsVyiKQnOPCRCKizGIPMBGRA+FCuXfffTfdG1BcCnDxHW7MgSSeiCgr2ANMRERERI7CHmAiIiIichQmwERERETkKEyAiYiIiMhRmAATERERkaMwASYiIiIiR2ECTERERESOwgSYiIiIiByFCTAREREROQoTYCIiIiISJ/l/U6oPUVFZa7IAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Labels extrahieren\n",
    "train_labels = [train_dataset[i][1] for i in range(len(train_dataset))]\n",
    "test_labels = [test_dataset[i][1] for i in range(len(test_dataset))]\n",
    "\n",
    "# Häufigkeiten berechnen\n",
    "train_counts = np.bincount(train_labels, minlength=10)\n",
    "test_counts = np.bincount(test_labels, minlength=10)\n",
    "\n",
    "# Balkenbreite und Positionen\n",
    "bar_width = 0.4\n",
    "indices = np.arange(10)\n",
    "\n",
    "# Farben\n",
    "train_color = \"#6a9c89\"  # Sanftes Grün\n",
    "test_color = \"#e69c5c\"   # Warmes Orange\n",
    "\n",
    "# Balkendiagramm mit nebeneinanderliegenden Balken erstellen\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(indices - bar_width/2, train_counts, width=bar_width, color=train_color, edgecolor='black', label=\"Trainingsdaten\")\n",
    "plt.bar(indices + bar_width/2, test_counts, width=bar_width, color=test_color, edgecolor='black', label=\"Testdaten\")\n",
    "\n",
    "# Achsenbeschriftungen\n",
    "plt.xticks(indices, range(10))\n",
    "plt.xlabel(\"Ziffern (Labels)\")\n",
    "plt.ylabel(\"Anzahl der Bilder\")\n",
    "plt.title(\"Verteilung der Labels im MNIST-Datensatz\")\n",
    "plt.legend()\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hilfsfunktion: Konvertieren des Datasets in numpy Arrays**\n",
    "\n",
    "Für die nachfolgenden Aufgaben (2–5) extrahieren wir die Bilder und Labels aus dem MNIST-Datensatz und wandeln sie in numpy Arrays um.  \n",
    "Dabei wird das Bild (1x28x28) zu einem Vektor (28*28 = 784) umgeformt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (60000, 784)\n",
      "y_train_bin shape: (60000,)\n"
     ]
    }
   ],
   "source": [
    "def dataset_to_numpy(dataset):\n",
    "    X = []\n",
    "    y = []\n",
    "    for img, label in dataset:\n",
    "        X.append(img.numpy().reshape(-1))\n",
    "        y.append(label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_train, y_train = dataset_to_numpy(train_dataset)\n",
    "X_test, y_test   = dataset_to_numpy(test_dataset)\n",
    "\n",
    "# Für die binäre Klassifikation (Aufgabe 3–4) definieren wir:\n",
    "# Ziel: Erkenne die Ziffer 7 (1, falls Label == 7, sonst 0)\n",
    "y_train_bin = (y_train == 7).astype(np.int32)\n",
    "y_test_bin  = (y_test == 7).astype(np.int32)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train_bin shape:\", y_train_bin.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 2: Implementierung eines linearen Layers\n",
    "\n",
    " Wir erstellen eine Klasse `LinearLayer`, die folgende Methoden besitzt:\n",
    " \n",
    " - `forward(x)`: Berechnet die Ausgabe \\( z = xW^T + b \\)\n",
    " - `backward(d_out)`: Berechnet die Gradienten (basierend auf dem gespeicherten Input)  \n",
    " - `update(learning_rate)`: Aktualisiert die Parameter mittels Gradient Descent\n",
    " \n",
    "Anschließend folgt ein Unittest, der anhand eines kleinen Beispiels (mit handberechneten Ergebnissen) die Funktionsweise überprüft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        # Initialisierung der Gewichte (W) und Bias (b)\n",
    "        # He-Initialization für ReLU: Skaliere die zufälligen Gewichte mit sqrt(2/input_dim)\n",
    "        self.W = np.random.randn(output_dim, input_dim) * np.sqrt(2.0 / input_dim)\n",
    "        self.b = np.zeros((output_dim, 1))\n",
    "        # Platzhalter für Zwischenergebnisse\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, input_dim)\n",
    "        self.x = x  # für Backward-Pass speichern\n",
    "        # Berechnung: x * W^T + b^T\n",
    "        out = x.dot(self.W.T) + self.b.T\n",
    "        return out\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        # d_out: (batch_size, output_dim)\n",
    "        batch_size = self.x.shape[0]\n",
    "        self.dW = (d_out.T.dot(self.x)) / batch_size\n",
    "        self.db = (np.sum(d_out, axis=0, keepdims=True).T) / batch_size\n",
    "        # Gradient bezüglich des Inputs\n",
    "        dx = d_out.dot(self.W)\n",
    "        return dx\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        self.W -= learning_rate * self.dW\n",
    "        self.b -= learning_rate * self.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unittest für LinearLayer**\n",
    " \n",
    " Wir definieren einen Test für ein Layer mit 2 Knoten und 2 Datensätzen (jeweils 2 floats).\n",
    " \n",
    " Gegebene Testwerte:\n",
    " \n",
    " - **Input:**  \n",
    "\n",
    "   $ X = \\begin{pmatrix} 1.0 & 2.0 \\\\ 3.0 & 4.0 \\end{pmatrix} $\n",
    " \n",
    " - **Initiale Parameter:**  \n",
    "\n",
    "   $ W = \\begin{pmatrix} 0.1 & 0.2 \\\\ 0.3 & 0.4 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 0.5 \\\\ 0.6 \\end{pmatrix} $\n",
    " \n",
    " - **Forward-Berechnung:**  \n",
    "\n",
    "    $ X \\cdot W^T = \\begin{pmatrix} 0.5 & 1.1 \\\\ 1.1 & 2.5 \\end{pmatrix}  $\n",
    "\n",
    "    $ \\text{out} = X \\cdot W^T + b^T = \\begin{pmatrix} 1.0 & 1.7 \\\\ 1.6 & 3.1 \\end{pmatrix} $\n",
    " \n",
    " - **Backward-Pass:**  \n",
    "\n",
    "   Mit $ d\\_out = \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix} $ und Batchgröße 2,  \n",
    "\n",
    "    $ dW = \\begin{pmatrix} 2 & 3 \\\\ 2 & 3 \\end{pmatrix} $ und $ db = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} $.\n",
    " \n",
    " - **Update (mit Lernrate 0.1):**  \n",
    "\n",
    "   Neue Parameter:  \n",
    "\n",
    "   $ W_{neu} = \\begin{pmatrix} -0.1 & -0.1 \\\\ 0.1 & 0.1 \\end{pmatrix} $, \n",
    "   $ b_{neu} = \\begin{pmatrix} 0.4 \\\\ 0.5 \\end{pmatrix} $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unittest für LinearLayer erfolgreich bestanden.\n"
     ]
    }
   ],
   "source": [
    "def test_linear_layer():\n",
    "    # Feste Testwerte\n",
    "    X_test_input = np.array([[1.0, 2.0],\n",
    "                             [3.0, 4.0]])\n",
    "    \n",
    "    # Erstellen eines Layers und manuelles Setzen der Parameter\n",
    "    layer = LinearLayer(input_dim=2, output_dim=2)\n",
    "    layer.W = np.array([[0.1, 0.2],\n",
    "                        [0.3, 0.4]])\n",
    "    layer.b = np.array([[0.5],\n",
    "                        [0.6]])\n",
    "    \n",
    "    # Forward-Pass\n",
    "    out = layer.forward(X_test_input)\n",
    "    expected_out = np.array([[1.0, 1.7],\n",
    "                             [1.6, 3.1]])\n",
    "    assert np.allclose(out, expected_out, atol=1e-6), f\"Forward-Pass fehlerhaft: {out} != {expected_out}\"\n",
    "    \n",
    "    # Backward-Pass\n",
    "    d_out = np.ones((2,2))\n",
    "    layer.backward(d_out)\n",
    "    # Erwartete Gradienten ohne zusätzliche Division, da backward() bereits durch die Batchgröße teilt.\n",
    "    expected_dW = np.array([[2, 3],\n",
    "                            [2, 3]])\n",
    "    expected_db = np.array([[1],\n",
    "                            [1]])\n",
    "    \n",
    "    assert np.allclose(layer.dW, expected_dW, atol=1e-6), f\"dW fehlerhaft: {layer.dW} != {expected_dW}\"\n",
    "    assert np.allclose(layer.db, expected_db, atol=1e-6), f\"db fehlerhaft: {layer.db} != {expected_db}\"\n",
    "    \n",
    "    # Parameter-Update mit Lernrate 0.1\n",
    "    learning_rate = 0.1\n",
    "    layer.update(learning_rate)\n",
    "    expected_W_new = np.array([[0.1, 0.2],\n",
    "                               [0.3, 0.4]]) - 0.1 * expected_dW\n",
    "    expected_b_new = np.array([[0.5],\n",
    "                               [0.6]]) - 0.1 * expected_db\n",
    "    assert np.allclose(layer.W, expected_W_new, atol=1e-6), f\"W update fehlerhaft: {layer.W} != {expected_W_new}\"\n",
    "    assert np.allclose(layer.b, expected_b_new, atol=1e-6), f\"b update fehlerhaft: {layer.b} != {expected_b_new}\"\n",
    "    \n",
    "    print(\"Unittest für LinearLayer erfolgreich bestanden.\")\n",
    "\n",
    "# Test ausführen\n",
    "test_linear_layer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 3: Einfaches neuronales Netzwerk ( binäre Klassifikation )\n",
    " \n",
    " **Ziel:** Ein Netzwerk, das anhand eines versteckten (Hidden) linearen Layers die Ziffer 7 (1) von allen anderen (0) unterscheidet.\n",
    " \n",
    " Wir implementieren:\n",
    " \n",
    " - **Aktivierungsfunktionen:** ReLU (für den Hidden Layer) und Sigmoid (für den Output)  \n",
    " - **Kostenfunktion:** Binary Cross Entropy (BCE)  \n",
    " - **Trainingsloop:** Vollständiger Batch-Training (ohne Mini-Batches)\n",
    " \n",
    " **Mathematische Definition der Binary Cross Entropy:**  \n",
    "\n",
    " $\n",
    "L = -\\frac{1}{N}\\sum_{i=1}^{N} \\Big[y_i \\log(\\hat{y}_i) + (1-y_i) \\log(1-\\hat{y}_i)\\Big]\n",
    " $\n",
    " \n",
    " _Hinweis:_ Für die Ableitungen nutzen wir die Eigenschaft, dass bei Verwendung von Sigmoid in Kombination mit der BCE der Gradiententerm vereinfacht zu $\\hat{y} - y$ wird.\n",
    "\n",
    "\n",
    " In diesem Modell haben wir eine ungleiche Verteilung der Klassen: Die Ziffer 7 tritt seltener auf als die anderen Ziffern. Um das Training nicht von der Mehrheitsklasse dominieren zu lassen, wenden wir eine Gewichtung an, die positive Beispiele (Ziffer 7) stärker berücksichtigt.\n",
    "\n",
    "**Gewichtung im Backpropagation-Schritt**\n",
    "\n",
    "Im Backward-Pass wird der Fehler gewichtet, indem ein Faktor  $w_{\\text{pos}}$  für positive Beispiele ( y = 1 ) eingeführt wird:\n",
    "\n",
    "\n",
    "$\\delta_2 = ( \\hat{y} - y ) \\cdot w(y)$\n",
    "\n",
    "\n",
    "wobei die Gewichtungsfunktion definiert ist als:\n",
    "\n",
    "\n",
    "$w(y) =\n",
    "\\begin{cases}\n",
    "w_{\\text{pos}}, & \\text{wenn } y = 1 \\\\\n",
    "1, & \\text{wenn } y = 0\n",
    "\\end{cases}$\n",
    "\n",
    "\n",
    "**Gewichtete Verlustfunktion (Binary Cross Entropy)**\n",
    "\n",
    "Die Standard-Binary-Cross-Entropy-Verlustfunktion wird durch die Gewichtung für positive Beispiele angepasst:\n",
    "\n",
    "\n",
    "$L = - \\frac{1}{N} \\sum_{i=1}^{N} \\left[ w_{\\text{pos}} y_i \\log \\hat{y}_i + (1 - y_i) \\log (1 - \\hat{y}_i) \\right]$\n",
    "\n",
    "\n",
    "Diese Methode sorgt dafür, dass Fehler bei der Ziffer 7 (die seltener vorkommt) stärker ins Training einfließen und nicht von den anderen Ziffern überlagert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aktivierungsfunktionen und deren Ableitungen\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN:\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        # Initialisierung der beiden linearen Layer\n",
    "        self.layer1 = LinearLayer(input_dim, hidden_dim)\n",
    "        self.layer2 = LinearLayer(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Vorwärtsdurchlauf:\n",
    "        # Hidden Layer: Linearer Transform gefolgt von ReLU\n",
    "        self.z1 = self.layer1.forward(x)\n",
    "        self.a1 = relu(self.z1)\n",
    "        # Output Layer: Linearer Transform gefolgt von Sigmoid\n",
    "        self.z2 = self.layer2.forward(self.a1)\n",
    "        self.a2 = sigmoid(self.z2)\n",
    "        return self.a2\n",
    "    \n",
    "    def backward(self, x, y, output, weight_pos=3):\n",
    "        \"\"\"\n",
    "        Berechnet den Gradienten unter Berücksichtigung einer Gewichtung für positive Beispiele.\n",
    "        weight_pos: Gewichtungsfaktor für Beispiele, bei denen y == 1 (z.B. Ziffer 7).\n",
    "        \"\"\"\n",
    "        # Erzeuge einen Gewichtungsfaktor: Bei y==1 wird weight_pos verwendet, sonst 1.\n",
    "        weight_factor = np.where(y.reshape(-1, 1) == 1, weight_pos, 1)\n",
    "        # Angepasster Fehler: (output - y) multipliziert mit dem Gewichtungsfaktor\n",
    "        dz2 = (output - y.reshape(-1, 1)) * weight_factor\n",
    "        # Backward-Pass durch den Output-Layer; liefert den Gradienten bezüglich der Hidden-Aktivierung zurück\n",
    "        da1 = self.layer2.backward(dz2)\n",
    "        # Gradient des Hidden Layers: Multiplikation mit der Ableitung der ReLU-Funktion\n",
    "        dz1 = da1 * relu_derivative(self.z1)\n",
    "        # Backward-Pass durch den Hidden Layer\n",
    "        self.layer1.backward(dz1)\n",
    "    \n",
    "    def update(self, learning_rate):\n",
    "        # Aktualisierung der Parameter beider Layer mittels Gradient Descent\n",
    "        self.layer1.update(learning_rate)\n",
    "        self.layer2.update(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kostenfunktion (Binary Cross Entropy) und Accuracy\n",
    "def compute_loss(y_true, y_pred):\n",
    "    epsilon = 1e-8\n",
    "    loss = -np.mean(y_true * np.log(y_pred + epsilon) + (1 - y_true) * np.log(1 - y_pred + epsilon))\n",
    "    return loss\n",
    "\n",
    "# Gewichteter Loss, um die Klassenungleichheit ( 7 vs. Rest der Ziffern ) auszugleichen\n",
    "def compute_loss_weighted(y_true, y_pred, weight_pos=3):\n",
    "    epsilon = 1e-8\n",
    "    loss = -np.mean(weight_pos * y_true * np.log(y_pred + epsilon) + (1 - y_true) * np.log(1 - y_pred + epsilon))\n",
    "    return loss\n",
    "\n",
    "def compute_accuracy(y_true, y_pred):\n",
    "    preds = (y_pred >= 0.5).astype(int).flatten()\n",
    "    return np.mean(preds == y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainingsloop\n",
    "def train_binary_nn_weighted(model, X, y, epochs, learning_rate, weight_pos=3):\n",
    "    loss_history = []\n",
    "    acc_history = []\n",
    "    for epoch in range(epochs):\n",
    "        # Vorwärtsdurchlauf\n",
    "        output = model.forward(X)\n",
    "        # Berechne den gewichteten Loss\n",
    "        loss = compute_loss_weighted(y, output, weight_pos)\n",
    "        # Berechne die Accuracy (diese bleibt weiterhin über den Schwellenwert definiert)\n",
    "        acc = compute_accuracy(y, output)\n",
    "        loss_history.append(loss)\n",
    "        acc_history.append(acc)\n",
    "        \n",
    "        # Rückwärtsdurchlauf mit Gewichtung im Gradienten\n",
    "        model.backward(X, y, output, weight_pos)\n",
    "        model.update(learning_rate)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} – Loss: {loss:.4f}, Accuracy: {acc*100:.2f}%\")\n",
    "    return loss_history, acc_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training des binären Netzwerks (Aufgabe 3 & 4)\n",
    " \n",
    " Wir wählen:\n",
    " - Input-Dimension: 784  \n",
    " - Hidden-Dimension: 32 (wird später variiert)  \n",
    " - Output-Dimension: 1  \n",
    " - Lernrate: 0.1 (wird später variiert)\n",
    " \n",
    " Die Daten sind bereits vorbereitet:\n",
    " \n",
    " `X_train` (Trainingsbilder, flach), `y_train_bin` (Labels: 1, falls Ziffer 7, sonst 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 – Loss: 0.7312, Accuracy: 88.86%\n",
      "Epoch 2/10 – Loss: 0.7309, Accuracy: 88.88%\n",
      "Epoch 3/10 – Loss: 0.7307, Accuracy: 88.90%\n",
      "Epoch 4/10 – Loss: 0.7306, Accuracy: 88.92%\n",
      "Epoch 5/10 – Loss: 0.7307, Accuracy: 88.92%\n",
      "Epoch 6/10 – Loss: 0.7308, Accuracy: 88.92%\n",
      "Epoch 7/10 – Loss: 0.7310, Accuracy: 88.94%\n",
      "Epoch 8/10 – Loss: 0.7312, Accuracy: 88.94%\n",
      "Epoch 9/10 – Loss: 0.7316, Accuracy: 88.96%\n",
      "Epoch 10/10 – Loss: 0.7320, Accuracy: 88.98%\n",
      "\n",
      "Test Loss: 0.7051, Test Accuracy: 89.66%\n"
     ]
    }
   ],
   "source": [
    "# Für die Experimente verwenden wir nur einen Teil der Trainingsdaten (z.B. 5000 Beispiele)\n",
    "sample_size = 5000\n",
    "X_train_sample = X_train[:sample_size]\n",
    "y_train_bin_sample = y_train_bin[:sample_size]\n",
    "\n",
    "# Hyperparameter für das initiale Training\n",
    "input_dim = 784\n",
    "hidden_dim = 32\n",
    "output_dim = 1\n",
    "epochs = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Initialisiere das Netzwerk\n",
    "binary_nn = SimpleNN(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Training\n",
    "loss_hist, acc_hist = train_binary_nn_weighted(binary_nn, X_train_sample, y_train_bin_sample, epochs, learning_rate)\n",
    "\n",
    "# Evaluierung auf Testdaten\n",
    "test_output = binary_nn.forward(X_test)\n",
    "test_loss = compute_loss_weighted(y_test_bin, test_output)\n",
    "test_acc = compute_accuracy(y_test_bin, test_output)\n",
    "print(f\"\\nTest Loss: {test_loss:.4f}, Test Accuracy: {test_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 4: Experimente mit unterschiedlichen Hyperparametern\n",
    " \n",
    " Wir variieren:\n",
    " - **Lernrate:** 0.01, 0.1, 1.0  \n",
    " - **Hidden-Layer-Größe:** 4, 8, 16\n",
    " \n",
    " Für jede Kombination trainieren wir 10 Epochen und protokollieren den Test-Accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training mit Lernrate=0.01 und Hidden-Dimension=4\n",
      "Epoch 1/10 – Loss: 0.7719, Accuracy: 88.38%\n",
      "Epoch 2/10 – Loss: 0.7656, Accuracy: 88.76%\n",
      "Epoch 3/10 – Loss: 0.7597, Accuracy: 88.86%\n",
      "Epoch 4/10 – Loss: 0.7544, Accuracy: 88.92%\n",
      "Epoch 5/10 – Loss: 0.7496, Accuracy: 88.96%\n",
      "Epoch 6/10 – Loss: 0.7455, Accuracy: 89.08%\n",
      "Epoch 7/10 – Loss: 0.7420, Accuracy: 89.10%\n",
      "Epoch 8/10 – Loss: 0.7391, Accuracy: 89.06%\n",
      "Epoch 9/10 – Loss: 0.7367, Accuracy: 89.04%\n",
      "Epoch 10/10 – Loss: 0.7348, Accuracy: 89.02%\n",
      "Test Accuracy: 89.74%\n",
      "\n",
      "Training mit Lernrate=0.01 und Hidden-Dimension=8\n",
      "Epoch 1/10 – Loss: 0.9765, Accuracy: 15.44%\n",
      "Epoch 2/10 – Loss: 0.9557, Accuracy: 18.04%\n",
      "Epoch 3/10 – Loss: 0.9377, Accuracy: 21.36%\n",
      "Epoch 4/10 – Loss: 0.9220, Accuracy: 25.52%\n",
      "Epoch 5/10 – Loss: 0.9080, Accuracy: 30.70%\n",
      "Epoch 6/10 – Loss: 0.8952, Accuracy: 36.52%\n",
      "Epoch 7/10 – Loss: 0.8833, Accuracy: 42.66%\n",
      "Epoch 8/10 – Loss: 0.8721, Accuracy: 49.44%\n",
      "Epoch 9/10 – Loss: 0.8612, Accuracy: 56.04%\n",
      "Epoch 10/10 – Loss: 0.8507, Accuracy: 62.62%\n",
      "Test Accuracy: 67.10%\n",
      "\n",
      "Training mit Lernrate=0.01 und Hidden-Dimension=16\n",
      "Epoch 1/10 – Loss: 0.7592, Accuracy: 86.62%\n",
      "Epoch 2/10 – Loss: 0.7561, Accuracy: 87.20%\n",
      "Epoch 3/10 – Loss: 0.7532, Accuracy: 87.46%\n",
      "Epoch 4/10 – Loss: 0.7506, Accuracy: 87.78%\n",
      "Epoch 5/10 – Loss: 0.7482, Accuracy: 88.18%\n",
      "Epoch 6/10 – Loss: 0.7460, Accuracy: 88.40%\n",
      "Epoch 7/10 – Loss: 0.7439, Accuracy: 88.58%\n",
      "Epoch 8/10 – Loss: 0.7422, Accuracy: 88.68%\n",
      "Epoch 9/10 – Loss: 0.7406, Accuracy: 88.82%\n",
      "Epoch 10/10 – Loss: 0.7392, Accuracy: 88.84%\n",
      "Test Accuracy: 89.60%\n",
      "\n",
      "Training mit Lernrate=0.1 und Hidden-Dimension=4\n",
      "Epoch 1/10 – Loss: 1.6005, Accuracy: 11.00%\n",
      "Epoch 2/10 – Loss: 0.8305, Accuracy: 86.38%\n",
      "Epoch 3/10 – Loss: 0.8234, Accuracy: 87.62%\n",
      "Epoch 4/10 – Loss: 0.8168, Accuracy: 88.58%\n",
      "Epoch 5/10 – Loss: 0.8107, Accuracy: 88.84%\n",
      "Epoch 6/10 – Loss: 0.8050, Accuracy: 89.00%\n",
      "Epoch 7/10 – Loss: 0.7996, Accuracy: 89.04%\n",
      "Epoch 8/10 – Loss: 0.7946, Accuracy: 89.00%\n",
      "Epoch 9/10 – Loss: 0.7899, Accuracy: 89.00%\n",
      "Epoch 10/10 – Loss: 0.7855, Accuracy: 89.02%\n",
      "Test Accuracy: 89.75%\n",
      "\n",
      "Training mit Lernrate=0.1 und Hidden-Dimension=8\n",
      "Epoch 1/10 – Loss: 0.8428, Accuracy: 45.66%\n",
      "Epoch 2/10 – Loss: 0.7685, Accuracy: 84.88%\n",
      "Epoch 3/10 – Loss: 0.7352, Accuracy: 89.18%\n",
      "Epoch 4/10 – Loss: 0.7414, Accuracy: 89.38%\n",
      "Epoch 5/10 – Loss: 0.7528, Accuracy: 89.98%\n",
      "Epoch 6/10 – Loss: 0.7673, Accuracy: 90.98%\n",
      "Epoch 7/10 – Loss: 0.7846, Accuracy: 92.04%\n",
      "Epoch 8/10 – Loss: 0.8027, Accuracy: 93.34%\n",
      "Epoch 9/10 – Loss: 0.8209, Accuracy: 94.44%\n",
      "Epoch 10/10 – Loss: 0.8403, Accuracy: 94.88%\n",
      "Test Accuracy: 95.49%\n",
      "\n",
      "Training mit Lernrate=0.1 und Hidden-Dimension=16\n",
      "Epoch 1/10 – Loss: 0.7589, Accuracy: 84.06%\n",
      "Epoch 2/10 – Loss: 0.7324, Accuracy: 89.00%\n",
      "Epoch 3/10 – Loss: 0.7408, Accuracy: 89.14%\n",
      "Epoch 4/10 – Loss: 0.7544, Accuracy: 90.16%\n",
      "Epoch 5/10 – Loss: 0.7723, Accuracy: 92.42%\n",
      "Epoch 6/10 – Loss: 0.7939, Accuracy: 94.28%\n",
      "Epoch 7/10 – Loss: 0.8178, Accuracy: 95.26%\n",
      "Epoch 8/10 – Loss: 0.8435, Accuracy: 95.78%\n",
      "Epoch 9/10 – Loss: 0.8697, Accuracy: 96.16%\n",
      "Epoch 10/10 – Loss: 0.8968, Accuracy: 96.40%\n",
      "Test Accuracy: 96.76%\n",
      "\n",
      "Training mit Lernrate=1.0 und Hidden-Dimension=4\n",
      "Epoch 1/10 – Loss: 0.8910, Accuracy: 60.18%\n",
      "Epoch 2/10 – Loss: 1.3453, Accuracy: 89.00%\n",
      "Epoch 3/10 – Loss: 0.8878, Accuracy: 46.28%\n",
      "Epoch 4/10 – Loss: 0.7669, Accuracy: 89.00%\n",
      "Epoch 5/10 – Loss: 0.7406, Accuracy: 89.00%\n",
      "Epoch 6/10 – Loss: 0.7273, Accuracy: 89.00%\n",
      "Epoch 7/10 – Loss: 0.7204, Accuracy: 89.00%\n",
      "Epoch 8/10 – Loss: 0.7167, Accuracy: 89.00%\n",
      "Epoch 9/10 – Loss: 0.7147, Accuracy: 89.00%\n",
      "Epoch 10/10 – Loss: 0.7136, Accuracy: 89.00%\n",
      "Test Accuracy: 89.72%\n",
      "\n",
      "Training mit Lernrate=1.0 und Hidden-Dimension=8\n",
      "Epoch 1/10 – Loss: 0.7945, Accuracy: 80.02%\n",
      "Epoch 2/10 – Loss: 1.1157, Accuracy: 89.00%\n",
      "Epoch 3/10 – Loss: 0.8576, Accuracy: 16.88%\n",
      "Epoch 4/10 – Loss: 0.7780, Accuracy: 89.00%\n",
      "Epoch 5/10 – Loss: 0.7398, Accuracy: 89.00%\n",
      "Epoch 6/10 – Loss: 0.7333, Accuracy: 89.00%\n",
      "Epoch 7/10 – Loss: 0.9440, Accuracy: 89.00%\n",
      "Epoch 8/10 – Loss: 1.0428, Accuracy: 94.10%\n",
      "Epoch 9/10 – Loss: 1.1960, Accuracy: 96.44%\n",
      "Epoch 10/10 – Loss: 1.4670, Accuracy: 97.28%\n",
      "Test Accuracy: 93.30%\n",
      "\n",
      "Training mit Lernrate=1.0 und Hidden-Dimension=16\n",
      "Epoch 1/10 – Loss: 0.8407, Accuracy: 54.96%\n",
      "Epoch 2/10 – Loss: 2.0725, Accuracy: 89.00%\n",
      "Epoch 3/10 – Loss: 2.1253, Accuracy: 11.00%\n",
      "Epoch 4/10 – Loss: 0.7234, Accuracy: 89.00%\n",
      "Epoch 5/10 – Loss: 0.7233, Accuracy: 89.52%\n",
      "Epoch 6/10 – Loss: 0.7821, Accuracy: 91.64%\n",
      "Epoch 7/10 – Loss: 0.7164, Accuracy: 89.28%\n",
      "Epoch 8/10 – Loss: 0.8307, Accuracy: 93.88%\n",
      "Epoch 9/10 – Loss: 0.8242, Accuracy: 94.90%\n",
      "Epoch 10/10 – Loss: 1.2984, Accuracy: 76.20%\n",
      "Test Accuracy: 89.72%\n",
      "\n",
      "Ergebnisse der Hyperparameter-Experimente:\n",
      "Lernrate 0.01, Hidden 4 -> Accuracy: 89.74%\n",
      "Lernrate 0.01, Hidden 8 -> Accuracy: 67.10%\n",
      "Lernrate 0.01, Hidden 16 -> Accuracy: 89.60%\n",
      "Lernrate 0.1, Hidden 4 -> Accuracy: 89.75%\n",
      "Lernrate 0.1, Hidden 8 -> Accuracy: 95.49%\n",
      "Lernrate 0.1, Hidden 16 -> Accuracy: 96.76%\n",
      "Lernrate 1.0, Hidden 4 -> Accuracy: 89.72%\n",
      "Lernrate 1.0, Hidden 8 -> Accuracy: 93.30%\n",
      "Lernrate 1.0, Hidden 16 -> Accuracy: 89.72%\n"
     ]
    }
   ],
   "source": [
    "# Für die Experimente verwenden wir nur einen Teil der Trainingsdaten (z.B. 5000 Beispiele)\n",
    "sample_size = 5000\n",
    "X_train_sample = X_train[:sample_size]\n",
    "y_train_bin_sample = y_train_bin[:sample_size]\n",
    "\n",
    "learning_rates = [0.01, 0.1, 1.0]\n",
    "hidden_sizes = [4, 8, 16]\n",
    "results = {}\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for h_dim in hidden_sizes:\n",
    "        print(f\"\\nTraining mit Lernrate={lr} und Hidden-Dimension={h_dim}\")\n",
    "        model = SimpleNN(input_dim, h_dim, output_dim)\n",
    "        train_binary_nn_weighted(model, X_train_sample, y_train_bin_sample, epochs=10, learning_rate=lr)\n",
    "        test_out = model.forward(X_test)\n",
    "        acc = compute_accuracy(y_test_bin, test_out)\n",
    "        results[(lr, h_dim)] = acc\n",
    "        print(f\"Test Accuracy: {acc*100:.2f}%\")\n",
    "\n",
    "print(\"\\nErgebnisse der Hyperparameter-Experimente:\")\n",
    "for key, acc in results.items():\n",
    "    print(f\"Lernrate {key[0]}, Hidden {key[1]} -> Accuracy: {acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 5: Erweiterung auf ein Netzwerk für Mehrklassenklassifikation\n",
    " \n",
    " **Ziel:** Klassifizierung aller 10 Ziffern (0–9)  \n",
    "\n",
    " **Architektur:**  \n",
    "\n",
    " - 3 Hidden Layers (alle mit gleicher Größe)  \n",
    " - Output Layer mit 10 Knoten\n",
    " \n",
    " **Kostenfunktion:** Softmax-Cross-Entropy  \n",
    " \n",
    " **Mathematische Definition (Softmax und Cross-Entropy):**\n",
    " \n",
    " - **Softmax:**  \n",
    "\n",
    "$\n",
    " \\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}}\n",
    "$\n",
    " \n",
    " - **Cross-Entropy:**  \n",
    "\n",
    "$\n",
    " L = -\\frac{1}{N}\\sum_{i=1}^{N} \\sum_{c=1}^{C} y_{i,c} \\log(\\hat{y}_{i,c})\n",
    "$\n",
    " \n",
    " **Warum Mini-Batches?**  \n",
    "\n",
    " - Reduzierung des Speicherbedarfs  \n",
    " - Bessere Generalisierung durch stochastische Gradientenabstiege  \n",
    " - Effiziente Nutzung von Rechenressourcen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax-Funktion\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "# Cross-Entropy Verlust\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    epsilon = 1e-8\n",
    "    loss = -np.mean(np.sum(y_true * np.log(y_pred + epsilon), axis=1))\n",
    "    return loss\n",
    "\n",
    "# Accuracy für Mehrklassenklassifikation\n",
    "def compute_accuracy_multiclass(y_true, y_pred):\n",
    "    preds = np.argmax(y_pred, axis=1)\n",
    "    true_labels = np.argmax(y_true, axis=1)\n",
    "    return np.mean(preds == true_labels)\n",
    "\n",
    "# One-hot Encoding\n",
    "def one_hot_encode(y, num_classes=10):\n",
    "    return np.eye(num_classes)[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mehrschichtiges Netzwerk mit 3 Hidden Layers und 10 Outputs\n",
    "class MultiLayerNN:\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim=10):\n",
    "        self.layer1 = LinearLayer(input_dim, hidden_dim)\n",
    "        self.layer2 = LinearLayer(hidden_dim, hidden_dim)\n",
    "        self.layer3 = LinearLayer(hidden_dim, hidden_dim)\n",
    "        self.layer4 = LinearLayer(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.z1 = self.layer1.forward(x)\n",
    "        self.a1 = relu(self.z1)\n",
    "        self.z2 = self.layer2.forward(self.a1)\n",
    "        self.a2 = relu(self.z2)\n",
    "        self.z3 = self.layer3.forward(self.a2)\n",
    "        self.a3 = relu(self.z3)\n",
    "        self.z4 = self.layer4.forward(self.a3)\n",
    "        self.a4 = softmax(self.z4)\n",
    "        return self.a4\n",
    "    \n",
    "    def backward(self, x, y, output):\n",
    "        m = y.shape[0]\n",
    "        # Für Softmax mit Cross-Entropy vereinfacht sich der Gradient:\n",
    "        dz4 = (output - y) / m\n",
    "        da3 = self.layer4.backward(dz4)\n",
    "        dz3 = da3 * relu_derivative(self.z3)\n",
    "        da2 = self.layer3.backward(dz3)\n",
    "        dz2 = da2 * relu_derivative(self.z2)\n",
    "        da1 = self.layer2.backward(dz2)\n",
    "        dz1 = da1 * relu_derivative(self.z1)\n",
    "        self.layer1.backward(dz1)\n",
    "        \n",
    "    def update(self, learning_rate):\n",
    "        self.layer1.update(learning_rate)\n",
    "        self.layer2.update(learning_rate)\n",
    "        self.layer3.update(learning_rate)\n",
    "        self.layer4.update(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainingsloop mit Mini-Batch Training\n",
    "def train_multiclass_nn(model, X, y, epochs, learning_rate, batch_size=64):\n",
    "    loss_history = []\n",
    "    acc_history = []\n",
    "    num_samples = X.shape[0]\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle der Daten\n",
    "        indices = np.arange(num_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        X_shuffled = X[indices]\n",
    "        y_shuffled = y[indices]\n",
    "        \n",
    "        epoch_loss = 0\n",
    "        num_batches = int(np.ceil(num_samples / batch_size))\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "            start = i * batch_size\n",
    "            end = min(start + batch_size, num_samples)\n",
    "            X_batch = X_shuffled[start:end]\n",
    "            y_batch = y_shuffled[start:end]\n",
    "            \n",
    "            # Forward-Pass\n",
    "            output = model.forward(X_batch)\n",
    "            loss = cross_entropy_loss(y_batch, output)\n",
    "            epoch_loss += loss\n",
    "            \n",
    "            # Backward-Pass\n",
    "            model.backward(X_batch, y_batch, output)\n",
    "            \n",
    "            # Update der Parameter\n",
    "            model.update(learning_rate)\n",
    "        \n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        # Evaluierung auf dem gesamten Trainingssatz\n",
    "        output_train = model.forward(X)\n",
    "        acc = compute_accuracy_multiclass(y, output_train)\n",
    "        loss_history.append(avg_loss)\n",
    "        acc_history.append(acc)\n",
    "  \n",
    "        # Nur jede 10. Epoche ausgeben\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} – Loss: {avg_loss:.4f}, Accuracy: {acc*100:.2f}%\")\n",
    "            \n",
    "    return loss_history, acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vorbereitung der Daten für Mehrklassenklassifikation\n",
    "# Wir verwenden nun die Original-Labels (0-9) und one-hot encodieren diese.\n",
    "y_train_oh = one_hot_encode(y_train, num_classes=10)\n",
    "y_test_oh  = one_hot_encode(y_test, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training (Mehrklassen) mit Lernrate=0.001 und Hidden-Dimension=16\n",
      "Epoch 10/100 – Loss: 2.3237, Accuracy: 12.98%\n",
      "Epoch 20/100 – Loss: 2.3148, Accuracy: 13.02%\n",
      "Epoch 30/100 – Loss: 2.3074, Accuracy: 13.04%\n",
      "Epoch 40/100 – Loss: 2.3013, Accuracy: 13.16%\n",
      "Epoch 50/100 – Loss: 2.2949, Accuracy: 13.22%\n",
      "Epoch 60/100 – Loss: 2.2861, Accuracy: 13.48%\n",
      "Epoch 70/100 – Loss: 2.2851, Accuracy: 13.66%\n",
      "Epoch 80/100 – Loss: 2.2809, Accuracy: 13.78%\n",
      "Epoch 90/100 – Loss: 2.2769, Accuracy: 13.94%\n",
      "Epoch 100/100 – Loss: 2.2733, Accuracy: 14.18%\n",
      "Test Accuracy (Mehrklassen): 13.88%\n",
      "\n",
      "Training (Mehrklassen) mit Lernrate=0.001 und Hidden-Dimension=32\n",
      "Epoch 10/100 – Loss: 2.2796, Accuracy: 12.32%\n",
      "Epoch 20/100 – Loss: 2.2729, Accuracy: 12.92%\n",
      "Epoch 30/100 – Loss: 2.2682, Accuracy: 13.68%\n",
      "Epoch 40/100 – Loss: 2.2610, Accuracy: 14.40%\n",
      "Epoch 50/100 – Loss: 2.2564, Accuracy: 15.18%\n",
      "Epoch 60/100 – Loss: 2.2506, Accuracy: 15.92%\n",
      "Epoch 70/100 – Loss: 2.2447, Accuracy: 16.54%\n",
      "Epoch 80/100 – Loss: 2.2375, Accuracy: 17.00%\n",
      "Epoch 90/100 – Loss: 2.2325, Accuracy: 17.68%\n",
      "Epoch 100/100 – Loss: 2.2275, Accuracy: 18.28%\n",
      "Test Accuracy (Mehrklassen): 18.29%\n",
      "\n",
      "Training (Mehrklassen) mit Lernrate=0.001 und Hidden-Dimension=64\n",
      "Epoch 10/100 – Loss: 2.4371, Accuracy: 8.64%\n",
      "Epoch 20/100 – Loss: 2.4163, Accuracy: 8.56%\n",
      "Epoch 30/100 – Loss: 2.3947, Accuracy: 8.54%\n",
      "Epoch 40/100 – Loss: 2.3795, Accuracy: 8.62%\n",
      "Epoch 50/100 – Loss: 2.3659, Accuracy: 8.60%\n",
      "Epoch 60/100 – Loss: 2.3502, Accuracy: 8.76%\n",
      "Epoch 70/100 – Loss: 2.3395, Accuracy: 8.90%\n",
      "Epoch 80/100 – Loss: 2.3280, Accuracy: 9.20%\n",
      "Epoch 90/100 – Loss: 2.3178, Accuracy: 9.48%\n",
      "Epoch 100/100 – Loss: 2.3068, Accuracy: 9.84%\n",
      "Test Accuracy (Mehrklassen): 9.61%\n",
      "\n",
      "Training (Mehrklassen) mit Lernrate=0.01 und Hidden-Dimension=16\n",
      "Epoch 10/100 – Loss: 2.2886, Accuracy: 13.00%\n",
      "Epoch 20/100 – Loss: 2.2342, Accuracy: 15.00%\n",
      "Epoch 30/100 – Loss: 2.1888, Accuracy: 17.32%\n",
      "Epoch 40/100 – Loss: 2.1441, Accuracy: 19.70%\n",
      "Epoch 50/100 – Loss: 2.0945, Accuracy: 23.72%\n",
      "Epoch 60/100 – Loss: 2.0405, Accuracy: 28.72%\n",
      "Epoch 70/100 – Loss: 1.9780, Accuracy: 34.22%\n",
      "Epoch 80/100 – Loss: 1.9105, Accuracy: 38.90%\n",
      "Epoch 90/100 – Loss: 1.8494, Accuracy: 41.34%\n",
      "Epoch 100/100 – Loss: 1.7835, Accuracy: 44.38%\n",
      "Test Accuracy (Mehrklassen): 42.09%\n",
      "\n",
      "Training (Mehrklassen) mit Lernrate=0.01 und Hidden-Dimension=32\n",
      "Epoch 10/100 – Loss: 2.2688, Accuracy: 16.24%\n",
      "Epoch 20/100 – Loss: 2.1863, Accuracy: 23.52%\n",
      "Epoch 30/100 – Loss: 2.1096, Accuracy: 30.70%\n",
      "Epoch 40/100 – Loss: 2.0216, Accuracy: 35.10%\n",
      "Epoch 50/100 – Loss: 1.9295, Accuracy: 39.16%\n",
      "Epoch 60/100 – Loss: 1.8384, Accuracy: 42.78%\n",
      "Epoch 70/100 – Loss: 1.7437, Accuracy: 46.52%\n",
      "Epoch 80/100 – Loss: 1.6565, Accuracy: 49.58%\n",
      "Epoch 90/100 – Loss: 1.5663, Accuracy: 53.26%\n",
      "Epoch 100/100 – Loss: 1.4801, Accuracy: 56.78%\n",
      "Test Accuracy (Mehrklassen): 55.68%\n",
      "\n",
      "Training (Mehrklassen) mit Lernrate=0.01 und Hidden-Dimension=64\n",
      "Epoch 10/100 – Loss: 2.2839, Accuracy: 15.28%\n",
      "Epoch 20/100 – Loss: 2.2067, Accuracy: 22.82%\n",
      "Epoch 30/100 – Loss: 2.1322, Accuracy: 31.24%\n",
      "Epoch 40/100 – Loss: 2.0557, Accuracy: 39.02%\n",
      "Epoch 50/100 – Loss: 1.9729, Accuracy: 44.94%\n",
      "Epoch 60/100 – Loss: 1.8866, Accuracy: 50.22%\n",
      "Epoch 70/100 – Loss: 1.7895, Accuracy: 54.64%\n",
      "Epoch 80/100 – Loss: 1.6847, Accuracy: 58.16%\n",
      "Epoch 90/100 – Loss: 1.5800, Accuracy: 62.98%\n",
      "Epoch 100/100 – Loss: 1.4744, Accuracy: 66.88%\n",
      "Test Accuracy (Mehrklassen): 65.03%\n",
      "\n",
      "Training (Mehrklassen) mit Lernrate=0.1 und Hidden-Dimension=16\n",
      "Epoch 10/100 – Loss: 2.0871, Accuracy: 23.00%\n",
      "Epoch 20/100 – Loss: 1.7921, Accuracy: 43.48%\n",
      "Epoch 30/100 – Loss: 1.4647, Accuracy: 56.76%\n",
      "Epoch 40/100 – Loss: 1.1533, Accuracy: 65.58%\n",
      "Epoch 50/100 – Loss: 0.8965, Accuracy: 70.24%\n",
      "Epoch 60/100 – Loss: 0.7259, Accuracy: 80.90%\n",
      "Epoch 70/100 – Loss: 0.6137, Accuracy: 81.94%\n",
      "Epoch 80/100 – Loss: 0.5466, Accuracy: 82.40%\n",
      "Epoch 90/100 – Loss: 0.4912, Accuracy: 87.14%\n",
      "Epoch 100/100 – Loss: 0.4504, Accuracy: 86.16%\n",
      "Test Accuracy (Mehrklassen): 82.93%\n",
      "\n",
      "Training (Mehrklassen) mit Lernrate=0.1 und Hidden-Dimension=32\n",
      "Epoch 10/100 – Loss: 1.8250, Accuracy: 47.12%\n",
      "Epoch 20/100 – Loss: 1.0484, Accuracy: 74.30%\n",
      "Epoch 30/100 – Loss: 0.6485, Accuracy: 83.30%\n",
      "Epoch 40/100 – Loss: 0.4937, Accuracy: 85.26%\n",
      "Epoch 50/100 – Loss: 0.4054, Accuracy: 89.02%\n",
      "Epoch 60/100 – Loss: 0.3604, Accuracy: 90.20%\n",
      "Epoch 70/100 – Loss: 0.3235, Accuracy: 90.46%\n",
      "Epoch 80/100 – Loss: 0.3104, Accuracy: 88.66%\n",
      "Epoch 90/100 – Loss: 0.2705, Accuracy: 90.72%\n",
      "Epoch 100/100 – Loss: 0.2587, Accuracy: 91.90%\n",
      "Test Accuracy (Mehrklassen): 88.23%\n",
      "\n",
      "Training (Mehrklassen) mit Lernrate=0.1 und Hidden-Dimension=64\n",
      "Epoch 10/100 – Loss: 1.4363, Accuracy: 71.42%\n",
      "Epoch 20/100 – Loss: 0.7293, Accuracy: 81.06%\n",
      "Epoch 30/100 – Loss: 0.5345, Accuracy: 85.64%\n",
      "Epoch 40/100 – Loss: 0.4386, Accuracy: 87.68%\n",
      "Epoch 50/100 – Loss: 0.3770, Accuracy: 89.76%\n",
      "Epoch 60/100 – Loss: 0.3351, Accuracy: 90.34%\n",
      "Epoch 70/100 – Loss: 0.3364, Accuracy: 90.40%\n",
      "Epoch 80/100 – Loss: 0.2856, Accuracy: 91.86%\n",
      "Epoch 90/100 – Loss: 0.2621, Accuracy: 92.64%\n",
      "Epoch 100/100 – Loss: 0.2429, Accuracy: 93.26%\n",
      "Test Accuracy (Mehrklassen): 89.45%\n",
      "\n",
      "Ergebnisse der Mehrklassen-Experimente:\n",
      "Lernrate 0.001, Hidden 16 -> Accuracy: 13.88%\n",
      "Lernrate 0.001, Hidden 32 -> Accuracy: 18.29%\n",
      "Lernrate 0.001, Hidden 64 -> Accuracy: 9.61%\n",
      "Lernrate 0.01, Hidden 16 -> Accuracy: 42.09%\n",
      "Lernrate 0.01, Hidden 32 -> Accuracy: 55.68%\n",
      "Lernrate 0.01, Hidden 64 -> Accuracy: 65.03%\n",
      "Lernrate 0.1, Hidden 16 -> Accuracy: 82.93%\n",
      "Lernrate 0.1, Hidden 32 -> Accuracy: 88.23%\n",
      "Lernrate 0.1, Hidden 64 -> Accuracy: 89.45%\n"
     ]
    }
   ],
   "source": [
    "# Für Experimente begrenzen wir auch hier aus Zeitgründen die Trainingsdaten (z.B. 5000 Beispiele)\n",
    "sample_size = 5000\n",
    "X_train_mc = X_train[:sample_size]\n",
    "y_train_mc = y_train_oh[:sample_size]\n",
    "\n",
    "learning_rates_mc = [0.001, 0.01, 0.1]\n",
    "hidden_sizes_mc = [16, 32, 64]\n",
    "results_mc = {}\n",
    "\n",
    "for lr in learning_rates_mc:\n",
    "    for h_dim in hidden_sizes_mc:\n",
    "        print(f\"\\nTraining (Mehrklassen) mit Lernrate={lr} und Hidden-Dimension={h_dim}\")\n",
    "        model_mc = MultiLayerNN(input_dim, h_dim, output_dim=10)\n",
    "        train_multiclass_nn(model_mc, X_train_mc, y_train_mc, epochs=100, learning_rate=lr, batch_size=64)\n",
    "        test_out_mc = model_mc.forward(X_test)\n",
    "        acc_mc = compute_accuracy_multiclass(y_test_oh, test_out_mc)\n",
    "        results_mc[(lr, h_dim)] = acc_mc\n",
    "        print(f\"Test Accuracy (Mehrklassen): {acc_mc*100:.2f}%\")\n",
    "\n",
    "print(\"\\nErgebnisse der Mehrklassen-Experimente:\")\n",
    "for key, acc in results_mc.items():\n",
    "    print(f\"Lernrate {key[0]}, Hidden {key[1]} -> Accuracy: {acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainiere das beste Modell (Lernrate=0.1, Hidden=64) mit dem vollen Datensatz...\n",
      "Epoch 10/100 – Loss: 0.3011, Accuracy: 91.47%\n",
      "Epoch 20/100 – Loss: 0.2244, Accuracy: 93.60%\n",
      "Epoch 30/100 – Loss: 0.1854, Accuracy: 94.66%\n",
      "Epoch 40/100 – Loss: 0.1594, Accuracy: 95.46%\n",
      "Epoch 50/100 – Loss: 0.1403, Accuracy: 95.99%\n",
      "Epoch 60/100 – Loss: 0.1254, Accuracy: 96.44%\n",
      "Epoch 70/100 – Loss: 0.1131, Accuracy: 96.78%\n",
      "Epoch 80/100 – Loss: 0.1027, Accuracy: 97.16%\n",
      "Epoch 90/100 – Loss: 0.0936, Accuracy: 97.46%\n",
      "Epoch 100/100 – Loss: 0.0859, Accuracy: 97.61%\n",
      "\n",
      "Test Loss: 0.1078, Test Accuracy: 96.89%\n"
     ]
    }
   ],
   "source": [
    "# Bestes Modell: Lernrate 0.1, Hidden 64, volle Trainingsdaten\n",
    "\n",
    "# Parameter festlegen\n",
    "best_lr = 0.1\n",
    "best_hidden = 64\n",
    "epochs = 100      # je nach Rechenleistung ggf. anpassen\n",
    "batch_size = 64   # Mini-Batch Größe\n",
    "\n",
    "print(\"Trainiere das beste Modell (Lernrate=0.1, Hidden=64) mit dem vollen Datensatz...\")\n",
    "\n",
    "# Modell initialisieren\n",
    "model_best = MultiLayerNN(input_dim, best_hidden, output_dim=10)\n",
    "\n",
    "# Verwenden Sie den vollen Trainingsdatensatz:\n",
    "X_train_full = X_train        # X_train enthält alle Trainingsbeispiele\n",
    "y_train_full = y_train_oh     # y_train_oh enthält die one-hot encodierten Labels\n",
    "\n",
    "# Training durchführen\n",
    "train_loss_hist, train_acc_hist = train_multiclass_nn(model_best, X_train_full, y_train_full, \n",
    "                                                      epochs=epochs, learning_rate=best_lr, \n",
    "                                                      batch_size=batch_size)\n",
    "\n",
    "# Evaluation auf den Testdaten\n",
    "test_out_best = model_best.forward(X_test)\n",
    "test_loss_best = cross_entropy_loss(y_test_oh, test_out_best)\n",
    "test_acc_best = compute_accuracy_multiclass(y_test_oh, test_out_best)\n",
    "\n",
    "print(f\"\\nTest Loss: {test_loss_best:.4f}, Test Accuracy: {test_acc_best*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zusatz: Precision, Recall und Konfusionmatrix\n",
    "\n",
    "•\tZunächst werden mit np.argmax sowohl die echten Labels als auch die Vorhersagen in Index-Form (0 bis 9) umgewandelt.\n",
    "\n",
    "•\tAnschließend wird die Confusion Matrix erstellt, in der der Eintrag conf_matrix[i, j] angibt, wie oft ein Beispiel der wahren Klasse i als Klasse j klassifiziert wurde.\n",
    "\n",
    "•\tFür jede Klasse wird dann Precision berechnet als  $\\frac{TP}{TP + FP}$  und Recall als  $\\frac{TP}{TP + FN}$ .\n",
    "    \n",
    "•\tAbschließend werden auch die Macro-Werte (Durchschnitt über alle Klassen) ausgegeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[ 953    0    1    4    0    7    7    2    5    1]\n",
      " [   0 1098    6    3    0    2    5    0   21    0]\n",
      " [  12    5  905   18   16    2   19   12   36    7]\n",
      " [   7    2   22  848    2   84    1   15   17   12]\n",
      " [   3    3    7    0  854    1   17    5    8   84]\n",
      " [  12    2    2   19   11  780   15   10   29   12]\n",
      " [  19    3   12    3   36   27  853    3    2    0]\n",
      " [   2   15   30    3    9    1    2  913    3   50]\n",
      " [   8   10   11   27    9   49   12    9  820   19]\n",
      " [  10    5    2   11   20   15    2   14    9  921]]\n",
      "\n",
      "Precision pro Klasse:\n",
      "Klasse 0: 0.9288\n",
      "Klasse 1: 0.9606\n",
      "Klasse 2: 0.9068\n",
      "Klasse 3: 0.9060\n",
      "Klasse 4: 0.8924\n",
      "Klasse 5: 0.8058\n",
      "Klasse 6: 0.9143\n",
      "Klasse 7: 0.9288\n",
      "Klasse 8: 0.8632\n",
      "Klasse 9: 0.8327\n",
      "\n",
      "Recall pro Klasse:\n",
      "Klasse 0: 0.9724\n",
      "Klasse 1: 0.9674\n",
      "Klasse 2: 0.8769\n",
      "Klasse 3: 0.8396\n",
      "Klasse 4: 0.8697\n",
      "Klasse 5: 0.8744\n",
      "Klasse 6: 0.8904\n",
      "Klasse 7: 0.8881\n",
      "Klasse 8: 0.8419\n",
      "Klasse 9: 0.9128\n",
      "\n",
      "Macro Precision: 0.8939\n",
      "Macro Recall: 0.8934\n"
     ]
    }
   ],
   "source": [
    "# - y_test_oh enthält die one-hot encodierten Ground-Truth-Labels (Form: [num_samples, 10])\n",
    "# - test_out_mc enthält die Softmax-Ausgaben des Modells (Form: [num_samples, 10])\n",
    "# Wir wandeln beide in Label-Indices um.\n",
    "\n",
    "true_labels = np.argmax(y_test_oh, axis=1)\n",
    "pred_labels = np.argmax(test_out_mc, axis=1)\n",
    "num_classes = 10\n",
    "\n",
    "# Confusion Matrix berechnen: Zeilen = wahre Klassen, Spalten = vorhergesagte Klassen\n",
    "conf_matrix = np.zeros((num_classes, num_classes), dtype=int)\n",
    "for t, p in zip(true_labels, pred_labels):\n",
    "    conf_matrix[t, p] += 1\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Precision und Recall pro Klasse berechnen\n",
    "precisions = np.zeros(num_classes)\n",
    "recalls = np.zeros(num_classes)\n",
    "for i in range(num_classes):\n",
    "    TP = conf_matrix[i, i]\n",
    "    FP = np.sum(conf_matrix[:, i]) - TP  # Fälschlicherweise als i klassifizierte Beispiele\n",
    "    FN = np.sum(conf_matrix[i, :]) - TP  # Beispiele der Klasse i, die fälschlicherweise anders klassifiziert wurden\n",
    "    precisions[i] = TP / (TP + FP) if (TP + FP) > 0 else 0.0\n",
    "    recalls[i] = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "\n",
    "print(\"\\nPrecision pro Klasse:\")\n",
    "for i in range(num_classes):\n",
    "    print(f\"Klasse {i}: {precisions[i]:.4f}\")\n",
    "\n",
    "print(\"\\nRecall pro Klasse:\")\n",
    "for i in range(num_classes):\n",
    "    print(f\"Klasse {i}: {recalls[i]:.4f}\")\n",
    "\n",
    "# Optional: Berechnung des durchschnittlichen (Macro-)Precision und -Recall\n",
    "macro_precision = np.mean(precisions)\n",
    "macro_recall = np.mean(recalls)\n",
    "print(f\"\\nMacro Precision: {macro_precision:.4f}\")\n",
    "print(f\"Macro Recall: {macro_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vta_mc_1-eS1Wwm3t",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
